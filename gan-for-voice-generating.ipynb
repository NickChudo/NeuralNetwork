{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5321255,"sourceType":"datasetVersion","datasetId":3091651},{"sourceId":5618710,"sourceType":"datasetVersion","datasetId":3230790},{"sourceId":5677279,"sourceType":"datasetVersion","datasetId":2989949,"isSourceIdPinned":false},{"sourceId":5677298,"sourceType":"datasetVersion","datasetId":3213578,"isSourceIdPinned":false},{"sourceId":5677449,"sourceType":"datasetVersion","datasetId":3071831,"isSourceIdPinned":false},{"sourceId":5760288,"sourceType":"datasetVersion","datasetId":3311237}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np \nimport matplotlib","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:02.471491Z","iopub.execute_input":"2023-11-15T15:28:02.472026Z","iopub.status.idle":"2023-11-15T15:28:05.856765Z","shell.execute_reply.started":"2023-11-15T15:28:02.471971Z","shell.execute_reply":"2023-11-15T15:28:05.855386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/rus-speech/Speeches.xlsx')\ny = [sentence for sentence in file['Русская речь']]\n\ndir_name = \"/kaggle/input/upd-speech/mono_voice/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in range(1, 2001):\n    file_name = f'{e}.wav'\n    sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    X.append(torch.Tensor(sampl))\n    if i % 100 == 0:\n        print(i)\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:05.859496Z","iopub.execute_input":"2023-11-15T15:28:05.860337Z","iopub.status.idle":"2023-11-15T15:28:36.794364Z","shell.execute_reply.started":"2023-11-15T15:28:05.860277Z","shell.execute_reply":"2023-11-15T15:28:36.793210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(X[0])","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:36.796706Z","iopub.execute_input":"2023-11-15T15:28:36.798208Z","iopub.status.idle":"2023-11-15T15:28:36.806299Z","shell.execute_reply.started":"2023-11-15T15:28:36.798159Z","shell.execute_reply":"2023-11-15T15:28:36.805324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('4', 'четыре').replace('Р-220', 'р двести двадцать').replace('6', 'шесть').replace(\"-\", \" \")\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    sentence = \" \".join(sentence.split())\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:36.807875Z","iopub.execute_input":"2023-11-15T15:28:36.808659Z","iopub.status.idle":"2023-11-15T15:28:36.837695Z","shell.execute_reply.started":"2023-11-15T15:28:36.808619Z","shell.execute_reply":"2023-11-15T15:28:36.836122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"from sklearn.model_selection import train_test_split\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nX_train = X[:1800]\nX_test = X[1800:]\ny_train = y[:1800]\ny_test = y[1800:]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:36.840506Z","iopub.execute_input":"2023-11-15T15:28:36.840904Z","iopub.status.idle":"2023-11-15T15:28:36.848253Z","shell.execute_reply.started":"2023-11-15T15:28:36.840869Z","shell.execute_reply":"2023-11-15T15:28:36.847091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X\nX = []","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:36.849702Z","iopub.execute_input":"2023-11-15T15:28:36.850132Z","iopub.status.idle":"2023-11-15T15:28:36.858809Z","shell.execute_reply.started":"2023-11-15T15:28:36.850096Z","shell.execute_reply":"2023-11-15T15:28:36.857561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tensor in X_train:\n    tensor = tensor.squeeze()\n    X.append(tensor)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:36.860353Z","iopub.execute_input":"2023-11-15T15:28:36.861172Z","iopub.status.idle":"2023-11-15T15:28:36.882023Z","shell.execute_reply.started":"2023-11-15T15:28:36.861122Z","shell.execute_reply":"2023-11-15T15:28:36.880620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_tensor = max(X, key=lambda x: x.numel())\n\nprint(max_tensor.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:36.883756Z","iopub.execute_input":"2023-11-15T15:28:36.884102Z","iopub.status.idle":"2023-11-15T15:28:36.893103Z","shell.execute_reply.started":"2023-11-15T15:28:36.884067Z","shell.execute_reply":"2023-11-15T15:28:36.891733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(X[0])","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:36.894808Z","iopub.execute_input":"2023-11-15T15:28:36.895210Z","iopub.status.idle":"2023-11-15T15:28:36.903880Z","shell.execute_reply.started":"2023-11-15T15:28:36.895174Z","shell.execute_reply":"2023-11-15T15:28:36.902652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:36.905692Z","iopub.execute_input":"2023-11-15T15:28:36.906018Z","iopub.status.idle":"2023-11-15T15:28:36.994986Z","shell.execute_reply.started":"2023-11-15T15:28:36.905986Z","shell.execute_reply":"2023-11-15T15:28:36.993742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# Задаем желаемый размер\ndesired_size = (213830,)\nresized_tensors = []\n\nfor tensor in X:\n    # Рассчитываем разницу в размерах\n    padding = [0] * (len(desired_size) * 2)  # padding = [0, 0, 0, 0, 0, 0]\n    padding[:len(desired_size)] = [(desired_size[i] - tensor.shape[i]) for i in range(len(desired_size))]  # padding[:1] = [(6 - 3) // 2] = [1]\n\n    # Применяем padding\n    padded_tensor = F.pad(tensor, padding, value=0)\n    resized_tensors.append(padded_tensor)\n    \nX = resized_tensors","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:36.998417Z","iopub.execute_input":"2023-11-15T15:28:36.998836Z","iopub.status.idle":"2023-11-15T15:28:38.162196Z","shell.execute_reply.started":"2023-11-15T15:28:36.998796Z","shell.execute_reply":"2023-11-15T15:28:38.160991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[5].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:38.163881Z","iopub.execute_input":"2023-11-15T15:28:38.164664Z","iopub.status.idle":"2023-11-15T15:28:38.172919Z","shell.execute_reply.started":"2023-11-15T15:28:38.164610Z","shell.execute_reply":"2023-11-15T15:28:38.171455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:38.174853Z","iopub.execute_input":"2023-11-15T15:28:38.175766Z","iopub.status.idle":"2023-11-15T15:28:38.185339Z","shell.execute_reply.started":"2023-11-15T15:28:38.175710Z","shell.execute_reply":"2023-11-15T15:28:38.184420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_dataset = AudioDataset(X, y)\ntrain_dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:38.186735Z","iopub.execute_input":"2023-11-15T15:28:38.187702Z","iopub.status.idle":"2023-11-15T15:28:38.197517Z","shell.execute_reply.started":"2023-11-15T15:28:38.187621Z","shell.execute_reply":"2023-11-15T15:28:38.196398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Определение генератора\nclass Generator(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(Generator, self).__init__()\n        self.fc = nn.Linear(input_dim, 256)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, output_dim)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.relu(self.fc(x))\n        x = self.relu(self.fc2(x))\n        x = self.tanh(self.fc3(x))\n        return x\n\n# Определение дискриминатора\nclass Discriminator(nn.Module):\n    def __init__(self, input_dim):\n        super(Discriminator, self).__init__()\n        self.fc = nn.Linear(input_dim, 512)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu(self.fc(x))\n        x = self.relu(self.fc2(x))\n        x = self.sigmoid(self.fc3(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:38.198936Z","iopub.execute_input":"2023-11-15T15:28:38.199265Z","iopub.status.idle":"2023-11-15T15:28:38.214447Z","shell.execute_reply.started":"2023-11-15T15:28:38.199232Z","shell.execute_reply":"2023-11-15T15:28:38.213597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Функция обучения GAN\ndef train_gan(generator, discriminator, dataloader, num_epochs, device):\n    criterion = nn.BCELoss()\n    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n\n    for epoch in range(num_epochs):\n        for i, real_data in enumerate(dataloader):\n            \n            \"\"\"real_data = []\n            # Преобразовываем каждый элемент в тензор и перемещаем на устройство\n            for item in dataloader:\n                item = item.to(device)\n                real_data.append(item)\"\"\"\n            print(type(real_data))\n            \n            real_data = real_data.to(device)\n\n            # Обучение дискриминатора\n            d_optimizer.zero_grad()\n            real_labels = torch.ones(real_data.size(0), 1).to(device)\n            fake_labels = torch.zeros(real_data.size(0), 1).to(device)\n\n            # Пропуск реальных данных через дискриминатор\n            real_outputs = discriminator(real_data)\n            d_loss_real = criterion(real_outputs, real_labels)\n\n            # Генерация фейковых данных и пропуск их через дискриминатор\n            noise = torch.randn(real_data.size(0), 100).to(device)\n            fake_data = generator(noise)\n            fake_outputs = discriminator(fake_data.detach())\n            d_loss_fake = criterion(fake_outputs, fake_labels)\n\n            # Общая потеря дискриминатора\n            d_loss = d_loss_real + d_loss_fake\n            d_loss.backward()\n            d_optimizer.step()\n\n            # Обучение генератора\n            g_optimizer.zero_grad()\n            fake_outputs = discriminator(fake_data)\n            g_loss = criterion(fake_outputs, real_labels)\n            g_loss.backward()\n            g_optimizer.step()\n\n            # Вывод промежуточной информации\n            if (i + 1) % 100 == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}],\"\n                      f\" d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:38.215943Z","iopub.execute_input":"2023-11-15T15:28:38.216597Z","iopub.status.idle":"2023-11-15T15:28:38.229704Z","shell.execute_reply.started":"2023-11-15T15:28:38.216560Z","shell.execute_reply":"2023-11-15T15:28:38.228757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Параметры генератора и дискриминатора\ninput_dim = 100  # Размерность входного шумового вектора\noutput_dim = 213830  # Размерность выходных данных (зависит от аудиоформата)\n\n# Создание генератора и дискриминатора\ngenerator = Generator(input_dim, output_dim).to(device)\ndiscriminator = Discriminator(output_dim).to(device)\n\n# Загрузка аудиоданных\ndataset = train_dataset  # Замените на свой собственный класс Dataset\n#dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n\n# Обучение GAN\nnum_epochs = 10\ntrain_gan(generator, discriminator, dataset, num_epochs, device)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:38.231202Z","iopub.execute_input":"2023-11-15T15:28:38.231994Z","iopub.status.idle":"2023-11-15T15:28:40.842633Z","shell.execute_reply.started":"2023-11-15T15:28:38.231957Z","shell.execute_reply":"2023-11-15T15:28:40.840902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader.batch_size","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:40.843705Z","iopub.status.idle":"2023-11-15T15:28:40.844152Z","shell.execute_reply.started":"2023-11-15T15:28:40.843945Z","shell.execute_reply":"2023-11-15T15:28:40.843972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(directory):\n    total_duration = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            filepath = os.path.join(directory, filename)\n            with wave.open(filepath, 'r') as wav_file:\n                frames = wav_file.getnframes()\n                rate = wav_file.getframerate()\n                duration = frames / float(rate)\n                total_duration += duration\n    return total_duration\n\ndirectory = '/kaggle/input/upd-speech/mono_voice'\ntotal_duration = get_wav_duration(directory)\nprint('Total duration of WAV files:', total_duration, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:40.846121Z","iopub.status.idle":"2023-11-15T15:28:40.847287Z","shell.execute_reply.started":"2023-11-15T15:28:40.846975Z","shell.execute_reply":"2023-11-15T15:28:40.847011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return '{:02d}:{:02d}:{:02d}'.format(int(hours), int(minutes), int(seconds))\nseconds = 3661\nformatted_time = format_time(total_duration)\nprint(formatted_time)  # Output: '01:01:01'","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:28:40.849528Z","iopub.status.idle":"2023-11-15T15:28:40.850430Z","shell.execute_reply.started":"2023-11-15T15:28:40.850119Z","shell.execute_reply":"2023-11-15T15:28:40.850152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}