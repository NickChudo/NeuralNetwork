{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5321255,"sourceType":"datasetVersion","datasetId":3091651},{"sourceId":5618710,"sourceType":"datasetVersion","datasetId":3230790},{"sourceId":5677279,"sourceType":"datasetVersion","datasetId":2989949},{"sourceId":5677449,"sourceType":"datasetVersion","datasetId":3071831},{"sourceId":5760288,"sourceType":"datasetVersion","datasetId":3311237},{"sourceId":8003942,"sourceType":"datasetVersion","datasetId":4230886},{"sourceId":8449752,"sourceType":"datasetVersion","datasetId":3213578}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np \nimport matplotlib\nfrom transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:57:14.330180Z","iopub.execute_input":"2024-05-19T07:57:14.331247Z","iopub.status.idle":"2024-05-19T07:57:19.979309Z","shell.execute_reply.started":"2024-05-19T07:57:14.331191Z","shell.execute_reply":"2024-05-19T07:57:19.978017Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        int_sequence = []\n        for c in text:\n            ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MFCC(n_mfcc=20)\n)\n\n\nvalid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//3)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=34, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:57:19.981807Z","iopub.execute_input":"2024-05-19T07:57:19.982455Z","iopub.status.idle":"2024-05-19T07:57:20.140826Z","shell.execute_reply.started":"2024-05-19T07:57:19.982416Z","shell.execute_reply":"2024-05-19T07:57:20.139400Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"class BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:57:20.142382Z","iopub.execute_input":"2024-05-19T07:57:20.142801Z","iopub.status.idle":"2024-05-19T07:57:20.153911Z","shell.execute_reply.started":"2024-05-19T07:57:20.142761Z","shell.execute_reply":"2024-05-19T07:57:20.152036Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Поменял там, где происходит загрузка, сохраняется id звукового файла, а потом в excel файле по колонке old_id ищется текст\n#И того звук и текст к нему\n\nimport pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches v1.xlsx')\n#y = [sentence for sentence in file['text']]\ny = []\ndir_name = \"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in os.listdir(\"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"):\n    file_name = e\n    for old_id in range(0, 2073):\n        if file_name.startswith(str(file['old_id'][old_id]) + '.'):\n            y.extend([''.join(file['text'][old_id])])\n            sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n            sampl = sampl[np.newaxis, :]\n            X.append(torch.Tensor(sampl))\n            break","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:57:20.157556Z","iopub.execute_input":"2024-05-19T07:57:20.158083Z","iopub.status.idle":"2024-05-19T07:59:28.862153Z","shell.execute_reply.started":"2024-05-19T07:57:20.158033Z","shell.execute_reply":"2024-05-19T07:59:28.860502Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import random\npairs = list(zip(X, y))\nrandom.Random(3016).shuffle(pairs)\nX, y = zip(*pairs)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:59:28.864128Z","iopub.execute_input":"2024-05-19T07:59:28.865147Z","iopub.status.idle":"2024-05-19T07:59:28.879219Z","shell.execute_reply.started":"2024-05-19T07:59:28.865094Z","shell.execute_reply":"2024-05-19T07:59:28.877775Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"y[:3]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:59:28.881204Z","iopub.execute_input":"2024-05-19T07:59:28.881699Z","iopub.status.idle":"2024-05-19T07:59:28.897502Z","shell.execute_reply.started":"2024-05-19T07:59:28.881643Z","shell.execute_reply":"2024-05-19T07:59:28.895904Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('Разнообразие кухонных блюд', 'Секс', 'Жаркое лето вызывает жажду')"},"metadata":{}}]},{"cell_type":"code","source":"X[:3]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:28:02.461598Z","iopub.execute_input":"2024-05-19T07:28:02.461975Z","iopub.status.idle":"2024-05-19T07:28:02.563068Z","shell.execute_reply.started":"2024-05-19T07:28:02.461941Z","shell.execute_reply":"2024-05-19T07:28:02.562029Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 5.5877e-09,  1.6280e-09, -1.3302e-10,  ..., -2.2728e-05,\n          -2.2285e-05,  0.0000e+00]]),\n tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0042,  0.0015,  0.0000]]),\n tensor([[-5.1168e-06,  5.6805e-06, -2.0654e-05,  ...,  1.8661e-05,\n           9.8214e-06,  1.7692e-05]]))"},"metadata":{}}]},{"cell_type":"code","source":"torchaudio.save('/kaggle/working/audio.wav', X[540], 16000)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:52:46.826831Z","iopub.execute_input":"2024-04-02T14:52:46.827791Z","iopub.status.idle":"2024-04-02T14:52:46.834792Z","shell.execute_reply.started":"2024-04-02T14:52:46.827746Z","shell.execute_reply":"2024-04-02T14:52:46.833811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"waveform, sample_rate = torchaudio.load('/kaggle/working/audio.wav')  # Загрузка аудиофайла\ntorchaudio.play(waveform, sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:22:15.368540Z","iopub.execute_input":"2024-04-02T07:22:15.368969Z","iopub.status.idle":"2024-04-02T07:22:15.395250Z","shell.execute_reply.started":"2024-04-02T07:22:15.368930Z","shell.execute_reply":"2024-04-02T07:22:15.393666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('4', 'четыре').replace('Р-220', 'р двести двадцать').replace('6', 'шесть').replace(\"-\", \" \")\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    sentence = \" \".join(sentence.split())\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:59:28.902249Z","iopub.execute_input":"2024-05-19T07:59:28.903068Z","iopub.status.idle":"2024-05-19T07:59:28.944445Z","shell.execute_reply.started":"2024-05-19T07:59:28.903021Z","shell.execute_reply":"2024-05-19T07:59:28.943014Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_train = X[:2200]\nX_test = X[2200:]\ny_train = y[:2200]\ny_test = y[2200:]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:59:28.946076Z","iopub.execute_input":"2024-05-19T07:59:28.946453Z","iopub.status.idle":"2024-05-19T07:59:28.953804Z","shell.execute_reply.started":"2024-05-19T07:59:28.946417Z","shell.execute_reply":"2024-05-19T07:59:28.952481Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:59:28.955577Z","iopub.execute_input":"2024-05-19T07:59:28.956027Z","iopub.status.idle":"2024-05-19T07:59:28.966014Z","shell.execute_reply.started":"2024-05-19T07:59:28.955970Z","shell.execute_reply":"2024-05-19T07:59:28.964715Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 32, kernel_size=(4,4), stride=(3,3), padding=(2,2)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.Conv2d(128, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n        )\n        \n        self.fc_1 = nn.Sequential(\n            nn.Linear(896, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n        )\n        \n        self.BiGRU_1 = BidirectionalGRU(270, 270, 0, True)\n        self.BiGRU_2 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_3 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_4 = BidirectionalGRU(540, 270, 0.5, True)\n        \n        self.fc_2 = nn.Sequential(\n            nn.Linear(540, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.fc_1(x)\n        x = self.BiGRU_1(x)\n        x = self.BiGRU_2(x)\n        x = self.BiGRU_3(x)\n        x = self.BiGRU_4(x)\n        x = self.fc_2(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:59:28.967735Z","iopub.execute_input":"2024-05-19T07:59:28.968145Z","iopub.status.idle":"2024-05-19T07:59:28.989764Z","shell.execute_reply.started":"2024-05-19T07:59:28.968105Z","shell.execute_reply":"2024-05-19T07:59:28.988065Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Зададим название выбронной модели из хаба\nMODEL_NAME = 'UrukHan/t5-russian-spell'\nMAX_INPUT = 256\n\n# Загрузка модели и токенизатора\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\ncorrector = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T03:55:57.788196Z","iopub.execute_input":"2024-05-19T03:55:57.788942Z","iopub.status.idle":"2024-05-19T03:56:24.041906Z","shell.execute_reply.started":"2024-05-19T03:55:57.788901Z","shell.execute_reply":"2024-05-19T03:56:24.040871Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1122f6fe40843358066ae853960f799"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/1.00M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d00ffb8c5f94ee79106c6e44ad8f232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/2.63M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b4f1fc2f6214a64a38c2123b7f7f938"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ae77e017bb645fa8699e307458f279e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ad5b07e8f4849ebb4cb5f11fc5d3a77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f560be26fc7460d8edcdb3dc3ef4f5c"}},"metadata":{}}]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        \"\"\"decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        for j in range(len(decoded_preds)):\n            train_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n            train_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n    avg_cer = sum(train_cer)/len(train_cer)\n    avg_wer = sum(train_wer)/len(train_wer)\n            \n    print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(train_loss, avg_cer, avg_wer))\"\"\"\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n            \n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n            \n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n   \n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model_for_correction_test.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:59:28.992157Z","iopub.execute_input":"2024-05-19T07:59:28.992709Z","iopub.status.idle":"2024-05-19T07:59:29.031098Z","shell.execute_reply.started":"2024-05-19T07:59:28.992634Z","shell.execute_reply":"2024-05-19T07:59:29.029595Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict(model, file_name, device):\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    sampl = librosa.load(file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    print(spectrogram_tensor.size())\n\n    with torch.no_grad():\n        spectrogram_tensor.to(device)\n        output = model(spectrogram_tensor)\n        print(output.size())\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:59:29.032805Z","iopub.execute_input":"2024-05-19T07:59:29.033217Z","iopub.status.idle":"2024-05-19T07:59:29.048205Z","shell.execute_reply.started":"2024-05-19T07:59:29.033152Z","shell.execute_reply":"2024-05-19T07:59:29.046537Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict_with_tensor(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    #sampl = librosa.load(file_name, sr=16000)[0]\n    #sampl = sampl[np.newaxis, :]\n    #sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    \"\"\"input_sequences = decodes[0]\n                \n    task_prefix = \"Spell correct: \"\n\n    if type(input_sequences) != list: input_sequences = [input_sequences]\n    encoded = tokenizer(\n      [task_prefix + sequence for sequence in input_sequences],\n      padding=\"longest\",\n      max_length=MAX_INPUT,\n      truncation=True,\n      return_tensors=\"pt\",\n    )\n\n    predicts = corrector.generate(**encoded.to(needed_device))   # # Прогнозирование\n\n    input_sequences = tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]\n    input_sequences = remove_characters(input_sequences)\n\n    return input_sequences\"\"\"\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:59:29.050330Z","iopub.execute_input":"2024-05-19T07:59:29.050875Z","iopub.status.idle":"2024-05-19T07:59:29.065373Z","shell.execute_reply.started":"2024-05-19T07:59:29.050810Z","shell.execute_reply":"2024-05-19T07:59:29.064014Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"%%time \nlearning_rate = 0.002\nbatch_size = 20\nepochs = 40\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-19T07:28:02.692311Z","iopub.execute_input":"2024-05-19T07:28:02.692724Z","iopub.status.idle":"2024-05-19T07:49:17.061698Z","shell.execute_reply.started":"2024-05-19T07:28:02.692688Z","shell.execute_reply":"2024-05-19T07:49:17.060458Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel1(\n  (conv): Sequential(\n    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Conv2d(1, 32, kernel_size=(4, 4), stride=(3, 3), padding=(2, 2))\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): GELU(approximate='none')\n    (4): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): GELU(approximate='none')\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): GELU(approximate='none')\n  )\n  (fc_1): Sequential(\n    (0): Linear(in_features=896, out_features=270, bias=True)\n    (1): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=270, out_features=270, bias=True)\n    (4): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (5): GELU(approximate='none')\n    (6): Linear(in_features=270, out_features=270, bias=True)\n    (7): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (8): GELU(approximate='none')\n  )\n  (BiGRU_1): BidirectionalGRU(\n    (BiGRU): GRU(270, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_2): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_3): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_4): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (fc_2): Sequential(\n    (0): Linear(in_features=540, out_features=35, bias=True)\n  )\n  (softmax): LogSoftmax(dim=2)\n)\nNum Model Parameters 5422923\nTrain Epoch: 1 [0/2200 (0%)]\tLoss: 17.996298\nTrain Epoch: 1 [400/2200 (18%)]\tLoss: 3.795234\nTrain Epoch: 1 [800/2200 (36%)]\tLoss: 3.434981\nTrain Epoch: 1 [1200/2200 (55%)]\tLoss: 3.308342\nTrain Epoch: 1 [1600/2200 (73%)]\tLoss: 3.306792\nTrain Epoch: 1 [2000/2200 (91%)]\tLoss: 3.322932\n\nevaluating...\nTest set:\tAverage loss: 3.2895, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 2 [0/2200 (0%)]\tLoss: 3.274664\nTrain Epoch: 2 [400/2200 (18%)]\tLoss: 3.252937\nTrain Epoch: 2 [800/2200 (36%)]\tLoss: 3.343894\nTrain Epoch: 2 [1200/2200 (55%)]\tLoss: 3.255402\nTrain Epoch: 2 [1600/2200 (73%)]\tLoss: 3.308183\nTrain Epoch: 2 [2000/2200 (91%)]\tLoss: 3.195855\n\nevaluating...\nTest set:\tAverage loss: 3.2295, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 3 [0/2200 (0%)]\tLoss: 3.258018\nTrain Epoch: 3 [400/2200 (18%)]\tLoss: 3.188490\nTrain Epoch: 3 [800/2200 (36%)]\tLoss: 3.032723\nTrain Epoch: 3 [1200/2200 (55%)]\tLoss: 2.931561\nTrain Epoch: 3 [1600/2200 (73%)]\tLoss: 2.750105\nTrain Epoch: 3 [2000/2200 (91%)]\tLoss: 2.497334\n\nevaluating...\nTest set:\tAverage loss: 2.4146, Average CER: 0.970429 Average WER: 1.0017\n\nTrain Epoch: 4 [0/2200 (0%)]\tLoss: 2.411399\nTrain Epoch: 4 [400/2200 (18%)]\tLoss: 2.156826\nTrain Epoch: 4 [800/2200 (36%)]\tLoss: 1.898460\nTrain Epoch: 4 [1200/2200 (55%)]\tLoss: 1.714116\nTrain Epoch: 4 [1600/2200 (73%)]\tLoss: 1.607740\nTrain Epoch: 4 [2000/2200 (91%)]\tLoss: 1.892034\n\nevaluating...\nTest set:\tAverage loss: 1.4843, Average CER: 0.469953 Average WER: 0.9768\n\nTrain Epoch: 5 [0/2200 (0%)]\tLoss: 1.367498\nTrain Epoch: 5 [400/2200 (18%)]\tLoss: 1.411606\nTrain Epoch: 5 [800/2200 (36%)]\tLoss: 1.319867\nTrain Epoch: 5 [1200/2200 (55%)]\tLoss: 1.291818\nTrain Epoch: 5 [1600/2200 (73%)]\tLoss: 1.356717\nTrain Epoch: 5 [2000/2200 (91%)]\tLoss: 1.313074\n\nevaluating...\nTest set:\tAverage loss: 1.1856, Average CER: 0.371672 Average WER: 0.9594\n\nTrain Epoch: 6 [0/2200 (0%)]\tLoss: 1.041490\nTrain Epoch: 6 [400/2200 (18%)]\tLoss: 1.051865\nTrain Epoch: 6 [800/2200 (36%)]\tLoss: 0.976751\nTrain Epoch: 6 [1200/2200 (55%)]\tLoss: 1.109226\nTrain Epoch: 6 [1600/2200 (73%)]\tLoss: 1.068391\nTrain Epoch: 6 [2000/2200 (91%)]\tLoss: 0.913292\n\nevaluating...\nTest set:\tAverage loss: 1.1092, Average CER: 0.339690 Average WER: 0.9758\n\nTrain Epoch: 7 [0/2200 (0%)]\tLoss: 1.054810\nTrain Epoch: 7 [400/2200 (18%)]\tLoss: 1.007857\nTrain Epoch: 7 [800/2200 (36%)]\tLoss: 0.877602\nTrain Epoch: 7 [1200/2200 (55%)]\tLoss: 0.861665\nTrain Epoch: 7 [1600/2200 (73%)]\tLoss: 0.849588\nTrain Epoch: 7 [2000/2200 (91%)]\tLoss: 0.878254\n\nevaluating...\nTest set:\tAverage loss: 0.9569, Average CER: 0.297728 Average WER: 0.9053\n\nTrain Epoch: 8 [0/2200 (0%)]\tLoss: 0.808190\nTrain Epoch: 8 [400/2200 (18%)]\tLoss: 0.829838\nTrain Epoch: 8 [800/2200 (36%)]\tLoss: 0.886822\nTrain Epoch: 8 [1200/2200 (55%)]\tLoss: 0.855263\nTrain Epoch: 8 [1600/2200 (73%)]\tLoss: 0.871101\nTrain Epoch: 8 [2000/2200 (91%)]\tLoss: 1.261826\n\nevaluating...\nTest set:\tAverage loss: 0.9464, Average CER: 0.290091 Average WER: 0.8992\n\nTrain Epoch: 9 [0/2200 (0%)]\tLoss: 0.734779\nTrain Epoch: 9 [400/2200 (18%)]\tLoss: 0.574313\nTrain Epoch: 9 [800/2200 (36%)]\tLoss: 0.677184\nTrain Epoch: 9 [1200/2200 (55%)]\tLoss: 0.798512\nTrain Epoch: 9 [1600/2200 (73%)]\tLoss: 0.809954\nTrain Epoch: 9 [2000/2200 (91%)]\tLoss: 0.778421\n\nevaluating...\nTest set:\tAverage loss: 0.9179, Average CER: 0.286155 Average WER: 0.8746\n\nTrain Epoch: 10 [0/2200 (0%)]\tLoss: 0.653043\nTrain Epoch: 10 [400/2200 (18%)]\tLoss: 0.679570\nTrain Epoch: 10 [800/2200 (36%)]\tLoss: 0.601794\nTrain Epoch: 10 [1200/2200 (55%)]\tLoss: 0.747115\nTrain Epoch: 10 [1600/2200 (73%)]\tLoss: 0.748055\nTrain Epoch: 10 [2000/2200 (91%)]\tLoss: 0.747939\n\nevaluating...\nTest set:\tAverage loss: 0.8738, Average CER: 0.269556 Average WER: 0.8698\n\nTrain Epoch: 11 [0/2200 (0%)]\tLoss: 0.498582\nTrain Epoch: 11 [400/2200 (18%)]\tLoss: 0.652851\nTrain Epoch: 11 [800/2200 (36%)]\tLoss: 0.569031\nTrain Epoch: 11 [1200/2200 (55%)]\tLoss: 0.635201\nTrain Epoch: 11 [1600/2200 (73%)]\tLoss: 0.658590\nTrain Epoch: 11 [2000/2200 (91%)]\tLoss: 0.513298\n\nevaluating...\nTest set:\tAverage loss: 0.9009, Average CER: 0.266395 Average WER: 0.8548\n\nTrain Epoch: 12 [0/2200 (0%)]\tLoss: 0.583576\nTrain Epoch: 12 [400/2200 (18%)]\tLoss: 0.466099\nTrain Epoch: 12 [800/2200 (36%)]\tLoss: 0.764849\nTrain Epoch: 12 [1200/2200 (55%)]\tLoss: 0.594933\nTrain Epoch: 12 [1600/2200 (73%)]\tLoss: 0.759829\nTrain Epoch: 12 [2000/2200 (91%)]\tLoss: 0.619574\n\nevaluating...\nTest set:\tAverage loss: 0.8453, Average CER: 0.255105 Average WER: 0.8455\n\nTrain Epoch: 13 [0/2200 (0%)]\tLoss: 0.519339\nTrain Epoch: 13 [400/2200 (18%)]\tLoss: 0.614125\nTrain Epoch: 13 [800/2200 (36%)]\tLoss: 0.615407\nTrain Epoch: 13 [1200/2200 (55%)]\tLoss: 0.676726\nTrain Epoch: 13 [1600/2200 (73%)]\tLoss: 0.621648\nTrain Epoch: 13 [2000/2200 (91%)]\tLoss: 0.649243\n\nevaluating...\nTest set:\tAverage loss: 0.8152, Average CER: 0.238383 Average WER: 0.8290\n\nTrain Epoch: 14 [0/2200 (0%)]\tLoss: 0.591632\nTrain Epoch: 14 [400/2200 (18%)]\tLoss: 0.595810\nTrain Epoch: 14 [800/2200 (36%)]\tLoss: 0.376661\nTrain Epoch: 14 [1200/2200 (55%)]\tLoss: 0.449928\nTrain Epoch: 14 [1600/2200 (73%)]\tLoss: 0.461367\nTrain Epoch: 14 [2000/2200 (91%)]\tLoss: 0.555699\n\nevaluating...\nTest set:\tAverage loss: 0.8362, Average CER: 0.250827 Average WER: 0.8184\n\nTrain Epoch: 15 [0/2200 (0%)]\tLoss: 0.463470\nTrain Epoch: 15 [400/2200 (18%)]\tLoss: 0.415753\nTrain Epoch: 15 [800/2200 (36%)]\tLoss: 0.468086\nTrain Epoch: 15 [1200/2200 (55%)]\tLoss: 0.487654\nTrain Epoch: 15 [1600/2200 (73%)]\tLoss: 0.361123\nTrain Epoch: 15 [2000/2200 (91%)]\tLoss: 0.369440\n\nevaluating...\nTest set:\tAverage loss: 0.8039, Average CER: 0.228802 Average WER: 0.7987\n\nTrain Epoch: 16 [0/2200 (0%)]\tLoss: 0.473564\nTrain Epoch: 16 [400/2200 (18%)]\tLoss: 0.371154\nTrain Epoch: 16 [800/2200 (36%)]\tLoss: 0.361906\nTrain Epoch: 16 [1200/2200 (55%)]\tLoss: 0.350820\nTrain Epoch: 16 [1600/2200 (73%)]\tLoss: 0.371019\nTrain Epoch: 16 [2000/2200 (91%)]\tLoss: 0.387654\n\nevaluating...\nTest set:\tAverage loss: 0.7701, Average CER: 0.216988 Average WER: 0.7644\n\nTrain Epoch: 17 [0/2200 (0%)]\tLoss: 0.274332\nTrain Epoch: 17 [400/2200 (18%)]\tLoss: 0.302260\nTrain Epoch: 17 [800/2200 (36%)]\tLoss: 0.221341\nTrain Epoch: 17 [1200/2200 (55%)]\tLoss: 0.320190\nTrain Epoch: 17 [1600/2200 (73%)]\tLoss: 0.362259\nTrain Epoch: 17 [2000/2200 (91%)]\tLoss: 0.330856\n\nevaluating...\nTest set:\tAverage loss: 0.7903, Average CER: 0.216384 Average WER: 0.7792\n\nTrain Epoch: 18 [0/2200 (0%)]\tLoss: 0.233856\nTrain Epoch: 18 [400/2200 (18%)]\tLoss: 0.179723\nTrain Epoch: 18 [800/2200 (36%)]\tLoss: 0.168549\nTrain Epoch: 18 [1200/2200 (55%)]\tLoss: 0.311884\nTrain Epoch: 18 [1600/2200 (73%)]\tLoss: 0.244669\nTrain Epoch: 18 [2000/2200 (91%)]\tLoss: 0.231078\n\nevaluating...\nTest set:\tAverage loss: 0.7600, Average CER: 0.201066 Average WER: 0.7509\n\nTrain Epoch: 19 [0/2200 (0%)]\tLoss: 0.144058\nTrain Epoch: 19 [400/2200 (18%)]\tLoss: 0.276683\nTrain Epoch: 19 [800/2200 (36%)]\tLoss: 0.156749\nTrain Epoch: 19 [1200/2200 (55%)]\tLoss: 0.190600\nTrain Epoch: 19 [1600/2200 (73%)]\tLoss: 0.151964\nTrain Epoch: 19 [2000/2200 (91%)]\tLoss: 0.184592\n\nevaluating...\nTest set:\tAverage loss: 0.8431, Average CER: 0.216449 Average WER: 0.7734\n\nTrain Epoch: 20 [0/2200 (0%)]\tLoss: 0.197705\nTrain Epoch: 20 [400/2200 (18%)]\tLoss: 0.177512\nTrain Epoch: 20 [800/2200 (36%)]\tLoss: 0.142611\nTrain Epoch: 20 [1200/2200 (55%)]\tLoss: 0.205964\nTrain Epoch: 20 [1600/2200 (73%)]\tLoss: 0.122301\nTrain Epoch: 20 [2000/2200 (91%)]\tLoss: 0.172610\n\nevaluating...\nTest set:\tAverage loss: 0.8309, Average CER: 0.202736 Average WER: 0.7365\n\nTrain Epoch: 21 [0/2200 (0%)]\tLoss: 0.108514\nTrain Epoch: 21 [400/2200 (18%)]\tLoss: 0.074123\nTrain Epoch: 21 [800/2200 (36%)]\tLoss: 0.121217\nTrain Epoch: 21 [1200/2200 (55%)]\tLoss: 0.133740\nTrain Epoch: 21 [1600/2200 (73%)]\tLoss: 0.096409\nTrain Epoch: 21 [2000/2200 (91%)]\tLoss: 0.105419\n\nevaluating...\nTest set:\tAverage loss: 0.8352, Average CER: 0.189925 Average WER: 0.7071\n\nTrain Epoch: 22 [0/2200 (0%)]\tLoss: 0.099029\nTrain Epoch: 22 [400/2200 (18%)]\tLoss: 0.069289\nTrain Epoch: 22 [800/2200 (36%)]\tLoss: 0.094433\nTrain Epoch: 22 [1200/2200 (55%)]\tLoss: 0.062049\nTrain Epoch: 22 [1600/2200 (73%)]\tLoss: 0.085684\nTrain Epoch: 22 [2000/2200 (91%)]\tLoss: 0.067380\n\nevaluating...\nTest set:\tAverage loss: 0.8256, Average CER: 0.184602 Average WER: 0.7049\n\nTrain Epoch: 23 [0/2200 (0%)]\tLoss: 0.077955\nTrain Epoch: 23 [400/2200 (18%)]\tLoss: 0.081360\nTrain Epoch: 23 [800/2200 (36%)]\tLoss: 0.064132\nTrain Epoch: 23 [1200/2200 (55%)]\tLoss: 0.061313\nTrain Epoch: 23 [1600/2200 (73%)]\tLoss: 0.120543\nTrain Epoch: 23 [2000/2200 (91%)]\tLoss: 0.129310\n\nevaluating...\nTest set:\tAverage loss: 0.8509, Average CER: 0.184013 Average WER: 0.7019\n\nTrain Epoch: 24 [0/2200 (0%)]\tLoss: 0.065266\nTrain Epoch: 24 [400/2200 (18%)]\tLoss: 0.059299\nTrain Epoch: 24 [800/2200 (36%)]\tLoss: 0.044366\nTrain Epoch: 24 [1200/2200 (55%)]\tLoss: 0.055583\nTrain Epoch: 24 [1600/2200 (73%)]\tLoss: 0.084223\nTrain Epoch: 24 [2000/2200 (91%)]\tLoss: 0.083510\n\nevaluating...\nTest set:\tAverage loss: 0.8624, Average CER: 0.181011 Average WER: 0.7059\n\nTrain Epoch: 25 [0/2200 (0%)]\tLoss: 0.056691\nTrain Epoch: 25 [400/2200 (18%)]\tLoss: 0.035296\nTrain Epoch: 25 [800/2200 (36%)]\tLoss: 0.035203\nTrain Epoch: 25 [1200/2200 (55%)]\tLoss: 0.028252\nTrain Epoch: 25 [1600/2200 (73%)]\tLoss: 0.021761\nTrain Epoch: 25 [2000/2200 (91%)]\tLoss: 0.059423\n\nevaluating...\nTest set:\tAverage loss: 0.8797, Average CER: 0.174962 Average WER: 0.6903\n\nTrain Epoch: 26 [0/2200 (0%)]\tLoss: 0.022084\nTrain Epoch: 26 [400/2200 (18%)]\tLoss: 0.011038\nTrain Epoch: 26 [800/2200 (36%)]\tLoss: 0.015695\nTrain Epoch: 26 [1200/2200 (55%)]\tLoss: 0.039244\nTrain Epoch: 26 [1600/2200 (73%)]\tLoss: 0.021565\nTrain Epoch: 26 [2000/2200 (91%)]\tLoss: 0.014353\n\nevaluating...\nTest set:\tAverage loss: 0.8850, Average CER: 0.170705 Average WER: 0.6683\n\nTrain Epoch: 27 [0/2200 (0%)]\tLoss: 0.013357\nTrain Epoch: 27 [400/2200 (18%)]\tLoss: 0.009494\nTrain Epoch: 27 [800/2200 (36%)]\tLoss: 0.011148\nTrain Epoch: 27 [1200/2200 (55%)]\tLoss: 0.011694\nTrain Epoch: 27 [1600/2200 (73%)]\tLoss: 0.015191\nTrain Epoch: 27 [2000/2200 (91%)]\tLoss: 0.002558\n\nevaluating...\nTest set:\tAverage loss: 0.8981, Average CER: 0.170466 Average WER: 0.6771\n\nTrain Epoch: 28 [0/2200 (0%)]\tLoss: 0.005768\nTrain Epoch: 28 [400/2200 (18%)]\tLoss: 0.005923\nTrain Epoch: 28 [800/2200 (36%)]\tLoss: 0.005238\nTrain Epoch: 28 [1200/2200 (55%)]\tLoss: 0.008116\nTrain Epoch: 28 [1600/2200 (73%)]\tLoss: 0.012344\nTrain Epoch: 28 [2000/2200 (91%)]\tLoss: 0.007962\n\nevaluating...\nTest set:\tAverage loss: 0.9212, Average CER: 0.169937 Average WER: 0.6669\n\nTrain Epoch: 29 [0/2200 (0%)]\tLoss: 0.005399\nTrain Epoch: 29 [400/2200 (18%)]\tLoss: 0.006105\nTrain Epoch: 29 [800/2200 (36%)]\tLoss: 0.009300\nTrain Epoch: 29 [1200/2200 (55%)]\tLoss: 0.002856\nTrain Epoch: 29 [1600/2200 (73%)]\tLoss: 0.002557\nTrain Epoch: 29 [2000/2200 (91%)]\tLoss: 0.005335\n\nevaluating...\nTest set:\tAverage loss: 0.9243, Average CER: 0.167258 Average WER: 0.6646\n\nTrain Epoch: 30 [0/2200 (0%)]\tLoss: 0.003497\nTrain Epoch: 30 [400/2200 (18%)]\tLoss: 0.002614\nTrain Epoch: 30 [800/2200 (36%)]\tLoss: 0.005772\nTrain Epoch: 30 [1200/2200 (55%)]\tLoss: 0.003503\nTrain Epoch: 30 [1600/2200 (73%)]\tLoss: 0.014014\nTrain Epoch: 30 [2000/2200 (91%)]\tLoss: 0.023582\n\nevaluating...\nTest set:\tAverage loss: 0.9680, Average CER: 0.170335 Average WER: 0.6746\n\nTrain Epoch: 31 [0/2200 (0%)]\tLoss: 0.004356\nTrain Epoch: 31 [400/2200 (18%)]\tLoss: 0.014326\nTrain Epoch: 31 [800/2200 (36%)]\tLoss: 0.004273\nTrain Epoch: 31 [1200/2200 (55%)]\tLoss: 0.005250\nTrain Epoch: 31 [1600/2200 (73%)]\tLoss: 0.007542\nTrain Epoch: 31 [2000/2200 (91%)]\tLoss: 0.004798\n\nevaluating...\nTest set:\tAverage loss: 0.9369, Average CER: 0.163541 Average WER: 0.6613\n\nTrain Epoch: 32 [0/2200 (0%)]\tLoss: 0.001875\nTrain Epoch: 32 [400/2200 (18%)]\tLoss: 0.006952\nTrain Epoch: 32 [800/2200 (36%)]\tLoss: 0.002679\nTrain Epoch: 32 [1200/2200 (55%)]\tLoss: 0.003119\nTrain Epoch: 32 [1600/2200 (73%)]\tLoss: 0.001158\nTrain Epoch: 32 [2000/2200 (91%)]\tLoss: 0.002413\n\nevaluating...\nTest set:\tAverage loss: 0.9486, Average CER: 0.162951 Average WER: 0.6572\n\nTrain Epoch: 33 [0/2200 (0%)]\tLoss: 0.001163\nTrain Epoch: 33 [400/2200 (18%)]\tLoss: 0.001725\nTrain Epoch: 33 [800/2200 (36%)]\tLoss: 0.002412\nTrain Epoch: 33 [1200/2200 (55%)]\tLoss: 0.001156\nTrain Epoch: 33 [1600/2200 (73%)]\tLoss: 0.001105\nTrain Epoch: 33 [2000/2200 (91%)]\tLoss: 0.001405\n\nevaluating...\nTest set:\tAverage loss: 0.9569, Average CER: 0.162423 Average WER: 0.6550\n\nTrain Epoch: 34 [0/2200 (0%)]\tLoss: 0.001083\nTrain Epoch: 34 [400/2200 (18%)]\tLoss: 0.002114\nTrain Epoch: 34 [800/2200 (36%)]\tLoss: 0.000944\nTrain Epoch: 34 [1200/2200 (55%)]\tLoss: 0.001224\nTrain Epoch: 34 [1600/2200 (73%)]\tLoss: 0.001193\nTrain Epoch: 34 [2000/2200 (91%)]\tLoss: 0.001184\n\nevaluating...\nTest set:\tAverage loss: 0.9636, Average CER: 0.160643 Average WER: 0.6577\n\nTrain Epoch: 35 [0/2200 (0%)]\tLoss: 0.001937\nTrain Epoch: 35 [400/2200 (18%)]\tLoss: 0.000782\nTrain Epoch: 35 [800/2200 (36%)]\tLoss: 0.001325\nTrain Epoch: 35 [1200/2200 (55%)]\tLoss: 0.000772\nTrain Epoch: 35 [1600/2200 (73%)]\tLoss: 0.001461\nTrain Epoch: 35 [2000/2200 (91%)]\tLoss: 0.000824\n\nevaluating...\nTest set:\tAverage loss: 0.9711, Average CER: 0.160478 Average WER: 0.6534\n\nTrain Epoch: 36 [0/2200 (0%)]\tLoss: 0.000903\nTrain Epoch: 36 [400/2200 (18%)]\tLoss: 0.000514\nTrain Epoch: 36 [800/2200 (36%)]\tLoss: 0.001302\nTrain Epoch: 36 [1200/2200 (55%)]\tLoss: 0.001507\nTrain Epoch: 36 [1600/2200 (73%)]\tLoss: 0.000983\nTrain Epoch: 36 [2000/2200 (91%)]\tLoss: 0.002331\n\nevaluating...\nTest set:\tAverage loss: 0.9727, Average CER: 0.160822 Average WER: 0.6518\n\nTrain Epoch: 37 [0/2200 (0%)]\tLoss: 0.000958\nTrain Epoch: 37 [400/2200 (18%)]\tLoss: 0.001342\nTrain Epoch: 37 [800/2200 (36%)]\tLoss: 0.000824\nTrain Epoch: 37 [1200/2200 (55%)]\tLoss: 0.000876\nTrain Epoch: 37 [1600/2200 (73%)]\tLoss: 0.001032\nTrain Epoch: 37 [2000/2200 (91%)]\tLoss: 0.000524\n\nevaluating...\nTest set:\tAverage loss: 0.9756, Average CER: 0.160272 Average WER: 0.6521\n\nTrain Epoch: 38 [0/2200 (0%)]\tLoss: 0.000644\nTrain Epoch: 38 [400/2200 (18%)]\tLoss: 0.000606\nTrain Epoch: 38 [800/2200 (36%)]\tLoss: 0.002343\nTrain Epoch: 38 [1200/2200 (55%)]\tLoss: 0.000607\nTrain Epoch: 38 [1600/2200 (73%)]\tLoss: 0.000616\nTrain Epoch: 38 [2000/2200 (91%)]\tLoss: 0.000907\n\nevaluating...\nTest set:\tAverage loss: 0.9764, Average CER: 0.160096 Average WER: 0.6491\n\nTrain Epoch: 39 [0/2200 (0%)]\tLoss: 0.001705\nTrain Epoch: 39 [400/2200 (18%)]\tLoss: 0.000840\nTrain Epoch: 39 [800/2200 (36%)]\tLoss: 0.001060\nTrain Epoch: 39 [1200/2200 (55%)]\tLoss: 0.000456\nTrain Epoch: 39 [1600/2200 (73%)]\tLoss: 0.000686\nTrain Epoch: 39 [2000/2200 (91%)]\tLoss: 0.000927\n\nevaluating...\nTest set:\tAverage loss: 0.9771, Average CER: 0.160594 Average WER: 0.6496\n\nTrain Epoch: 40 [0/2200 (0%)]\tLoss: 0.000827\nTrain Epoch: 40 [400/2200 (18%)]\tLoss: 0.000559\nTrain Epoch: 40 [800/2200 (36%)]\tLoss: 0.000643\nTrain Epoch: 40 [1200/2200 (55%)]\tLoss: 0.000524\nTrain Epoch: 40 [1600/2200 (73%)]\tLoss: 0.001376\nTrain Epoch: 40 [2000/2200 (91%)]\tLoss: 0.000954\n\nevaluating...\nTest set:\tAverage loss: 0.9773, Average CER: 0.160493 Average WER: 0.6499\n\nCPU times: user 20min 15s, sys: 59.5 s, total: 21min 14s\nWall time: 21min 14s\n","output_type":"stream"}]},{"cell_type":"code","source":"#use_cuda = torch.cuda.is_available()\n#device = torch.device(\"cpu\")\nneeded_device = torch.device(\"cpu\")\nmodel = torch.load('/kaggle/input/dop-test-files/model_for_making_dataset_v7(3016).pt', map_location=torch.device('cpu'))\n\n#1543 1882 1372\n\nmodel.to(needed_device)\nprint(needed_device)\n#predict(model, '/kaggle/input/upd-speech/mono_voice/1964.wav', device)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:08:16.330735Z","iopub.execute_input":"2024-05-19T08:08:16.332002Z","iopub.status.idle":"2024-05-19T08:08:16.670949Z","shell.execute_reply.started":"2024-05-19T08:08:16.331951Z","shell.execute_reply":"2024-05-19T08:08:16.669250Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"d = {'X_test': X_test, 'label': y_test}\ndf_test = pd.DataFrame(data=d)\ndf_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:08:18.563459Z","iopub.execute_input":"2024-05-19T08:08:18.564020Z","iopub.status.idle":"2024-05-19T08:08:27.394760Z","shell.execute_reply.started":"2024-05-19T08:08:18.563971Z","shell.execute_reply":"2024-05-19T08:08:27.393586Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                              X_test  \\\n0  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n1  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n2  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n3  [[tensor(2.0789e-06), tensor(-3.5355e-06), ten...   \n4  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n\n                                label  \n0                            отмечать  \n1   тапки не стоит оставлять на улице  \n2                              зверёк  \n3                    любовь к природе  \n4  вселенная бесконечна действительно  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X_test</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>отмечать</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>тапки не стоит оставлять на улице</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>зверёк</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[tensor(2.0789e-06), tensor(-3.5355e-06), ten...</td>\n      <td>любовь к природе</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>вселенная бесконечна действительно</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"y_test[:5]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:08:27.397094Z","iopub.execute_input":"2024-05-19T08:08:27.397713Z","iopub.status.idle":"2024-05-19T08:08:27.407228Z","shell.execute_reply.started":"2024-05-19T08:08:27.397626Z","shell.execute_reply":"2024-05-19T08:08:27.405881Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['отмечать',\n 'тапки не стоит оставлять на улице',\n 'зверёк',\n 'любовь к природе',\n 'вселенная бесконечна действительно']"},"metadata":{}}]},{"cell_type":"code","source":"def count_test_cer(row, model):\n    prediction = predict_with_tensor(model, row['X_test'])\n    return cer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:08:27.408853Z","iopub.execute_input":"2024-05-19T08:08:27.409286Z","iopub.status.idle":"2024-05-19T08:08:27.417374Z","shell.execute_reply.started":"2024-05-19T08:08:27.409248Z","shell.execute_reply":"2024-05-19T08:08:27.416028Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def count_test_wer(row, model):\n    prediction = predict_with_tensor(model, row['X_test'])\n    return wer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:08:27.420918Z","iopub.execute_input":"2024-05-19T08:08:27.421939Z","iopub.status.idle":"2024-05-19T08:08:27.430112Z","shell.execute_reply.started":"2024-05-19T08:08:27.421897Z","shell.execute_reply":"2024-05-19T08:08:27.428713Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def write_preds(row, model):\n    return predict_with_tensor(model, row['X_test'])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:08:27.431740Z","iopub.execute_input":"2024-05-19T08:08:27.432185Z","iopub.status.idle":"2024-05-19T08:08:27.441842Z","shell.execute_reply.started":"2024-05-19T08:08:27.432130Z","shell.execute_reply":"2024-05-19T08:08:27.440171Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df_test['CER'] = df_test.apply(count_test_cer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:08:27.443656Z","iopub.execute_input":"2024-05-19T08:08:27.444842Z","iopub.status.idle":"2024-05-19T08:09:27.210765Z","shell.execute_reply.started":"2024-05-19T08:08:27.444780Z","shell.execute_reply":"2024-05-19T08:09:27.208654Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"df_test['WER'] = df_test.apply(count_test_wer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:09:27.213497Z","iopub.execute_input":"2024-05-19T08:09:27.214774Z","iopub.status.idle":"2024-05-19T08:10:20.669928Z","shell.execute_reply.started":"2024-05-19T08:09:27.214711Z","shell.execute_reply":"2024-05-19T08:10:20.668821Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df_test['preds'] = df_test.apply(write_preds, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:10:20.671702Z","iopub.execute_input":"2024-05-19T08:10:20.672155Z","iopub.status.idle":"2024-05-19T08:11:14.078718Z","shell.execute_reply.started":"2024-05-19T08:10:20.672113Z","shell.execute_reply":"2024-05-19T08:11:14.076977Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_test.loc[df_test['CER'] > 0]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:12:14.453849Z","iopub.execute_input":"2024-05-19T08:12:14.454812Z","iopub.status.idle":"2024-05-19T08:12:25.325371Z","shell.execute_reply.started":"2024-05-19T08:12:14.454762Z","shell.execute_reply":"2024-05-19T08:12:25.324032Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"                                                X_test  \\\n1    [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n2    [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n3    [[tensor(2.0789e-06), tensor(-3.5355e-06), ten...   \n4    [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n5    [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n..                                                 ...   \n592  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n593  [[tensor(4.6664e-06), tensor(-2.2184e-05), ten...   \n594  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n595  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n596  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n\n                                      label       CER    WER  \\\n1         тапки не стоит оставлять на улице  0.090909  0.500   \n2                                    зверёк  0.166667  1.000   \n3                          любовь к природе  0.187500  1.000   \n4        вселенная бесконечна действительно  0.088235  1.000   \n5               поворот к парку развлечений  0.111111  0.500   \n..                                      ...       ...    ...   \n592       там начальство всё время меняется  0.030303  0.200   \n593  улыбка залог успеха в любых начинаниях  0.315789  1.000   \n594                  тонкие линии на бумаге  0.090909  0.750   \n595  по коже и волосам видно всё о человеке  0.105263  0.625   \n596                          темная комната  0.071429  0.500   \n\n                                     preds  \n1         тапке не стои оставлять на улицы  \n2                                   зверок  \n3                           любовьк приоды  \n4        вселенноя бесконечно дествительно  \n5               повародк парку развлечений  \n..                                     ...  \n592      там начальство все время меняется  \n593        улыка зелу узкеха влубхначнания  \n594                тонкие линие на бу маге  \n595  покоже и волосам видно все очело веке  \n596                         томная комната  \n\n[533 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X_test</th>\n      <th>label</th>\n      <th>CER</th>\n      <th>WER</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>тапки не стоит оставлять на улице</td>\n      <td>0.090909</td>\n      <td>0.500</td>\n      <td>тапке не стои оставлять на улицы</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>зверёк</td>\n      <td>0.166667</td>\n      <td>1.000</td>\n      <td>зверок</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[tensor(2.0789e-06), tensor(-3.5355e-06), ten...</td>\n      <td>любовь к природе</td>\n      <td>0.187500</td>\n      <td>1.000</td>\n      <td>любовьк приоды</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>вселенная бесконечна действительно</td>\n      <td>0.088235</td>\n      <td>1.000</td>\n      <td>вселенноя бесконечно дествительно</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>поворот к парку развлечений</td>\n      <td>0.111111</td>\n      <td>0.500</td>\n      <td>повародк парку развлечений</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>там начальство всё время меняется</td>\n      <td>0.030303</td>\n      <td>0.200</td>\n      <td>там начальство все время меняется</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>[[tensor(4.6664e-06), tensor(-2.2184e-05), ten...</td>\n      <td>улыбка залог успеха в любых начинаниях</td>\n      <td>0.315789</td>\n      <td>1.000</td>\n      <td>улыка зелу узкеха влубхначнания</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>тонкие линии на бумаге</td>\n      <td>0.090909</td>\n      <td>0.750</td>\n      <td>тонкие линие на бу маге</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>по коже и волосам видно всё о человеке</td>\n      <td>0.105263</td>\n      <td>0.625</td>\n      <td>покоже и волосам видно все очело веке</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>темная комната</td>\n      <td>0.071429</td>\n      <td>0.500</td>\n      <td>томная комната</td>\n    </tr>\n  </tbody>\n</table>\n<p>533 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"result_df = df_test.loc[df_test['CER'] > 0].drop(columns=['X_test', 'CER', 'WER'])\nresult_df","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:12:47.439095Z","iopub.execute_input":"2024-05-19T08:12:47.439539Z","iopub.status.idle":"2024-05-19T08:12:47.459972Z","shell.execute_reply.started":"2024-05-19T08:12:47.439491Z","shell.execute_reply":"2024-05-19T08:12:47.458572Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                      label  \\\n1         тапки не стоит оставлять на улице   \n2                                    зверёк   \n3                          любовь к природе   \n4        вселенная бесконечна действительно   \n5               поворот к парку развлечений   \n..                                      ...   \n592       там начальство всё время меняется   \n593  улыбка залог успеха в любых начинаниях   \n594                  тонкие линии на бумаге   \n595  по коже и волосам видно всё о человеке   \n596                          темная комната   \n\n                                     preds  \n1         тапке не стои оставлять на улицы  \n2                                   зверок  \n3                           любовьк приоды  \n4        вселенноя бесконечно дествительно  \n5               повародк парку развлечений  \n..                                     ...  \n592      там начальство все время меняется  \n593        улыка зелу узкеха влубхначнания  \n594                тонкие линие на бу маге  \n595  покоже и волосам видно все очело веке  \n596                         томная комната  \n\n[533 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>тапки не стоит оставлять на улице</td>\n      <td>тапке не стои оставлять на улицы</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>зверёк</td>\n      <td>зверок</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>любовь к природе</td>\n      <td>любовьк приоды</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>вселенная бесконечна действительно</td>\n      <td>вселенноя бесконечно дествительно</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>поворот к парку развлечений</td>\n      <td>повародк парку развлечений</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>там начальство всё время меняется</td>\n      <td>там начальство все время меняется</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>улыбка залог успеха в любых начинаниях</td>\n      <td>улыка зелу узкеха влубхначнания</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <td>тонкие линии на бумаге</td>\n      <td>тонкие линие на бу маге</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>по коже и волосам видно всё о человеке</td>\n      <td>покоже и волосам видно все очело веке</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>темная комната</td>\n      <td>томная комната</td>\n    </tr>\n  </tbody>\n</table>\n<p>533 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_errors = pd.read_csv('/kaggle/input/dop-test-files/errors.csv').drop(columns=['Unnamed: 0'])\ndf_errors","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:12:51.542241Z","iopub.execute_input":"2024-05-19T08:12:51.542672Z","iopub.status.idle":"2024-05-19T08:12:51.580888Z","shell.execute_reply.started":"2024-05-19T08:12:51.542632Z","shell.execute_reply":"2024-05-19T08:12:51.579325Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                             label  \\\n0                                иностранный агент   \n1            свидетельствуют о проблемах с печенью   \n2       найдите способ быть полезными другим людям   \n3                    я уже поставил белье в стирку   \n4                                      круглый мяч   \n...                                            ...   \n1347                          носки для альпинизма   \n1348                           устроился на работу   \n1349                                      мошенник   \n1350  мечты стали былью только если ты верил в них   \n1351                нам добавить ваше имя к номеру   \n\n                                            preds  \n0                                иностранный аген  \n1             свидетельствуют о праблемах спецнью  \n2      найдиче сьпособ быть полезном другим людем  \n3                     я уже поставил бельо встиру  \n4                                   круглый мядчь  \n...                                           ...  \n1347                         наски для ольтенизма  \n1348                          устроелся на работу  \n1349                                      мошеник  \n1350  мечты стале былью только е слеты верем вних  \n1351               т нам да бавить вышиими номеру  \n\n[1352 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>иностранный агент</td>\n      <td>иностранный аген</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>свидетельствуют о проблемах с печенью</td>\n      <td>свидетельствуют о праблемах спецнью</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>найдите способ быть полезными другим людям</td>\n      <td>найдиче сьпособ быть полезном другим людем</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>я уже поставил белье в стирку</td>\n      <td>я уже поставил бельо встиру</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>круглый мяч</td>\n      <td>круглый мядчь</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1347</th>\n      <td>носки для альпинизма</td>\n      <td>наски для ольтенизма</td>\n    </tr>\n    <tr>\n      <th>1348</th>\n      <td>устроился на работу</td>\n      <td>устроелся на работу</td>\n    </tr>\n    <tr>\n      <th>1349</th>\n      <td>мошенник</td>\n      <td>мошеник</td>\n    </tr>\n    <tr>\n      <th>1350</th>\n      <td>мечты стали былью только если ты верил в них</td>\n      <td>мечты стале былью только е слеты верем вних</td>\n    </tr>\n    <tr>\n      <th>1351</th>\n      <td>нам добавить ваше имя к номеру</td>\n      <td>т нам да бавить вышиими номеру</td>\n    </tr>\n  </tbody>\n</table>\n<p>1352 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_to_plus = pd.concat([df_errors, result_df])\ndf_to_plus","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:12:54.849917Z","iopub.execute_input":"2024-05-19T08:12:54.850493Z","iopub.status.idle":"2024-05-19T08:12:54.872143Z","shell.execute_reply.started":"2024-05-19T08:12:54.850436Z","shell.execute_reply":"2024-05-19T08:12:54.870728Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                          label  \\\n0                             иностранный агент   \n1         свидетельствуют о проблемах с печенью   \n2    найдите способ быть полезными другим людям   \n3                 я уже поставил белье в стирку   \n4                                   круглый мяч   \n..                                          ...   \n592           там начальство всё время меняется   \n593      улыбка залог успеха в любых начинаниях   \n594                      тонкие линии на бумаге   \n595      по коже и волосам видно всё о человеке   \n596                              темная комната   \n\n                                          preds  \n0                              иностранный аген  \n1           свидетельствуют о праблемах спецнью  \n2    найдиче сьпособ быть полезном другим людем  \n3                   я уже поставил бельо встиру  \n4                                 круглый мядчь  \n..                                          ...  \n592           там начальство все время меняется  \n593             улыка зелу узкеха влубхначнания  \n594                     тонкие линие на бу маге  \n595       покоже и волосам видно все очело веке  \n596                              томная комната  \n\n[1885 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>иностранный агент</td>\n      <td>иностранный аген</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>свидетельствуют о проблемах с печенью</td>\n      <td>свидетельствуют о праблемах спецнью</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>найдите способ быть полезными другим людям</td>\n      <td>найдиче сьпособ быть полезном другим людем</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>я уже поставил белье в стирку</td>\n      <td>я уже поставил бельо встиру</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>круглый мяч</td>\n      <td>круглый мядчь</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>там начальство всё время меняется</td>\n      <td>там начальство все время меняется</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>улыбка залог успеха в любых начинаниях</td>\n      <td>улыка зелу узкеха влубхначнания</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <td>тонкие линии на бумаге</td>\n      <td>тонкие линие на бу маге</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>по коже и волосам видно всё о человеке</td>\n      <td>покоже и волосам видно все очело веке</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>темная комната</td>\n      <td>томная комната</td>\n    </tr>\n  </tbody>\n</table>\n<p>1885 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_to_plus = df_to_plus.drop_duplicates(subset=['label'])\ndf_to_plus","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:12:58.740215Z","iopub.execute_input":"2024-05-19T08:12:58.740680Z","iopub.status.idle":"2024-05-19T08:12:58.763293Z","shell.execute_reply.started":"2024-05-19T08:12:58.740634Z","shell.execute_reply":"2024-05-19T08:12:58.761877Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                                          label  \\\n0                             иностранный агент   \n1         свидетельствуют о проблемах с печенью   \n2    найдите способ быть полезными другим людям   \n3                 я уже поставил белье в стирку   \n4                                   круглый мяч   \n..                                          ...   \n580                    летим в отпуск на гавайи   \n581                           кротовая настойка   \n583         думаю нам пора расходиться по домам   \n590                 красивые цветы украшают сад   \n591     кока кола зеро все еще такая же вкусная   \n\n                                          preds  \n0                              иностранный аген  \n1           свидетельствуют о праблемах спецнью  \n2    найдиче сьпособ быть полезном другим людем  \n3                   я уже поставил бельо встиру  \n4                                 круглый мядчь  \n..                                          ...  \n580                     влетим в отпуск наговаи  \n581                          кротовая на стойка  \n583           домаю ном порарасходится по домам  \n590                 красивые цветы укрошают сад  \n591       укаколо зыру в сё ечу такаеже вкусное  \n\n[1480 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>иностранный агент</td>\n      <td>иностранный аген</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>свидетельствуют о проблемах с печенью</td>\n      <td>свидетельствуют о праблемах спецнью</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>найдите способ быть полезными другим людям</td>\n      <td>найдиче сьпособ быть полезном другим людем</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>я уже поставил белье в стирку</td>\n      <td>я уже поставил бельо встиру</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>круглый мяч</td>\n      <td>круглый мядчь</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>580</th>\n      <td>летим в отпуск на гавайи</td>\n      <td>влетим в отпуск наговаи</td>\n    </tr>\n    <tr>\n      <th>581</th>\n      <td>кротовая настойка</td>\n      <td>кротовая на стойка</td>\n    </tr>\n    <tr>\n      <th>583</th>\n      <td>думаю нам пора расходиться по домам</td>\n      <td>домаю ном порарасходится по домам</td>\n    </tr>\n    <tr>\n      <th>590</th>\n      <td>красивые цветы украшают сад</td>\n      <td>красивые цветы укрошают сад</td>\n    </tr>\n    <tr>\n      <th>591</th>\n      <td>кока кола зеро все еще такая же вкусная</td>\n      <td>укаколо зыру в сё ечу такаеже вкусное</td>\n    </tr>\n  </tbody>\n</table>\n<p>1480 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_to_plus.to_csv('/kaggle/working/errors.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:13:03.263221Z","iopub.execute_input":"2024-05-19T08:13:03.263731Z","iopub.status.idle":"2024-05-19T08:13:03.285946Z","shell.execute_reply.started":"2024-05-19T08:13:03.263668Z","shell.execute_reply":"2024-05-19T08:13:03.284451Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"result_df","metadata":{"execution":{"iopub.status.busy":"2024-05-18T12:56:35.244871Z","iopub.execute_input":"2024-05-18T12:56:35.245344Z","iopub.status.idle":"2024-05-18T12:56:35.260518Z","shell.execute_reply.started":"2024-05-18T12:56:35.245301Z","shell.execute_reply":"2024-05-18T12:56:35.259124Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"                                          label  \\\n5                             иностранный агент   \n8         свидетельствуют о проблемах с печенью   \n11   найдите способ быть полезными другим людям   \n32                я уже поставил белье в стирку   \n38                                  круглый мяч   \n..                                          ...   \n545        не могу позвонить у меня сел телефон   \n549               служба в армии дело серьезное   \n557             тебе точно больше нечего делать   \n568                              добиться целей   \n590              она бы выставила тебя за дверь   \n\n                                          preds  \n5                              иностранный аген  \n8           свидетельствуют о праблемах спецнью  \n11   найдиче сьпособ быть полезном другим людем  \n32                  я уже поставил бельо встиру  \n38                                круглый мядчь  \n..                                          ...  \n545       не могу позовонить у меня сел телефон  \n549                 хложба вармии делосерьязное  \n557             тебе точна больсе нечего делать  \n568                              добиться целий  \n590                 онавы выставела девя зядвер  \n\n[77 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>иностранный агент</td>\n      <td>иностранный аген</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>свидетельствуют о проблемах с печенью</td>\n      <td>свидетельствуют о праблемах спецнью</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>найдите способ быть полезными другим людям</td>\n      <td>найдиче сьпособ быть полезном другим людем</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>я уже поставил белье в стирку</td>\n      <td>я уже поставил бельо встиру</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>круглый мяч</td>\n      <td>круглый мядчь</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>545</th>\n      <td>не могу позвонить у меня сел телефон</td>\n      <td>не могу позовонить у меня сел телефон</td>\n    </tr>\n    <tr>\n      <th>549</th>\n      <td>служба в армии дело серьезное</td>\n      <td>хложба вармии делосерьязное</td>\n    </tr>\n    <tr>\n      <th>557</th>\n      <td>тебе точно больше нечего делать</td>\n      <td>тебе точна больсе нечего делать</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>добиться целей</td>\n      <td>добиться целий</td>\n    </tr>\n    <tr>\n      <th>590</th>\n      <td>она бы выставила тебя за дверь</td>\n      <td>онавы выставела девя зядвер</td>\n    </tr>\n  </tbody>\n</table>\n<p>77 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#import os  \n#os.makedirs('folder/subfolder', exist_ok=True)  \nresult_df.to_csv('/kaggle/working/errors.csv')  ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T12:56:48.533237Z","iopub.execute_input":"2024-05-18T12:56:48.533659Z","iopub.status.idle":"2024-05-18T12:56:48.545875Z","shell.execute_reply.started":"2024-05-18T12:56:48.533622Z","shell.execute_reply":"2024-05-18T12:56:48.544724Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"#df_test = df_test.drop(columns=['X_test', 'CER', 'WER'])","metadata":{"execution":{"iopub.status.busy":"2024-05-18T12:29:37.051455Z","iopub.execute_input":"2024-05-18T12:29:37.052149Z","iopub.status.idle":"2024-05-18T12:29:37.061377Z","shell.execute_reply.started":"2024-05-18T12:29:37.052097Z","shell.execute_reply":"2024-05-18T12:29:37.059743Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2024-05-18T12:29:45.778953Z","iopub.execute_input":"2024-05-18T12:29:45.780288Z","iopub.status.idle":"2024-05-18T12:29:45.795418Z","shell.execute_reply.started":"2024-05-18T12:29:45.780236Z","shell.execute_reply":"2024-05-18T12:29:45.793899Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                                              label  \\\n0                                 прямая трансляция   \n1                    яркий цвет привлекает внимание   \n2                                      яркая краска   \n3                      поможешь мне с этим проектом   \n4                                 пальто нараспашку   \n..                                              ...   \n592  лист бумаги использован в принтере бедный лист   \n593                                   свежий аромат   \n594                                      расширения   \n595                                    имбирный чай   \n596                                        лабиринт   \n\n                                              preds  \n0                                 прямая трансляция  \n1                    яркий цвет привлекает внимание  \n2                                      яркая краска  \n3                      поможешь мне с этим проектом  \n4                                 пальто нараспашку  \n..                                              ...  \n592  лист бумаги использован в принтере бедный лист  \n593                                   свежий аромат  \n594                                      расширения  \n595                                    имбирный чай  \n596                                        лабиринт  \n\n[597 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>прямая трансляция</td>\n      <td>прямая трансляция</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>яркий цвет привлекает внимание</td>\n      <td>яркий цвет привлекает внимание</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>яркая краска</td>\n      <td>яркая краска</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>поможешь мне с этим проектом</td>\n      <td>поможешь мне с этим проектом</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>пальто нараспашку</td>\n      <td>пальто нараспашку</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>лист бумаги использован в принтере бедный лист</td>\n      <td>лист бумаги использован в принтере бедный лист</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>свежий аромат</td>\n      <td>свежий аромат</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <td>расширения</td>\n      <td>расширения</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>имбирный чай</td>\n      <td>имбирный чай</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>лабиринт</td>\n      <td>лабиринт</td>\n    </tr>\n  </tbody>\n</table>\n<p>597 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_test['CER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T06:17:52.619953Z","iopub.execute_input":"2024-05-19T06:17:52.621473Z","iopub.status.idle":"2024-05-19T06:17:52.632925Z","shell.execute_reply.started":"2024-05-19T06:17:52.621414Z","shell.execute_reply":"2024-05-19T06:17:52.631648Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"0.16802409682925326"},"metadata":{}}]},{"cell_type":"code","source":"df_test['WER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T06:17:53.403546Z","iopub.execute_input":"2024-05-19T06:17:53.403974Z","iopub.status.idle":"2024-05-19T06:17:53.413529Z","shell.execute_reply.started":"2024-05-19T06:17:53.403934Z","shell.execute_reply":"2024-05-19T06:17:53.412209Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"0.6618514651177968"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:26:24.339488Z","iopub.execute_input":"2023-05-24T10:26:24.340405Z","iopub.status.idle":"2023-05-24T10:26:24.350954Z","shell.execute_reply.started":"2023-05-24T10:26:24.340364Z","shell.execute_reply":"2023-05-24T10:26:24.349831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:16:11.401175Z","iopub.execute_input":"2023-05-13T16:16:11.401879Z","iopub.status.idle":"2023-05-13T16:16:11.430020Z","shell.execute_reply.started":"2023-05-13T16:16:11.401838Z","shell.execute_reply":"2023-05-13T16:16:11.428960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(directory):\n    total_duration = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            filepath = os.path.join(directory, filename)\n            with wave.open(filepath, 'r') as wav_file:\n                frames = wav_file.getnframes()\n                rate = wav_file.getframerate()\n                duration = frames / float(rate)\n                total_duration += duration\n    return total_duration\n\ndirectory = '/kaggle/input/upd-speech/mono_voice'\ntotal_duration = get_wav_duration(directory)\nprint('Total duration of WAV files:', total_duration, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:15.415086Z","iopub.execute_input":"2023-07-05T10:09:15.415876Z","iopub.status.idle":"2023-07-05T10:09:18.755936Z","shell.execute_reply.started":"2023-07-05T10:09:15.415836Z","shell.execute_reply":"2023-07-05T10:09:18.754693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return '{:02d}:{:02d}:{:02d}'.format(int(hours), int(minutes), int(seconds))\nseconds = 3661\nformatted_time = format_time(total_duration)\nprint(formatted_time)  # Output: '01:01:01'","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:23.353548Z","iopub.execute_input":"2023-07-05T10:09:23.354296Z","iopub.status.idle":"2023-07-05T10:09:23.361628Z","shell.execute_reply.started":"2023-07-05T10:09:23.354254Z","shell.execute_reply":"2023-07-05T10:09:23.360431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}