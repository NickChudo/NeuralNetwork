{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5321255,"sourceType":"datasetVersion","datasetId":3091651},{"sourceId":5618710,"sourceType":"datasetVersion","datasetId":3230790},{"sourceId":5677279,"sourceType":"datasetVersion","datasetId":2989949,"isSourceIdPinned":false},{"sourceId":5677298,"sourceType":"datasetVersion","datasetId":3213578,"isSourceIdPinned":false},{"sourceId":5677449,"sourceType":"datasetVersion","datasetId":3071831,"isSourceIdPinned":false},{"sourceId":5760288,"sourceType":"datasetVersion","datasetId":3311237},{"sourceId":7294569,"sourceType":"datasetVersion","datasetId":4230886}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np \nimport matplotlib\n\n\"\"\"Гипотезы которые будут проверяться: \n1)Дизбаланс классов мешает обучению (нейросеть думает что данные грязные)\n- исправляется только выравниваем количества записей в равных пропорциях \n2)Для таких данных нейросеть слишком слаба, надо тюнить параметры\n3)Неправильное разделение на тест/трэин, возможно дизбаланс не столь\nзначимый (75% - Борис, 25% - Карина), но в тестовой выборке \nсильный дизбаланс из за неправильного разделения \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:20:41.460145Z","iopub.execute_input":"2024-02-20T05:20:41.461138Z","iopub.status.idle":"2024-02-20T05:20:41.470858Z","shell.execute_reply.started":"2024-02-20T05:20:41.461098Z","shell.execute_reply":"2024-02-20T05:20:41.469564Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"'Гипотезы которые будут проверяться: \\n1)Дизбаланс классов мешает обучению (нейросеть думает что данные грязные)\\n- исправляется только выравниваем количества записей в равных пропорциях \\n2)Для таких данных нейросеть слишком слаба, надо тюнить параметры\\n3)Неправильное разделение на тест/трэин, возможно дизбаланс не столь\\nзначимый (75% - Борис, 25% - Карина), но в тестовой выборке \\nсильный дизбаланс из за неправильного разделения '"},"metadata":{}}]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        int_sequence = []\n        for c in text:\n            ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MFCC(n_mfcc=20)\n)\n\n\nvalid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//3)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=34, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:20:41.783654Z","iopub.execute_input":"2024-02-20T05:20:41.783986Z","iopub.status.idle":"2024-02-20T05:20:41.825002Z","shell.execute_reply.started":"2024-02-20T05:20:41.783956Z","shell.execute_reply":"2024-02-20T05:20:41.824012Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"class BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:20:42.243254Z","iopub.execute_input":"2024-02-20T05:20:42.243596Z","iopub.status.idle":"2024-02-20T05:20:42.251118Z","shell.execute_reply.started":"2024-02-20T05:20:42.243565Z","shell.execute_reply":"2024-02-20T05:20:42.250171Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"\"\"\"import pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/rus-speech/Speeches.xlsx')\ny = [sentence for sentence in file['Русская речь']]\n\ndir_name = \"/kaggle/input/upd-speech/mono_voice/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in range(1, 2001):\n    file_name = f'{e}.wav'\n    sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    X.append(torch.Tensor(sampl))\n    if i % 100 == 0:\n        print(i)\n    i += 1\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:20:42.652540Z","iopub.execute_input":"2024-02-20T05:20:42.653004Z","iopub.status.idle":"2024-02-20T05:20:42.661246Z","shell.execute_reply.started":"2024-02-20T05:20:42.652960Z","shell.execute_reply":"2024-02-20T05:20:42.660252Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"'import pandas as pd\\nimport librosa\\n\\nfile = pd.read_excel(\\'/kaggle/input/rus-speech/Speeches.xlsx\\')\\ny = [sentence for sentence in file[\\'Русская речь\\']]\\n\\ndir_name = \"/kaggle/input/upd-speech/mono_voice/\"\\nfiles_in_dir = os.listdir(dir_name)\\n\\nX = []\\ni = 1\\n\\nfor e in range(1, 2001):\\n    file_name = f\\'{e}.wav\\'\\n    sampl = librosa.load(dir_name + file_name, sr=16000)[0]\\n    sampl = sampl[np.newaxis, :]\\n    X.append(torch.Tensor(sampl))\\n    if i % 100 == 0:\\n        print(i)\\n    i += 1'"},"metadata":{}}]},{"cell_type":"code","source":"#Поменял там, где происходит загрузка, сохраняется id звукового файла, а потом в excel файле по колонке old_id ищется текст\n#И того звук и текст к нему\n\nimport pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/2700-audio/Speeches v1.xlsx')\n#y = [sentence for sentence in file['text']]\ny = []\ndir_name = \"/kaggle/input/2700-audio/Speeches/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in os.listdir(\"/kaggle/input/2700-audio/Speeches/\"):\n    file_name = e\n    for old_id in range(0, 2073):\n        if file_name.startswith(str(file['old_id'][old_id]) + '.'):\n            y.extend([''.join(file['text'][old_id])])\n            sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n            sampl = sampl[np.newaxis, :]\n            X.append(torch.Tensor(sampl))\n            break","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:20:43.088993Z","iopub.execute_input":"2024-02-20T05:20:43.089736Z","iopub.status.idle":"2024-02-20T05:21:43.588414Z","shell.execute_reply.started":"2024-02-20T05:20:43.089701Z","shell.execute_reply":"2024-02-20T05:21:43.587491Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import random\npairs = list(zip(X, y))\nrandom.shuffle(pairs)\nX, y = zip(*pairs)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:21:43.590043Z","iopub.execute_input":"2024-02-20T05:21:43.590359Z","iopub.status.idle":"2024-02-20T05:21:43.600603Z","shell.execute_reply.started":"2024-02-20T05:21:43.590329Z","shell.execute_reply":"2024-02-20T05:21:43.599633Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('4', 'четыре').replace('Р-220', 'р двести двадцать').replace('6', 'шесть').replace(\"-\", \" \")\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    sentence = \" \".join(sentence.split())\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:21:43.601834Z","iopub.execute_input":"2024-02-20T05:21:43.602168Z","iopub.status.idle":"2024-02-20T05:21:43.635166Z","shell.execute_reply.started":"2024-02-20T05:21:43.602121Z","shell.execute_reply":"2024-02-20T05:21:43.634360Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\"\"\"#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nX_train = X[:1800]\nX_test = X[1800:]\ny_train = y[:1800]\ny_test = y[1800:]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:21:43.637343Z","iopub.execute_input":"2024-02-20T05:21:43.637626Z","iopub.status.idle":"2024-02-20T05:21:43.643944Z","shell.execute_reply.started":"2024-02-20T05:21:43.637599Z","shell.execute_reply":"2024-02-20T05:21:43.642932Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"'#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\\nX_train = X[:1800]\\nX_test = X[1800:]\\ny_train = y[:1800]\\ny_test = y[1800:]'"},"metadata":{}}]},{"cell_type":"code","source":"X_train = X[:2430]\nX_test = X[2430:]\ny_train = y[:2430]\ny_test = y[2430:]","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:21:43.645255Z","iopub.execute_input":"2024-02-20T05:21:43.645698Z","iopub.status.idle":"2024-02-20T05:21:43.654039Z","shell.execute_reply.started":"2024-02-20T05:21:43.645660Z","shell.execute_reply":"2024-02-20T05:21:43.653193Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:21:43.655017Z","iopub.execute_input":"2024-02-20T05:21:43.655322Z","iopub.status.idle":"2024-02-20T05:21:43.665041Z","shell.execute_reply.started":"2024-02-20T05:21:43.655294Z","shell.execute_reply":"2024-02-20T05:21:43.663951Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 32, kernel_size=(4,4), stride=(3,3), padding=(2,2)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.Conv2d(128, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n        )\n        \n        self.fc_1 = nn.Sequential(\n            nn.Linear(896, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n        )\n        \n        self.BiGRU_1 = BidirectionalGRU(270, 270, 0, True)\n        self.BiGRU_2 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_3 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_4 = BidirectionalGRU(540, 270, 0.5, True)\n        \n        self.fc_2 = nn.Sequential(\n            nn.Linear(540, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.fc_1(x)\n        x = self.BiGRU_1(x)\n        x = self.BiGRU_2(x)\n        x = self.BiGRU_3(x)\n        x = self.BiGRU_4(x)\n        x = self.fc_2(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:30:03.458009Z","iopub.execute_input":"2024-02-20T05:30:03.458910Z","iopub.status.idle":"2024-02-20T05:30:03.475157Z","shell.execute_reply.started":"2024-02-20T05:30:03.458870Z","shell.execute_reply":"2024-02-20T05:30:03.474172Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        #decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        \"\"\"for j in range(len(decoded_preds)):\n            train_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n            train_wer.append(wer(decoded_targets[j], decoded_preds[j]))\"\"\"\n    \n    #avg_cer = sum(train_cer)/len(train_cer)\n    #avg_wer = sum(train_wer)/len(train_wer)\n    \n    \n            \n    #print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          #.format(train_loss, avg_cer, avg_wer, median_cer, median_wer))\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n            \n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n            \n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n   \n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model.pt')","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:30:03.741464Z","iopub.execute_input":"2024-02-20T05:30:03.741787Z","iopub.status.idle":"2024-02-20T05:30:03.769405Z","shell.execute_reply.started":"2024-02-20T05:30:03.741738Z","shell.execute_reply":"2024-02-20T05:30:03.768321Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"def predict(model, file_name, device):\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    sampl = librosa.load(file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    print(spectrogram_tensor.size())\n\n    with torch.no_grad():\n        spectrogram_tensor.to(device)\n        output = model(spectrogram_tensor)\n        print(output.size())\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-20T05:30:04.004122Z","iopub.execute_input":"2024-02-20T05:30:04.004874Z","iopub.status.idle":"2024-02-20T05:30:04.015595Z","shell.execute_reply.started":"2024-02-20T05:30:04.004818Z","shell.execute_reply":"2024-02-20T05:30:04.014531Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"%%time \nlearning_rate = 0.002\nbatch_size = 10\nepochs = 100\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-02-20T05:30:27.264965Z","iopub.execute_input":"2024-02-20T05:30:27.265359Z","iopub.status.idle":"2024-02-20T06:15:55.756786Z","shell.execute_reply.started":"2024-02-20T05:30:27.265322Z","shell.execute_reply":"2024-02-20T06:15:55.755566Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel1(\n  (conv): Sequential(\n    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Conv2d(1, 32, kernel_size=(4, 4), stride=(3, 3), padding=(2, 2))\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): GELU(approximate='none')\n    (4): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): GELU(approximate='none')\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): GELU(approximate='none')\n  )\n  (fc_1): Sequential(\n    (0): Linear(in_features=896, out_features=270, bias=True)\n    (1): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=270, out_features=270, bias=True)\n    (4): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (5): GELU(approximate='none')\n    (6): Linear(in_features=270, out_features=270, bias=True)\n    (7): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (8): GELU(approximate='none')\n  )\n  (BiGRU_1): BidirectionalGRU(\n    (BiGRU): GRU(270, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_2): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_3): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_4): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (fc_2): Sequential(\n    (0): Linear(in_features=540, out_features=35, bias=True)\n  )\n  (softmax): LogSoftmax(dim=2)\n)\nNum Model Parameters 5422923\nTrain Epoch: 1 [0/2430 (0%)]\tLoss: 17.365046\nTrain Epoch: 1 [200/2430 (8%)]\tLoss: 3.830300\nTrain Epoch: 1 [400/2430 (16%)]\tLoss: 3.573591\nTrain Epoch: 1 [600/2430 (25%)]\tLoss: 3.425484\nTrain Epoch: 1 [800/2430 (33%)]\tLoss: 3.325423\nTrain Epoch: 1 [1000/2430 (41%)]\tLoss: 3.286942\nTrain Epoch: 1 [1200/2430 (49%)]\tLoss: 3.198491\nTrain Epoch: 1 [1400/2430 (58%)]\tLoss: 3.214007\nTrain Epoch: 1 [1600/2430 (66%)]\tLoss: 3.279965\nTrain Epoch: 1 [1800/2430 (74%)]\tLoss: 3.325577\nTrain Epoch: 1 [2000/2430 (82%)]\tLoss: 3.300125\nTrain Epoch: 1 [2200/2430 (91%)]\tLoss: 3.365905\nTrain Epoch: 1 [2400/2430 (99%)]\tLoss: 3.315844\n\nevaluating...\nTest set:\tAverage loss: 3.2912, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 2 [0/2430 (0%)]\tLoss: 3.230373\nTrain Epoch: 2 [200/2430 (8%)]\tLoss: 3.206939\nTrain Epoch: 2 [400/2430 (16%)]\tLoss: 3.230489\nTrain Epoch: 2 [600/2430 (25%)]\tLoss: 3.283243\nTrain Epoch: 2 [800/2430 (33%)]\tLoss: 3.266942\nTrain Epoch: 2 [1000/2430 (41%)]\tLoss: 3.098680\nTrain Epoch: 2 [1200/2430 (49%)]\tLoss: 2.979209\nTrain Epoch: 2 [1400/2430 (58%)]\tLoss: 2.938733\nTrain Epoch: 2 [1600/2430 (66%)]\tLoss: 3.032553\nTrain Epoch: 2 [1800/2430 (74%)]\tLoss: 2.907489\nTrain Epoch: 2 [2000/2430 (82%)]\tLoss: 2.633358\nTrain Epoch: 2 [2200/2430 (91%)]\tLoss: 2.609658\nTrain Epoch: 2 [2400/2430 (99%)]\tLoss: 2.511271\n\nevaluating...\nTest set:\tAverage loss: 2.5266, Average CER: 0.995808 Average WER: 1.0000\n\nTrain Epoch: 3 [0/2430 (0%)]\tLoss: 2.338872\nTrain Epoch: 3 [200/2430 (8%)]\tLoss: 2.269269\nTrain Epoch: 3 [400/2430 (16%)]\tLoss: 2.413105\nTrain Epoch: 3 [600/2430 (25%)]\tLoss: 2.029725\nTrain Epoch: 3 [800/2430 (33%)]\tLoss: 2.357443\nTrain Epoch: 3 [1000/2430 (41%)]\tLoss: 2.032022\nTrain Epoch: 3 [1200/2430 (49%)]\tLoss: 2.047744\nTrain Epoch: 3 [1400/2430 (58%)]\tLoss: 1.571747\nTrain Epoch: 3 [1600/2430 (66%)]\tLoss: 1.828867\nTrain Epoch: 3 [1800/2430 (74%)]\tLoss: 1.566615\nTrain Epoch: 3 [2000/2430 (82%)]\tLoss: 1.366927\nTrain Epoch: 3 [2200/2430 (91%)]\tLoss: 1.510062\nTrain Epoch: 3 [2400/2430 (99%)]\tLoss: 1.447221\n\nevaluating...\nTest set:\tAverage loss: 1.4776, Average CER: 0.471480 Average WER: 0.9916\n\nTrain Epoch: 4 [0/2430 (0%)]\tLoss: 1.348536\nTrain Epoch: 4 [200/2430 (8%)]\tLoss: 1.547343\nTrain Epoch: 4 [400/2430 (16%)]\tLoss: 1.464268\nTrain Epoch: 4 [600/2430 (25%)]\tLoss: 1.342020\nTrain Epoch: 4 [800/2430 (33%)]\tLoss: 1.384625\nTrain Epoch: 4 [1000/2430 (41%)]\tLoss: 1.195585\nTrain Epoch: 4 [1200/2430 (49%)]\tLoss: 1.214402\nTrain Epoch: 4 [1400/2430 (58%)]\tLoss: 1.286022\nTrain Epoch: 4 [1600/2430 (66%)]\tLoss: 1.250098\nTrain Epoch: 4 [1800/2430 (74%)]\tLoss: 1.178397\nTrain Epoch: 4 [2000/2430 (82%)]\tLoss: 1.415861\nTrain Epoch: 4 [2200/2430 (91%)]\tLoss: 0.977480\nTrain Epoch: 4 [2400/2430 (99%)]\tLoss: 1.139172\n\nevaluating...\nTest set:\tAverage loss: 1.0982, Average CER: 0.355649 Average WER: 0.9486\n\nTrain Epoch: 5 [0/2430 (0%)]\tLoss: 0.966457\nTrain Epoch: 5 [200/2430 (8%)]\tLoss: 0.977261\nTrain Epoch: 5 [400/2430 (16%)]\tLoss: 1.108791\nTrain Epoch: 5 [600/2430 (25%)]\tLoss: 0.983026\nTrain Epoch: 5 [800/2430 (33%)]\tLoss: 0.961901\nTrain Epoch: 5 [1000/2430 (41%)]\tLoss: 0.816781\nTrain Epoch: 5 [1200/2430 (49%)]\tLoss: 0.809500\nTrain Epoch: 5 [1400/2430 (58%)]\tLoss: 0.888754\nTrain Epoch: 5 [1600/2430 (66%)]\tLoss: 0.786212\nTrain Epoch: 5 [1800/2430 (74%)]\tLoss: 0.623593\nTrain Epoch: 5 [2000/2430 (82%)]\tLoss: 0.811856\nTrain Epoch: 5 [2200/2430 (91%)]\tLoss: 0.790857\nTrain Epoch: 5 [2400/2430 (99%)]\tLoss: 0.728101\n\nevaluating...\nTest set:\tAverage loss: 0.9934, Average CER: 0.320276 Average WER: 0.9198\n\nTrain Epoch: 6 [0/2430 (0%)]\tLoss: 0.759972\nTrain Epoch: 6 [200/2430 (8%)]\tLoss: 0.875388\nTrain Epoch: 6 [400/2430 (16%)]\tLoss: 0.849522\nTrain Epoch: 6 [600/2430 (25%)]\tLoss: 0.976579\nTrain Epoch: 6 [800/2430 (33%)]\tLoss: 0.893092\nTrain Epoch: 6 [1000/2430 (41%)]\tLoss: 0.839820\nTrain Epoch: 6 [1200/2430 (49%)]\tLoss: 0.701398\nTrain Epoch: 6 [1400/2430 (58%)]\tLoss: 0.777455\nTrain Epoch: 6 [1600/2430 (66%)]\tLoss: 0.745355\nTrain Epoch: 6 [1800/2430 (74%)]\tLoss: 1.008129\nTrain Epoch: 6 [2000/2430 (82%)]\tLoss: 0.658334\nTrain Epoch: 6 [2200/2430 (91%)]\tLoss: 0.696072\nTrain Epoch: 6 [2400/2430 (99%)]\tLoss: 1.197861\n\nevaluating...\nTest set:\tAverage loss: 0.9665, Average CER: 0.304280 Average WER: 0.9169\n\nTrain Epoch: 7 [0/2430 (0%)]\tLoss: 0.789961\nTrain Epoch: 7 [200/2430 (8%)]\tLoss: 0.897000\nTrain Epoch: 7 [400/2430 (16%)]\tLoss: 0.787354\nTrain Epoch: 7 [600/2430 (25%)]\tLoss: 0.921425\nTrain Epoch: 7 [800/2430 (33%)]\tLoss: 0.593196\nTrain Epoch: 7 [1000/2430 (41%)]\tLoss: 0.717052\nTrain Epoch: 7 [1200/2430 (49%)]\tLoss: 0.672858\nTrain Epoch: 7 [1400/2430 (58%)]\tLoss: 0.624022\nTrain Epoch: 7 [1600/2430 (66%)]\tLoss: 0.586924\nTrain Epoch: 7 [1800/2430 (74%)]\tLoss: 0.646136\nTrain Epoch: 7 [2000/2430 (82%)]\tLoss: 0.720735\nTrain Epoch: 7 [2200/2430 (91%)]\tLoss: 0.533205\nTrain Epoch: 7 [2400/2430 (99%)]\tLoss: 0.729848\n\nevaluating...\nTest set:\tAverage loss: 0.8225, Average CER: 0.265747 Average WER: 0.8646\n\nTrain Epoch: 8 [0/2430 (0%)]\tLoss: 0.541983\nTrain Epoch: 8 [200/2430 (8%)]\tLoss: 0.668301\nTrain Epoch: 8 [400/2430 (16%)]\tLoss: 0.746854\nTrain Epoch: 8 [600/2430 (25%)]\tLoss: 0.689978\nTrain Epoch: 8 [800/2430 (33%)]\tLoss: 0.567440\nTrain Epoch: 8 [1000/2430 (41%)]\tLoss: 0.665578\nTrain Epoch: 8 [1200/2430 (49%)]\tLoss: 0.747632\nTrain Epoch: 8 [1400/2430 (58%)]\tLoss: 0.561554\nTrain Epoch: 8 [1600/2430 (66%)]\tLoss: 0.703916\nTrain Epoch: 8 [1800/2430 (74%)]\tLoss: 0.730517\nTrain Epoch: 8 [2000/2430 (82%)]\tLoss: 0.839420\nTrain Epoch: 8 [2200/2430 (91%)]\tLoss: 0.591940\nTrain Epoch: 8 [2400/2430 (99%)]\tLoss: 0.665594\n\nevaluating...\nTest set:\tAverage loss: 0.8677, Average CER: 0.267458 Average WER: 0.8835\n\nTrain Epoch: 9 [0/2430 (0%)]\tLoss: 0.757966\nTrain Epoch: 9 [200/2430 (8%)]\tLoss: 0.466265\nTrain Epoch: 9 [400/2430 (16%)]\tLoss: 0.471895\nTrain Epoch: 9 [600/2430 (25%)]\tLoss: 0.541190\nTrain Epoch: 9 [800/2430 (33%)]\tLoss: 0.904237\nTrain Epoch: 9 [1000/2430 (41%)]\tLoss: 0.672601\nTrain Epoch: 9 [1200/2430 (49%)]\tLoss: 0.772985\nTrain Epoch: 9 [1400/2430 (58%)]\tLoss: 0.535173\nTrain Epoch: 9 [1600/2430 (66%)]\tLoss: 0.771355\nTrain Epoch: 9 [1800/2430 (74%)]\tLoss: 0.622629\nTrain Epoch: 9 [2000/2430 (82%)]\tLoss: 0.620680\nTrain Epoch: 9 [2200/2430 (91%)]\tLoss: 0.697040\nTrain Epoch: 9 [2400/2430 (99%)]\tLoss: 0.601268\n\nevaluating...\nTest set:\tAverage loss: 0.8555, Average CER: 0.262648 Average WER: 0.8564\n\nTrain Epoch: 10 [0/2430 (0%)]\tLoss: 0.455571\nTrain Epoch: 10 [200/2430 (8%)]\tLoss: 0.352638\nTrain Epoch: 10 [400/2430 (16%)]\tLoss: 0.543264\nTrain Epoch: 10 [600/2430 (25%)]\tLoss: 0.726819\nTrain Epoch: 10 [800/2430 (33%)]\tLoss: 0.335388\nTrain Epoch: 10 [1000/2430 (41%)]\tLoss: 0.671840\nTrain Epoch: 10 [1200/2430 (49%)]\tLoss: 0.523624\nTrain Epoch: 10 [1400/2430 (58%)]\tLoss: 0.536727\nTrain Epoch: 10 [1600/2430 (66%)]\tLoss: 0.620117\nTrain Epoch: 10 [1800/2430 (74%)]\tLoss: 0.651708\nTrain Epoch: 10 [2000/2430 (82%)]\tLoss: 0.471517\nTrain Epoch: 10 [2200/2430 (91%)]\tLoss: 0.444758\nTrain Epoch: 10 [2400/2430 (99%)]\tLoss: 0.518905\n\nevaluating...\nTest set:\tAverage loss: 0.8600, Average CER: 0.259239 Average WER: 0.8686\n\nTrain Epoch: 11 [0/2430 (0%)]\tLoss: 0.422385\nTrain Epoch: 11 [200/2430 (8%)]\tLoss: 0.538461\nTrain Epoch: 11 [400/2430 (16%)]\tLoss: 0.804835\nTrain Epoch: 11 [600/2430 (25%)]\tLoss: 0.376248\nTrain Epoch: 11 [800/2430 (33%)]\tLoss: 0.481823\nTrain Epoch: 11 [1000/2430 (41%)]\tLoss: 0.628263\nTrain Epoch: 11 [1200/2430 (49%)]\tLoss: 0.728060\nTrain Epoch: 11 [1400/2430 (58%)]\tLoss: 0.618886\nTrain Epoch: 11 [1600/2430 (66%)]\tLoss: 0.434810\nTrain Epoch: 11 [1800/2430 (74%)]\tLoss: 0.553381\nTrain Epoch: 11 [2000/2430 (82%)]\tLoss: 0.357771\nTrain Epoch: 11 [2200/2430 (91%)]\tLoss: 0.590333\nTrain Epoch: 11 [2400/2430 (99%)]\tLoss: 0.713621\n\nevaluating...\nTest set:\tAverage loss: 0.8975, Average CER: 0.262794 Average WER: 0.8410\n\nTrain Epoch: 12 [0/2430 (0%)]\tLoss: 0.515010\nTrain Epoch: 12 [200/2430 (8%)]\tLoss: 0.606066\nTrain Epoch: 12 [400/2430 (16%)]\tLoss: 0.628109\nTrain Epoch: 12 [600/2430 (25%)]\tLoss: 0.508224\nTrain Epoch: 12 [800/2430 (33%)]\tLoss: 0.432435\nTrain Epoch: 12 [1000/2430 (41%)]\tLoss: 0.721461\nTrain Epoch: 12 [1200/2430 (49%)]\tLoss: 0.633042\nTrain Epoch: 12 [1400/2430 (58%)]\tLoss: 0.366136\nTrain Epoch: 12 [1600/2430 (66%)]\tLoss: 0.491160\nTrain Epoch: 12 [1800/2430 (74%)]\tLoss: 0.639115\nTrain Epoch: 12 [2000/2430 (82%)]\tLoss: 0.553256\nTrain Epoch: 12 [2200/2430 (91%)]\tLoss: 0.342297\nTrain Epoch: 12 [2400/2430 (99%)]\tLoss: 0.586724\n\nevaluating...\nTest set:\tAverage loss: 0.8121, Average CER: 0.241137 Average WER: 0.8357\n\nTrain Epoch: 13 [0/2430 (0%)]\tLoss: 0.373302\nTrain Epoch: 13 [200/2430 (8%)]\tLoss: 0.383233\nTrain Epoch: 13 [400/2430 (16%)]\tLoss: 0.467395\nTrain Epoch: 13 [600/2430 (25%)]\tLoss: 0.464658\nTrain Epoch: 13 [800/2430 (33%)]\tLoss: 0.452017\nTrain Epoch: 13 [1000/2430 (41%)]\tLoss: 0.541717\nTrain Epoch: 13 [1200/2430 (49%)]\tLoss: 0.515488\nTrain Epoch: 13 [1400/2430 (58%)]\tLoss: 0.558052\nTrain Epoch: 13 [1600/2430 (66%)]\tLoss: 0.985080\nTrain Epoch: 13 [1800/2430 (74%)]\tLoss: 0.608541\nTrain Epoch: 13 [2000/2430 (82%)]\tLoss: 0.667968\nTrain Epoch: 13 [2200/2430 (91%)]\tLoss: 0.483869\nTrain Epoch: 13 [2400/2430 (99%)]\tLoss: 0.516176\n\nevaluating...\nTest set:\tAverage loss: 0.8116, Average CER: 0.224432 Average WER: 0.8280\n\nTrain Epoch: 14 [0/2430 (0%)]\tLoss: 0.383194\nTrain Epoch: 14 [200/2430 (8%)]\tLoss: 0.351529\nTrain Epoch: 14 [400/2430 (16%)]\tLoss: 0.529194\nTrain Epoch: 14 [600/2430 (25%)]\tLoss: 0.325119\nTrain Epoch: 14 [800/2430 (33%)]\tLoss: 0.416541\nTrain Epoch: 14 [1000/2430 (41%)]\tLoss: 0.349667\nTrain Epoch: 14 [1200/2430 (49%)]\tLoss: 0.342228\nTrain Epoch: 14 [1400/2430 (58%)]\tLoss: 0.308735\nTrain Epoch: 14 [1600/2430 (66%)]\tLoss: 0.377348\nTrain Epoch: 14 [1800/2430 (74%)]\tLoss: 0.693519\nTrain Epoch: 14 [2000/2430 (82%)]\tLoss: 0.365377\nTrain Epoch: 14 [2200/2430 (91%)]\tLoss: 0.460319\nTrain Epoch: 14 [2400/2430 (99%)]\tLoss: 1.085014\n\nevaluating...\nTest set:\tAverage loss: 0.7702, Average CER: 0.225232 Average WER: 0.8081\n\nTrain Epoch: 15 [0/2430 (0%)]\tLoss: 0.286588\nTrain Epoch: 15 [200/2430 (8%)]\tLoss: 0.270022\nTrain Epoch: 15 [400/2430 (16%)]\tLoss: 0.184713\nTrain Epoch: 15 [600/2430 (25%)]\tLoss: 0.390093\nTrain Epoch: 15 [800/2430 (33%)]\tLoss: 0.456298\nTrain Epoch: 15 [1000/2430 (41%)]\tLoss: 0.543799\nTrain Epoch: 15 [1200/2430 (49%)]\tLoss: 0.437272\nTrain Epoch: 15 [1400/2430 (58%)]\tLoss: 0.409712\nTrain Epoch: 15 [1600/2430 (66%)]\tLoss: 0.474348\nTrain Epoch: 15 [1800/2430 (74%)]\tLoss: 0.614950\nTrain Epoch: 15 [2000/2430 (82%)]\tLoss: 0.472005\nTrain Epoch: 15 [2200/2430 (91%)]\tLoss: 0.764622\nTrain Epoch: 15 [2400/2430 (99%)]\tLoss: 0.506996\n\nevaluating...\nTest set:\tAverage loss: 0.8276, Average CER: 0.238386 Average WER: 0.8309\n\nTrain Epoch: 16 [0/2430 (0%)]\tLoss: 0.632103\nTrain Epoch: 16 [200/2430 (8%)]\tLoss: 0.476465\nTrain Epoch: 16 [400/2430 (16%)]\tLoss: 0.226223\nTrain Epoch: 16 [600/2430 (25%)]\tLoss: 0.195004\nTrain Epoch: 16 [800/2430 (33%)]\tLoss: 0.308143\nTrain Epoch: 16 [1000/2430 (41%)]\tLoss: 0.414214\nTrain Epoch: 16 [1200/2430 (49%)]\tLoss: 0.439629\nTrain Epoch: 16 [1400/2430 (58%)]\tLoss: 0.363268\nTrain Epoch: 16 [1600/2430 (66%)]\tLoss: 0.412884\nTrain Epoch: 16 [1800/2430 (74%)]\tLoss: 0.509928\nTrain Epoch: 16 [2000/2430 (82%)]\tLoss: 0.370651\nTrain Epoch: 16 [2200/2430 (91%)]\tLoss: 0.423780\nTrain Epoch: 16 [2400/2430 (99%)]\tLoss: 0.409445\n\nevaluating...\nTest set:\tAverage loss: 0.8190, Average CER: 0.231356 Average WER: 0.7962\n\nTrain Epoch: 17 [0/2430 (0%)]\tLoss: 0.246862\nTrain Epoch: 17 [200/2430 (8%)]\tLoss: 0.624920\nTrain Epoch: 17 [400/2430 (16%)]\tLoss: 0.351887\nTrain Epoch: 17 [600/2430 (25%)]\tLoss: 0.311395\nTrain Epoch: 17 [800/2430 (33%)]\tLoss: 0.434164\nTrain Epoch: 17 [1000/2430 (41%)]\tLoss: 0.444367\nTrain Epoch: 17 [1200/2430 (49%)]\tLoss: 0.735244\nTrain Epoch: 17 [1400/2430 (58%)]\tLoss: 0.502217\nTrain Epoch: 17 [1600/2430 (66%)]\tLoss: 0.300235\nTrain Epoch: 17 [1800/2430 (74%)]\tLoss: 0.447213\nTrain Epoch: 17 [2000/2430 (82%)]\tLoss: 0.407646\nTrain Epoch: 17 [2200/2430 (91%)]\tLoss: 0.714348\nTrain Epoch: 17 [2400/2430 (99%)]\tLoss: 0.511596\n\nevaluating...\nTest set:\tAverage loss: 0.8799, Average CER: 0.243578 Average WER: 0.8256\n\nTrain Epoch: 18 [0/2430 (0%)]\tLoss: 0.566697\nTrain Epoch: 18 [200/2430 (8%)]\tLoss: 0.332694\nTrain Epoch: 18 [400/2430 (16%)]\tLoss: 0.417467\nTrain Epoch: 18 [600/2430 (25%)]\tLoss: 0.252941\nTrain Epoch: 18 [800/2430 (33%)]\tLoss: 0.430464\nTrain Epoch: 18 [1000/2430 (41%)]\tLoss: 0.295752\nTrain Epoch: 18 [1200/2430 (49%)]\tLoss: 0.339736\nTrain Epoch: 18 [1400/2430 (58%)]\tLoss: 0.433880\nTrain Epoch: 18 [1600/2430 (66%)]\tLoss: 0.311488\nTrain Epoch: 18 [1800/2430 (74%)]\tLoss: 0.436624\nTrain Epoch: 18 [2000/2430 (82%)]\tLoss: 0.409860\nTrain Epoch: 18 [2200/2430 (91%)]\tLoss: 0.337319\nTrain Epoch: 18 [2400/2430 (99%)]\tLoss: 0.422262\n\nevaluating...\nTest set:\tAverage loss: 0.7998, Average CER: 0.223073 Average WER: 0.7965\n\nTrain Epoch: 19 [0/2430 (0%)]\tLoss: 0.325361\nTrain Epoch: 19 [200/2430 (8%)]\tLoss: 0.515235\nTrain Epoch: 19 [400/2430 (16%)]\tLoss: 0.468719\nTrain Epoch: 19 [600/2430 (25%)]\tLoss: 0.383361\nTrain Epoch: 19 [800/2430 (33%)]\tLoss: 0.329874\nTrain Epoch: 19 [1000/2430 (41%)]\tLoss: 0.262721\nTrain Epoch: 19 [1200/2430 (49%)]\tLoss: 0.320827\nTrain Epoch: 19 [1400/2430 (58%)]\tLoss: 0.413558\nTrain Epoch: 19 [1600/2430 (66%)]\tLoss: 0.455588\nTrain Epoch: 19 [1800/2430 (74%)]\tLoss: 0.520479\nTrain Epoch: 19 [2000/2430 (82%)]\tLoss: 0.386045\nTrain Epoch: 19 [2200/2430 (91%)]\tLoss: 0.538922\nTrain Epoch: 19 [2400/2430 (99%)]\tLoss: 0.509797\n\nevaluating...\nTest set:\tAverage loss: 0.8796, Average CER: 0.243254 Average WER: 0.8213\n\nTrain Epoch: 20 [0/2430 (0%)]\tLoss: 0.448558\nTrain Epoch: 20 [200/2430 (8%)]\tLoss: 0.396222\nTrain Epoch: 20 [400/2430 (16%)]\tLoss: 0.289490\nTrain Epoch: 20 [600/2430 (25%)]\tLoss: 0.421660\nTrain Epoch: 20 [800/2430 (33%)]\tLoss: 0.292786\nTrain Epoch: 20 [1000/2430 (41%)]\tLoss: 0.380751\nTrain Epoch: 20 [1200/2430 (49%)]\tLoss: 0.377132\nTrain Epoch: 20 [1400/2430 (58%)]\tLoss: 0.410098\nTrain Epoch: 20 [1600/2430 (66%)]\tLoss: 0.320673\nTrain Epoch: 20 [1800/2430 (74%)]\tLoss: 0.422075\nTrain Epoch: 20 [2000/2430 (82%)]\tLoss: 0.457457\nTrain Epoch: 20 [2200/2430 (91%)]\tLoss: 0.440004\nTrain Epoch: 20 [2400/2430 (99%)]\tLoss: 0.488362\n\nevaluating...\nTest set:\tAverage loss: 0.7955, Average CER: 0.212986 Average WER: 0.7764\n\nTrain Epoch: 21 [0/2430 (0%)]\tLoss: 0.401591\nTrain Epoch: 21 [200/2430 (8%)]\tLoss: 0.400324\nTrain Epoch: 21 [400/2430 (16%)]\tLoss: 0.159426\nTrain Epoch: 21 [600/2430 (25%)]\tLoss: 0.444555\nTrain Epoch: 21 [800/2430 (33%)]\tLoss: 0.519641\nTrain Epoch: 21 [1000/2430 (41%)]\tLoss: 0.303344\nTrain Epoch: 21 [1200/2430 (49%)]\tLoss: 0.303372\nTrain Epoch: 21 [1400/2430 (58%)]\tLoss: 0.433471\nTrain Epoch: 21 [1600/2430 (66%)]\tLoss: 0.391426\nTrain Epoch: 21 [1800/2430 (74%)]\tLoss: 0.387229\nTrain Epoch: 21 [2000/2430 (82%)]\tLoss: 0.282742\nTrain Epoch: 21 [2200/2430 (91%)]\tLoss: 0.363325\nTrain Epoch: 21 [2400/2430 (99%)]\tLoss: 0.310463\n\nevaluating...\nTest set:\tAverage loss: 0.8760, Average CER: 0.234737 Average WER: 0.7939\n\nTrain Epoch: 22 [0/2430 (0%)]\tLoss: 0.393914\nTrain Epoch: 22 [200/2430 (8%)]\tLoss: 0.690073\nTrain Epoch: 22 [400/2430 (16%)]\tLoss: 0.421103\nTrain Epoch: 22 [600/2430 (25%)]\tLoss: 0.348800\nTrain Epoch: 22 [800/2430 (33%)]\tLoss: 0.275325\nTrain Epoch: 22 [1000/2430 (41%)]\tLoss: 0.464069\nTrain Epoch: 22 [1200/2430 (49%)]\tLoss: 0.349199\nTrain Epoch: 22 [1400/2430 (58%)]\tLoss: 0.362999\nTrain Epoch: 22 [1600/2430 (66%)]\tLoss: 0.381581\nTrain Epoch: 22 [1800/2430 (74%)]\tLoss: 0.363301\nTrain Epoch: 22 [2000/2430 (82%)]\tLoss: 0.585971\nTrain Epoch: 22 [2200/2430 (91%)]\tLoss: 0.374695\nTrain Epoch: 22 [2400/2430 (99%)]\tLoss: 0.789122\n\nevaluating...\nTest set:\tAverage loss: 0.9133, Average CER: 0.240629 Average WER: 0.7890\n\nTrain Epoch: 23 [0/2430 (0%)]\tLoss: 0.340908\nTrain Epoch: 23 [200/2430 (8%)]\tLoss: 0.326653\nTrain Epoch: 23 [400/2430 (16%)]\tLoss: 0.294250\nTrain Epoch: 23 [600/2430 (25%)]\tLoss: 0.618607\nTrain Epoch: 23 [800/2430 (33%)]\tLoss: 0.312665\nTrain Epoch: 23 [1000/2430 (41%)]\tLoss: 0.503295\nTrain Epoch: 23 [1200/2430 (49%)]\tLoss: 0.298562\nTrain Epoch: 23 [1400/2430 (58%)]\tLoss: 0.325218\nTrain Epoch: 23 [1600/2430 (66%)]\tLoss: 0.427532\nTrain Epoch: 23 [1800/2430 (74%)]\tLoss: 0.350282\nTrain Epoch: 23 [2000/2430 (82%)]\tLoss: 0.404813\nTrain Epoch: 23 [2200/2430 (91%)]\tLoss: 0.493256\nTrain Epoch: 23 [2400/2430 (99%)]\tLoss: 0.435556\n\nevaluating...\nTest set:\tAverage loss: 0.9384, Average CER: 0.255996 Average WER: 0.8590\n\nTrain Epoch: 24 [0/2430 (0%)]\tLoss: 0.555834\nTrain Epoch: 24 [200/2430 (8%)]\tLoss: 0.371337\nTrain Epoch: 24 [400/2430 (16%)]\tLoss: 0.294853\nTrain Epoch: 24 [600/2430 (25%)]\tLoss: 0.436694\nTrain Epoch: 24 [800/2430 (33%)]\tLoss: 0.345268\nTrain Epoch: 24 [1000/2430 (41%)]\tLoss: 0.430197\nTrain Epoch: 24 [1200/2430 (49%)]\tLoss: 0.310824\nTrain Epoch: 24 [1400/2430 (58%)]\tLoss: 0.292194\nTrain Epoch: 24 [1600/2430 (66%)]\tLoss: 0.244811\nTrain Epoch: 24 [1800/2430 (74%)]\tLoss: 0.451203\nTrain Epoch: 24 [2000/2430 (82%)]\tLoss: 0.404206\nTrain Epoch: 24 [2200/2430 (91%)]\tLoss: 0.294871\nTrain Epoch: 24 [2400/2430 (99%)]\tLoss: 0.366026\n\nevaluating...\nTest set:\tAverage loss: 0.8025, Average CER: 0.209168 Average WER: 0.7469\n\nTrain Epoch: 25 [0/2430 (0%)]\tLoss: 0.275821\nTrain Epoch: 25 [200/2430 (8%)]\tLoss: 0.373626\nTrain Epoch: 25 [400/2430 (16%)]\tLoss: 0.328207\nTrain Epoch: 25 [600/2430 (25%)]\tLoss: 0.301030\nTrain Epoch: 25 [800/2430 (33%)]\tLoss: 0.349420\nTrain Epoch: 25 [1000/2430 (41%)]\tLoss: 0.234495\nTrain Epoch: 25 [1200/2430 (49%)]\tLoss: 0.457305\nTrain Epoch: 25 [1400/2430 (58%)]\tLoss: 0.487820\nTrain Epoch: 25 [1600/2430 (66%)]\tLoss: 0.430257\nTrain Epoch: 25 [1800/2430 (74%)]\tLoss: 0.621556\nTrain Epoch: 25 [2000/2430 (82%)]\tLoss: 0.407202\nTrain Epoch: 25 [2200/2430 (91%)]\tLoss: 0.528885\nTrain Epoch: 25 [2400/2430 (99%)]\tLoss: 0.492264\n\nevaluating...\nTest set:\tAverage loss: 0.8525, Average CER: 0.223863 Average WER: 0.7886\n\nTrain Epoch: 26 [0/2430 (0%)]\tLoss: 0.405050\nTrain Epoch: 26 [200/2430 (8%)]\tLoss: 0.258077\nTrain Epoch: 26 [400/2430 (16%)]\tLoss: 0.239973\nTrain Epoch: 26 [600/2430 (25%)]\tLoss: 0.347356\nTrain Epoch: 26 [800/2430 (33%)]\tLoss: 0.326994\nTrain Epoch: 26 [1000/2430 (41%)]\tLoss: 0.405797\nTrain Epoch: 26 [1200/2430 (49%)]\tLoss: 0.533366\nTrain Epoch: 26 [1400/2430 (58%)]\tLoss: 0.635272\nTrain Epoch: 26 [1600/2430 (66%)]\tLoss: 0.311729\nTrain Epoch: 26 [1800/2430 (74%)]\tLoss: 0.380540\nTrain Epoch: 26 [2000/2430 (82%)]\tLoss: 0.330662\nTrain Epoch: 26 [2200/2430 (91%)]\tLoss: 0.269001\nTrain Epoch: 26 [2400/2430 (99%)]\tLoss: 0.404933\n\nevaluating...\nTest set:\tAverage loss: 0.8934, Average CER: 0.234308 Average WER: 0.8027\n\nTrain Epoch: 27 [0/2430 (0%)]\tLoss: 0.368121\nTrain Epoch: 27 [200/2430 (8%)]\tLoss: 0.193341\nTrain Epoch: 27 [400/2430 (16%)]\tLoss: 0.380897\nTrain Epoch: 27 [600/2430 (25%)]\tLoss: 0.637695\nTrain Epoch: 27 [800/2430 (33%)]\tLoss: 0.420746\nTrain Epoch: 27 [1000/2430 (41%)]\tLoss: 0.450104\nTrain Epoch: 27 [1200/2430 (49%)]\tLoss: 0.424111\nTrain Epoch: 27 [1400/2430 (58%)]\tLoss: 0.327875\nTrain Epoch: 27 [1600/2430 (66%)]\tLoss: 0.318338\nTrain Epoch: 27 [1800/2430 (74%)]\tLoss: 0.470276\nTrain Epoch: 27 [2000/2430 (82%)]\tLoss: 0.406518\nTrain Epoch: 27 [2200/2430 (91%)]\tLoss: 0.311820\nTrain Epoch: 27 [2400/2430 (99%)]\tLoss: 0.250184\n\nevaluating...\nTest set:\tAverage loss: 0.8250, Average CER: 0.213202 Average WER: 0.7803\n\nTrain Epoch: 28 [0/2430 (0%)]\tLoss: 0.254537\nTrain Epoch: 28 [200/2430 (8%)]\tLoss: 0.206395\nTrain Epoch: 28 [400/2430 (16%)]\tLoss: 0.187017\nTrain Epoch: 28 [600/2430 (25%)]\tLoss: 0.384291\nTrain Epoch: 28 [800/2430 (33%)]\tLoss: 0.413015\nTrain Epoch: 28 [1000/2430 (41%)]\tLoss: 0.568294\nTrain Epoch: 28 [1200/2430 (49%)]\tLoss: 0.368087\nTrain Epoch: 28 [1400/2430 (58%)]\tLoss: 0.355165\nTrain Epoch: 28 [1600/2430 (66%)]\tLoss: 0.227216\nTrain Epoch: 28 [1800/2430 (74%)]\tLoss: 0.374844\nTrain Epoch: 28 [2000/2430 (82%)]\tLoss: 0.507566\nTrain Epoch: 28 [2200/2430 (91%)]\tLoss: 0.564357\nTrain Epoch: 28 [2400/2430 (99%)]\tLoss: 0.526237\n\nevaluating...\nTest set:\tAverage loss: 0.9102, Average CER: 0.240733 Average WER: 0.8131\n\nTrain Epoch: 29 [0/2430 (0%)]\tLoss: 0.439016\nTrain Epoch: 29 [200/2430 (8%)]\tLoss: 0.479393\nTrain Epoch: 29 [400/2430 (16%)]\tLoss: 0.294986\nTrain Epoch: 29 [600/2430 (25%)]\tLoss: 0.283973\nTrain Epoch: 29 [800/2430 (33%)]\tLoss: 0.321444\nTrain Epoch: 29 [1000/2430 (41%)]\tLoss: 0.400448\nTrain Epoch: 29 [1200/2430 (49%)]\tLoss: 0.322660\nTrain Epoch: 29 [1400/2430 (58%)]\tLoss: 0.664302\nTrain Epoch: 29 [1600/2430 (66%)]\tLoss: 0.491443\nTrain Epoch: 29 [1800/2430 (74%)]\tLoss: 0.535216\nTrain Epoch: 29 [2000/2430 (82%)]\tLoss: 0.546088\nTrain Epoch: 29 [2200/2430 (91%)]\tLoss: 0.400674\nTrain Epoch: 29 [2400/2430 (99%)]\tLoss: 0.480974\n\nevaluating...\nTest set:\tAverage loss: 0.8369, Average CER: 0.217991 Average WER: 0.7656\n\nTrain Epoch: 30 [0/2430 (0%)]\tLoss: 0.348203\nTrain Epoch: 30 [200/2430 (8%)]\tLoss: 0.284917\nTrain Epoch: 30 [400/2430 (16%)]\tLoss: 0.330456\nTrain Epoch: 30 [600/2430 (25%)]\tLoss: 0.165072\nTrain Epoch: 30 [800/2430 (33%)]\tLoss: 0.194610\nTrain Epoch: 30 [1000/2430 (41%)]\tLoss: 0.510743\nTrain Epoch: 30 [1200/2430 (49%)]\tLoss: 0.420331\nTrain Epoch: 30 [1400/2430 (58%)]\tLoss: 0.349406\nTrain Epoch: 30 [1600/2430 (66%)]\tLoss: 0.238639\nTrain Epoch: 30 [1800/2430 (74%)]\tLoss: 0.204938\nTrain Epoch: 30 [2000/2430 (82%)]\tLoss: 0.589735\nTrain Epoch: 30 [2200/2430 (91%)]\tLoss: 0.393734\nTrain Epoch: 30 [2400/2430 (99%)]\tLoss: 0.188745\n\nevaluating...\nTest set:\tAverage loss: 0.8712, Average CER: 0.220498 Average WER: 0.7671\n\nTrain Epoch: 31 [0/2430 (0%)]\tLoss: 0.496109\nTrain Epoch: 31 [200/2430 (8%)]\tLoss: 0.221837\nTrain Epoch: 31 [400/2430 (16%)]\tLoss: 0.269311\nTrain Epoch: 31 [600/2430 (25%)]\tLoss: 0.261107\nTrain Epoch: 31 [800/2430 (33%)]\tLoss: 0.439572\nTrain Epoch: 31 [1000/2430 (41%)]\tLoss: 0.693772\nTrain Epoch: 31 [1200/2430 (49%)]\tLoss: 0.481010\nTrain Epoch: 31 [1400/2430 (58%)]\tLoss: 0.357765\nTrain Epoch: 31 [1600/2430 (66%)]\tLoss: 0.293358\nTrain Epoch: 31 [1800/2430 (74%)]\tLoss: 0.405375\nTrain Epoch: 31 [2000/2430 (82%)]\tLoss: 0.278305\nTrain Epoch: 31 [2200/2430 (91%)]\tLoss: 0.365918\nTrain Epoch: 31 [2400/2430 (99%)]\tLoss: 0.485568\n\nevaluating...\nTest set:\tAverage loss: 0.9179, Average CER: 0.230892 Average WER: 0.7719\n\nTrain Epoch: 32 [0/2430 (0%)]\tLoss: 0.716585\nTrain Epoch: 32 [200/2430 (8%)]\tLoss: 0.463009\nTrain Epoch: 32 [400/2430 (16%)]\tLoss: 0.377925\nTrain Epoch: 32 [600/2430 (25%)]\tLoss: 0.162754\nTrain Epoch: 32 [800/2430 (33%)]\tLoss: 0.456584\nTrain Epoch: 32 [1000/2430 (41%)]\tLoss: 0.192713\nTrain Epoch: 32 [1200/2430 (49%)]\tLoss: 0.371355\nTrain Epoch: 32 [1400/2430 (58%)]\tLoss: 0.106133\nTrain Epoch: 32 [1600/2430 (66%)]\tLoss: 0.415978\nTrain Epoch: 32 [1800/2430 (74%)]\tLoss: 0.200609\nTrain Epoch: 32 [2000/2430 (82%)]\tLoss: 0.332966\nTrain Epoch: 32 [2200/2430 (91%)]\tLoss: 0.351281\nTrain Epoch: 32 [2400/2430 (99%)]\tLoss: 0.559254\n\nevaluating...\nTest set:\tAverage loss: 0.8105, Average CER: 0.200263 Average WER: 0.7253\n\nTrain Epoch: 33 [0/2430 (0%)]\tLoss: 0.262905\nTrain Epoch: 33 [200/2430 (8%)]\tLoss: 0.205553\nTrain Epoch: 33 [400/2430 (16%)]\tLoss: 0.312284\nTrain Epoch: 33 [600/2430 (25%)]\tLoss: 0.156915\nTrain Epoch: 33 [800/2430 (33%)]\tLoss: 0.185661\nTrain Epoch: 33 [1000/2430 (41%)]\tLoss: 0.264226\nTrain Epoch: 33 [1200/2430 (49%)]\tLoss: 0.342023\nTrain Epoch: 33 [1400/2430 (58%)]\tLoss: 0.284682\nTrain Epoch: 33 [1600/2430 (66%)]\tLoss: 0.189092\nTrain Epoch: 33 [1800/2430 (74%)]\tLoss: 0.312814\nTrain Epoch: 33 [2000/2430 (82%)]\tLoss: 0.587312\nTrain Epoch: 33 [2200/2430 (91%)]\tLoss: 0.302895\nTrain Epoch: 33 [2400/2430 (99%)]\tLoss: 0.585686\n\nevaluating...\nTest set:\tAverage loss: 0.9647, Average CER: 0.233405 Average WER: 0.7816\n\nTrain Epoch: 34 [0/2430 (0%)]\tLoss: 0.348322\nTrain Epoch: 34 [200/2430 (8%)]\tLoss: 0.390674\nTrain Epoch: 34 [400/2430 (16%)]\tLoss: 0.196058\nTrain Epoch: 34 [600/2430 (25%)]\tLoss: 0.321614\nTrain Epoch: 34 [800/2430 (33%)]\tLoss: 0.137345\nTrain Epoch: 34 [1000/2430 (41%)]\tLoss: 0.434370\nTrain Epoch: 34 [1200/2430 (49%)]\tLoss: 0.428408\nTrain Epoch: 34 [1400/2430 (58%)]\tLoss: 0.569199\nTrain Epoch: 34 [1600/2430 (66%)]\tLoss: 0.381929\nTrain Epoch: 34 [1800/2430 (74%)]\tLoss: 0.388160\nTrain Epoch: 34 [2000/2430 (82%)]\tLoss: 0.401670\nTrain Epoch: 34 [2200/2430 (91%)]\tLoss: 0.250381\nTrain Epoch: 34 [2400/2430 (99%)]\tLoss: 0.412751\n\nevaluating...\nTest set:\tAverage loss: 0.8399, Average CER: 0.207573 Average WER: 0.7525\n\nTrain Epoch: 35 [0/2430 (0%)]\tLoss: 0.273075\nTrain Epoch: 35 [200/2430 (8%)]\tLoss: 0.387916\nTrain Epoch: 35 [400/2430 (16%)]\tLoss: 0.209158\nTrain Epoch: 35 [600/2430 (25%)]\tLoss: 0.211623\nTrain Epoch: 35 [800/2430 (33%)]\tLoss: 0.266021\nTrain Epoch: 35 [1000/2430 (41%)]\tLoss: 0.337149\nTrain Epoch: 35 [1200/2430 (49%)]\tLoss: 0.228273\nTrain Epoch: 35 [1400/2430 (58%)]\tLoss: 0.344695\nTrain Epoch: 35 [1600/2430 (66%)]\tLoss: 0.305960\nTrain Epoch: 35 [1800/2430 (74%)]\tLoss: 0.482667\nTrain Epoch: 35 [2000/2430 (82%)]\tLoss: 0.441102\nTrain Epoch: 35 [2200/2430 (91%)]\tLoss: 0.287286\nTrain Epoch: 35 [2400/2430 (99%)]\tLoss: 0.434260\n\nevaluating...\nTest set:\tAverage loss: 0.8234, Average CER: 0.202112 Average WER: 0.7412\n\nTrain Epoch: 36 [0/2430 (0%)]\tLoss: 0.366148\nTrain Epoch: 36 [200/2430 (8%)]\tLoss: 0.218246\nTrain Epoch: 36 [400/2430 (16%)]\tLoss: 0.332852\nTrain Epoch: 36 [600/2430 (25%)]\tLoss: 0.320000\nTrain Epoch: 36 [800/2430 (33%)]\tLoss: 0.271406\nTrain Epoch: 36 [1000/2430 (41%)]\tLoss: 0.166740\nTrain Epoch: 36 [1200/2430 (49%)]\tLoss: 0.265640\nTrain Epoch: 36 [1400/2430 (58%)]\tLoss: 0.202244\nTrain Epoch: 36 [1600/2430 (66%)]\tLoss: 0.263230\nTrain Epoch: 36 [1800/2430 (74%)]\tLoss: 0.299419\nTrain Epoch: 36 [2000/2430 (82%)]\tLoss: 0.246447\nTrain Epoch: 36 [2200/2430 (91%)]\tLoss: 0.268548\nTrain Epoch: 36 [2400/2430 (99%)]\tLoss: 0.368027\n\nevaluating...\nTest set:\tAverage loss: 0.8372, Average CER: 0.209039 Average WER: 0.7391\n\nTrain Epoch: 37 [0/2430 (0%)]\tLoss: 0.193853\nTrain Epoch: 37 [200/2430 (8%)]\tLoss: 0.160229\nTrain Epoch: 37 [400/2430 (16%)]\tLoss: 0.192597\nTrain Epoch: 37 [600/2430 (25%)]\tLoss: 0.062805\nTrain Epoch: 37 [800/2430 (33%)]\tLoss: 0.230312\nTrain Epoch: 37 [1000/2430 (41%)]\tLoss: 0.232301\nTrain Epoch: 37 [1200/2430 (49%)]\tLoss: 0.267213\nTrain Epoch: 37 [1400/2430 (58%)]\tLoss: 0.187318\nTrain Epoch: 37 [1600/2430 (66%)]\tLoss: 0.142815\nTrain Epoch: 37 [1800/2430 (74%)]\tLoss: 0.181023\nTrain Epoch: 37 [2000/2430 (82%)]\tLoss: 0.205508\nTrain Epoch: 37 [2200/2430 (91%)]\tLoss: 0.111827\nTrain Epoch: 37 [2400/2430 (99%)]\tLoss: 0.389379\n\nevaluating...\nTest set:\tAverage loss: 0.8499, Average CER: 0.203807 Average WER: 0.7318\n\nTrain Epoch: 38 [0/2430 (0%)]\tLoss: 0.154284\nTrain Epoch: 38 [200/2430 (8%)]\tLoss: 0.161100\nTrain Epoch: 38 [400/2430 (16%)]\tLoss: 0.399113\nTrain Epoch: 38 [600/2430 (25%)]\tLoss: 0.221663\nTrain Epoch: 38 [800/2430 (33%)]\tLoss: 0.305484\nTrain Epoch: 38 [1000/2430 (41%)]\tLoss: 0.383322\nTrain Epoch: 38 [1200/2430 (49%)]\tLoss: 0.322884\nTrain Epoch: 38 [1400/2430 (58%)]\tLoss: 0.167972\nTrain Epoch: 38 [1600/2430 (66%)]\tLoss: 0.406747\nTrain Epoch: 38 [1800/2430 (74%)]\tLoss: 0.199323\nTrain Epoch: 38 [2000/2430 (82%)]\tLoss: 0.471430\nTrain Epoch: 38 [2200/2430 (91%)]\tLoss: 0.206167\nTrain Epoch: 38 [2400/2430 (99%)]\tLoss: 0.281763\n\nevaluating...\nTest set:\tAverage loss: 0.7984, Average CER: 0.199087 Average WER: 0.7350\n\nTrain Epoch: 39 [0/2430 (0%)]\tLoss: 0.132730\nTrain Epoch: 39 [200/2430 (8%)]\tLoss: 0.098953\nTrain Epoch: 39 [400/2430 (16%)]\tLoss: 0.196014\nTrain Epoch: 39 [600/2430 (25%)]\tLoss: 0.082566\nTrain Epoch: 39 [800/2430 (33%)]\tLoss: 0.162564\nTrain Epoch: 39 [1000/2430 (41%)]\tLoss: 0.111307\nTrain Epoch: 39 [1200/2430 (49%)]\tLoss: 0.300187\nTrain Epoch: 39 [1400/2430 (58%)]\tLoss: 0.421051\nTrain Epoch: 39 [1600/2430 (66%)]\tLoss: 0.227433\nTrain Epoch: 39 [1800/2430 (74%)]\tLoss: 0.143913\nTrain Epoch: 39 [2000/2430 (82%)]\tLoss: 0.241410\nTrain Epoch: 39 [2200/2430 (91%)]\tLoss: 0.176013\nTrain Epoch: 39 [2400/2430 (99%)]\tLoss: 0.147572\n\nevaluating...\nTest set:\tAverage loss: 0.8364, Average CER: 0.193320 Average WER: 0.7016\n\nTrain Epoch: 40 [0/2430 (0%)]\tLoss: 0.174707\nTrain Epoch: 40 [200/2430 (8%)]\tLoss: 0.167713\nTrain Epoch: 40 [400/2430 (16%)]\tLoss: 0.147297\nTrain Epoch: 40 [600/2430 (25%)]\tLoss: 0.059836\nTrain Epoch: 40 [800/2430 (33%)]\tLoss: 0.135838\nTrain Epoch: 40 [1000/2430 (41%)]\tLoss: 0.226071\nTrain Epoch: 40 [1200/2430 (49%)]\tLoss: 0.105757\nTrain Epoch: 40 [1400/2430 (58%)]\tLoss: 0.191250\nTrain Epoch: 40 [1600/2430 (66%)]\tLoss: 0.161777\nTrain Epoch: 40 [1800/2430 (74%)]\tLoss: 0.413498\nTrain Epoch: 40 [2000/2430 (82%)]\tLoss: 0.114320\nTrain Epoch: 40 [2200/2430 (91%)]\tLoss: 0.273610\nTrain Epoch: 40 [2400/2430 (99%)]\tLoss: 0.202477\n\nevaluating...\nTest set:\tAverage loss: 0.8533, Average CER: 0.191259 Average WER: 0.7186\n\nTrain Epoch: 41 [0/2430 (0%)]\tLoss: 0.107731\nTrain Epoch: 41 [200/2430 (8%)]\tLoss: 0.122919\nTrain Epoch: 41 [400/2430 (16%)]\tLoss: 0.143862\nTrain Epoch: 41 [600/2430 (25%)]\tLoss: 0.201317\nTrain Epoch: 41 [800/2430 (33%)]\tLoss: 0.215918\nTrain Epoch: 41 [1000/2430 (41%)]\tLoss: 0.170217\nTrain Epoch: 41 [1200/2430 (49%)]\tLoss: 0.169858\nTrain Epoch: 41 [1400/2430 (58%)]\tLoss: 0.102312\nTrain Epoch: 41 [1600/2430 (66%)]\tLoss: 0.200405\nTrain Epoch: 41 [1800/2430 (74%)]\tLoss: 0.145788\nTrain Epoch: 41 [2000/2430 (82%)]\tLoss: 0.245329\nTrain Epoch: 41 [2200/2430 (91%)]\tLoss: 0.265212\nTrain Epoch: 41 [2400/2430 (99%)]\tLoss: 0.122341\n\nevaluating...\nTest set:\tAverage loss: 0.8529, Average CER: 0.203736 Average WER: 0.7538\n\nTrain Epoch: 42 [0/2430 (0%)]\tLoss: 0.154692\nTrain Epoch: 42 [200/2430 (8%)]\tLoss: 0.186638\nTrain Epoch: 42 [400/2430 (16%)]\tLoss: 0.162414\nTrain Epoch: 42 [600/2430 (25%)]\tLoss: 0.093098\nTrain Epoch: 42 [800/2430 (33%)]\tLoss: 0.180430\nTrain Epoch: 42 [1000/2430 (41%)]\tLoss: 0.223593\nTrain Epoch: 42 [1200/2430 (49%)]\tLoss: 0.201824\nTrain Epoch: 42 [1400/2430 (58%)]\tLoss: 0.080919\nTrain Epoch: 42 [1600/2430 (66%)]\tLoss: 0.229602\nTrain Epoch: 42 [1800/2430 (74%)]\tLoss: 0.254811\nTrain Epoch: 42 [2000/2430 (82%)]\tLoss: 0.303147\nTrain Epoch: 42 [2200/2430 (91%)]\tLoss: 0.220615\nTrain Epoch: 42 [2400/2430 (99%)]\tLoss: 0.188438\n\nevaluating...\nTest set:\tAverage loss: 0.8680, Average CER: 0.194981 Average WER: 0.7182\n\nTrain Epoch: 43 [0/2430 (0%)]\tLoss: 0.072582\nTrain Epoch: 43 [200/2430 (8%)]\tLoss: 0.102479\nTrain Epoch: 43 [400/2430 (16%)]\tLoss: 0.149488\nTrain Epoch: 43 [600/2430 (25%)]\tLoss: 0.284750\nTrain Epoch: 43 [800/2430 (33%)]\tLoss: 0.329446\nTrain Epoch: 43 [1000/2430 (41%)]\tLoss: 0.210414\nTrain Epoch: 43 [1200/2430 (49%)]\tLoss: 0.169832\nTrain Epoch: 43 [1400/2430 (58%)]\tLoss: 0.209072\nTrain Epoch: 43 [1600/2430 (66%)]\tLoss: 0.215430\nTrain Epoch: 43 [1800/2430 (74%)]\tLoss: 0.143488\nTrain Epoch: 43 [2000/2430 (82%)]\tLoss: 0.215983\nTrain Epoch: 43 [2200/2430 (91%)]\tLoss: 0.090840\nTrain Epoch: 43 [2400/2430 (99%)]\tLoss: 0.179331\n\nevaluating...\nTest set:\tAverage loss: 0.8617, Average CER: 0.190843 Average WER: 0.7038\n\nTrain Epoch: 44 [0/2430 (0%)]\tLoss: 0.135901\nTrain Epoch: 44 [200/2430 (8%)]\tLoss: 0.136961\nTrain Epoch: 44 [400/2430 (16%)]\tLoss: 0.217703\nTrain Epoch: 44 [600/2430 (25%)]\tLoss: 0.155839\nTrain Epoch: 44 [800/2430 (33%)]\tLoss: 0.184516\nTrain Epoch: 44 [1000/2430 (41%)]\tLoss: 0.188260\nTrain Epoch: 44 [1200/2430 (49%)]\tLoss: 0.147261\nTrain Epoch: 44 [1400/2430 (58%)]\tLoss: 0.172973\nTrain Epoch: 44 [1600/2430 (66%)]\tLoss: 0.123376\nTrain Epoch: 44 [1800/2430 (74%)]\tLoss: 0.210403\nTrain Epoch: 44 [2000/2430 (82%)]\tLoss: 0.117530\nTrain Epoch: 44 [2200/2430 (91%)]\tLoss: 0.207655\nTrain Epoch: 44 [2400/2430 (99%)]\tLoss: 0.210279\n\nevaluating...\nTest set:\tAverage loss: 0.8887, Average CER: 0.196394 Average WER: 0.7132\n\nTrain Epoch: 45 [0/2430 (0%)]\tLoss: 0.125932\nTrain Epoch: 45 [200/2430 (8%)]\tLoss: 0.074464\nTrain Epoch: 45 [400/2430 (16%)]\tLoss: 0.112299\nTrain Epoch: 45 [600/2430 (25%)]\tLoss: 0.119836\nTrain Epoch: 45 [800/2430 (33%)]\tLoss: 0.137400\nTrain Epoch: 45 [1000/2430 (41%)]\tLoss: 0.081308\nTrain Epoch: 45 [1200/2430 (49%)]\tLoss: 0.248097\nTrain Epoch: 45 [1400/2430 (58%)]\tLoss: 0.146863\nTrain Epoch: 45 [1600/2430 (66%)]\tLoss: 0.244495\nTrain Epoch: 45 [1800/2430 (74%)]\tLoss: 0.266633\nTrain Epoch: 45 [2000/2430 (82%)]\tLoss: 0.254597\nTrain Epoch: 45 [2200/2430 (91%)]\tLoss: 0.236351\nTrain Epoch: 45 [2400/2430 (99%)]\tLoss: 0.215598\n\nevaluating...\nTest set:\tAverage loss: 0.8697, Average CER: 0.196561 Average WER: 0.7173\n\nTrain Epoch: 46 [0/2430 (0%)]\tLoss: 0.107344\nTrain Epoch: 46 [200/2430 (8%)]\tLoss: 0.224989\nTrain Epoch: 46 [400/2430 (16%)]\tLoss: 0.193617\nTrain Epoch: 46 [600/2430 (25%)]\tLoss: 0.201973\nTrain Epoch: 46 [800/2430 (33%)]\tLoss: 0.165329\nTrain Epoch: 46 [1000/2430 (41%)]\tLoss: 0.221116\nTrain Epoch: 46 [1200/2430 (49%)]\tLoss: 0.171532\nTrain Epoch: 46 [1400/2430 (58%)]\tLoss: 0.121747\nTrain Epoch: 46 [1600/2430 (66%)]\tLoss: 0.153719\nTrain Epoch: 46 [1800/2430 (74%)]\tLoss: 0.262532\nTrain Epoch: 46 [2000/2430 (82%)]\tLoss: 0.214406\nTrain Epoch: 46 [2200/2430 (91%)]\tLoss: 0.095744\nTrain Epoch: 46 [2400/2430 (99%)]\tLoss: 0.211249\n\nevaluating...\nTest set:\tAverage loss: 0.9080, Average CER: 0.189406 Average WER: 0.7087\n\nTrain Epoch: 47 [0/2430 (0%)]\tLoss: 0.153375\nTrain Epoch: 47 [200/2430 (8%)]\tLoss: 0.118538\nTrain Epoch: 47 [400/2430 (16%)]\tLoss: 0.130112\nTrain Epoch: 47 [600/2430 (25%)]\tLoss: 0.076722\nTrain Epoch: 47 [800/2430 (33%)]\tLoss: 0.146463\nTrain Epoch: 47 [1000/2430 (41%)]\tLoss: 0.215350\nTrain Epoch: 47 [1200/2430 (49%)]\tLoss: 0.162893\nTrain Epoch: 47 [1400/2430 (58%)]\tLoss: 0.176462\nTrain Epoch: 47 [1600/2430 (66%)]\tLoss: 0.156288\nTrain Epoch: 47 [1800/2430 (74%)]\tLoss: 0.126584\nTrain Epoch: 47 [2000/2430 (82%)]\tLoss: 0.061663\nTrain Epoch: 47 [2200/2430 (91%)]\tLoss: 0.150669\nTrain Epoch: 47 [2400/2430 (99%)]\tLoss: 0.171143\n\nevaluating...\nTest set:\tAverage loss: 0.8524, Average CER: 0.174140 Average WER: 0.6685\n\nTrain Epoch: 48 [0/2430 (0%)]\tLoss: 0.036279\nTrain Epoch: 48 [200/2430 (8%)]\tLoss: 0.084001\nTrain Epoch: 48 [400/2430 (16%)]\tLoss: 0.040007\nTrain Epoch: 48 [600/2430 (25%)]\tLoss: 0.084131\nTrain Epoch: 48 [800/2430 (33%)]\tLoss: 0.096444\nTrain Epoch: 48 [1000/2430 (41%)]\tLoss: 0.132945\nTrain Epoch: 48 [1200/2430 (49%)]\tLoss: 0.046383\nTrain Epoch: 48 [1400/2430 (58%)]\tLoss: 0.124152\nTrain Epoch: 48 [1600/2430 (66%)]\tLoss: 0.106590\nTrain Epoch: 48 [1800/2430 (74%)]\tLoss: 0.071350\nTrain Epoch: 48 [2000/2430 (82%)]\tLoss: 0.208796\nTrain Epoch: 48 [2200/2430 (91%)]\tLoss: 0.078677\nTrain Epoch: 48 [2400/2430 (99%)]\tLoss: 0.063057\n\nevaluating...\nTest set:\tAverage loss: 0.9098, Average CER: 0.182490 Average WER: 0.6818\n\nTrain Epoch: 49 [0/2430 (0%)]\tLoss: 0.044737\nTrain Epoch: 49 [200/2430 (8%)]\tLoss: 0.083592\nTrain Epoch: 49 [400/2430 (16%)]\tLoss: 0.118475\nTrain Epoch: 49 [600/2430 (25%)]\tLoss: 0.065999\nTrain Epoch: 49 [800/2430 (33%)]\tLoss: 0.043396\nTrain Epoch: 49 [1000/2430 (41%)]\tLoss: 0.040030\nTrain Epoch: 49 [1200/2430 (49%)]\tLoss: 0.052791\nTrain Epoch: 49 [1400/2430 (58%)]\tLoss: 0.110536\nTrain Epoch: 49 [1600/2430 (66%)]\tLoss: 0.117564\nTrain Epoch: 49 [1800/2430 (74%)]\tLoss: 0.069784\nTrain Epoch: 49 [2000/2430 (82%)]\tLoss: 0.080011\nTrain Epoch: 49 [2200/2430 (91%)]\tLoss: 0.059831\nTrain Epoch: 49 [2400/2430 (99%)]\tLoss: 0.113205\n\nevaluating...\nTest set:\tAverage loss: 0.9597, Average CER: 0.181459 Average WER: 0.6968\n\nTrain Epoch: 50 [0/2430 (0%)]\tLoss: 0.079365\nTrain Epoch: 50 [200/2430 (8%)]\tLoss: 0.029708\nTrain Epoch: 50 [400/2430 (16%)]\tLoss: 0.062578\nTrain Epoch: 50 [600/2430 (25%)]\tLoss: 0.043313\nTrain Epoch: 50 [800/2430 (33%)]\tLoss: 0.022904\nTrain Epoch: 50 [1000/2430 (41%)]\tLoss: 0.073745\nTrain Epoch: 50 [1200/2430 (49%)]\tLoss: 0.090208\nTrain Epoch: 50 [1400/2430 (58%)]\tLoss: 0.059091\nTrain Epoch: 50 [1600/2430 (66%)]\tLoss: 0.182754\nTrain Epoch: 50 [1800/2430 (74%)]\tLoss: 0.161203\nTrain Epoch: 50 [2000/2430 (82%)]\tLoss: 0.208959\nTrain Epoch: 50 [2200/2430 (91%)]\tLoss: 0.184088\nTrain Epoch: 50 [2400/2430 (99%)]\tLoss: 0.127899\n\nevaluating...\nTest set:\tAverage loss: 1.0081, Average CER: 0.197982 Average WER: 0.7111\n\nTrain Epoch: 51 [0/2430 (0%)]\tLoss: 0.192116\nTrain Epoch: 51 [200/2430 (8%)]\tLoss: 0.211553\nTrain Epoch: 51 [400/2430 (16%)]\tLoss: 0.071614\nTrain Epoch: 51 [600/2430 (25%)]\tLoss: 0.138066\nTrain Epoch: 51 [800/2430 (33%)]\tLoss: 0.086704\nTrain Epoch: 51 [1000/2430 (41%)]\tLoss: 0.039989\nTrain Epoch: 51 [1200/2430 (49%)]\tLoss: 0.182853\nTrain Epoch: 51 [1400/2430 (58%)]\tLoss: 0.150776\nTrain Epoch: 51 [1600/2430 (66%)]\tLoss: 0.211414\nTrain Epoch: 51 [1800/2430 (74%)]\tLoss: 0.165610\nTrain Epoch: 51 [2000/2430 (82%)]\tLoss: 0.182816\nTrain Epoch: 51 [2200/2430 (91%)]\tLoss: 0.110418\nTrain Epoch: 51 [2400/2430 (99%)]\tLoss: 0.091116\n\nevaluating...\nTest set:\tAverage loss: 0.9832, Average CER: 0.191209 Average WER: 0.7033\n\nTrain Epoch: 52 [0/2430 (0%)]\tLoss: 0.092412\nTrain Epoch: 52 [200/2430 (8%)]\tLoss: 0.134989\nTrain Epoch: 52 [400/2430 (16%)]\tLoss: 0.219288\nTrain Epoch: 52 [600/2430 (25%)]\tLoss: 0.230196\nTrain Epoch: 52 [800/2430 (33%)]\tLoss: 0.172204\nTrain Epoch: 52 [1000/2430 (41%)]\tLoss: 0.080737\nTrain Epoch: 52 [1200/2430 (49%)]\tLoss: 0.219308\nTrain Epoch: 52 [1400/2430 (58%)]\tLoss: 0.146142\nTrain Epoch: 52 [1600/2430 (66%)]\tLoss: 0.240124\nTrain Epoch: 52 [1800/2430 (74%)]\tLoss: 0.228407\nTrain Epoch: 52 [2000/2430 (82%)]\tLoss: 0.155702\nTrain Epoch: 52 [2200/2430 (91%)]\tLoss: 0.107485\nTrain Epoch: 52 [2400/2430 (99%)]\tLoss: 0.108359\n\nevaluating...\nTest set:\tAverage loss: 0.9042, Average CER: 0.184563 Average WER: 0.7029\n\nTrain Epoch: 53 [0/2430 (0%)]\tLoss: 0.105354\nTrain Epoch: 53 [200/2430 (8%)]\tLoss: 0.179179\nTrain Epoch: 53 [400/2430 (16%)]\tLoss: 0.101371\nTrain Epoch: 53 [600/2430 (25%)]\tLoss: 0.159090\nTrain Epoch: 53 [800/2430 (33%)]\tLoss: 0.069701\nTrain Epoch: 53 [1000/2430 (41%)]\tLoss: 0.176840\nTrain Epoch: 53 [1200/2430 (49%)]\tLoss: 0.063069\nTrain Epoch: 53 [1400/2430 (58%)]\tLoss: 0.192119\nTrain Epoch: 53 [1600/2430 (66%)]\tLoss: 0.226096\nTrain Epoch: 53 [1800/2430 (74%)]\tLoss: 0.165291\nTrain Epoch: 53 [2000/2430 (82%)]\tLoss: 0.198807\nTrain Epoch: 53 [2200/2430 (91%)]\tLoss: 0.101672\nTrain Epoch: 53 [2400/2430 (99%)]\tLoss: 0.087546\n\nevaluating...\nTest set:\tAverage loss: 0.9682, Average CER: 0.199580 Average WER: 0.7295\n\nTrain Epoch: 54 [0/2430 (0%)]\tLoss: 0.139233\nTrain Epoch: 54 [200/2430 (8%)]\tLoss: 0.126412\nTrain Epoch: 54 [400/2430 (16%)]\tLoss: 0.083511\nTrain Epoch: 54 [600/2430 (25%)]\tLoss: 0.077712\nTrain Epoch: 54 [800/2430 (33%)]\tLoss: 0.047865\nTrain Epoch: 54 [1000/2430 (41%)]\tLoss: 0.114226\nTrain Epoch: 54 [1200/2430 (49%)]\tLoss: 0.110380\nTrain Epoch: 54 [1400/2430 (58%)]\tLoss: 0.116757\nTrain Epoch: 54 [1600/2430 (66%)]\tLoss: 0.133624\nTrain Epoch: 54 [1800/2430 (74%)]\tLoss: 0.146120\nTrain Epoch: 54 [2000/2430 (82%)]\tLoss: 0.098179\nTrain Epoch: 54 [2200/2430 (91%)]\tLoss: 0.094092\nTrain Epoch: 54 [2400/2430 (99%)]\tLoss: 0.194496\n\nevaluating...\nTest set:\tAverage loss: 0.9638, Average CER: 0.181764 Average WER: 0.6809\n\nTrain Epoch: 55 [0/2430 (0%)]\tLoss: 0.034418\nTrain Epoch: 55 [200/2430 (8%)]\tLoss: 0.107543\nTrain Epoch: 55 [400/2430 (16%)]\tLoss: 0.026192\nTrain Epoch: 55 [600/2430 (25%)]\tLoss: 0.083615\nTrain Epoch: 55 [800/2430 (33%)]\tLoss: 0.131635\nTrain Epoch: 55 [1000/2430 (41%)]\tLoss: 0.068344\nTrain Epoch: 55 [1200/2430 (49%)]\tLoss: 0.148024\nTrain Epoch: 55 [1400/2430 (58%)]\tLoss: 0.041113\nTrain Epoch: 55 [1600/2430 (66%)]\tLoss: 0.070513\nTrain Epoch: 55 [1800/2430 (74%)]\tLoss: 0.087525\nTrain Epoch: 55 [2000/2430 (82%)]\tLoss: 0.067969\nTrain Epoch: 55 [2200/2430 (91%)]\tLoss: 0.050607\nTrain Epoch: 55 [2400/2430 (99%)]\tLoss: 0.091375\n\nevaluating...\nTest set:\tAverage loss: 0.9010, Average CER: 0.169586 Average WER: 0.6686\n\nTrain Epoch: 56 [0/2430 (0%)]\tLoss: 0.043186\nTrain Epoch: 56 [200/2430 (8%)]\tLoss: 0.061297\nTrain Epoch: 56 [400/2430 (16%)]\tLoss: 0.022961\nTrain Epoch: 56 [600/2430 (25%)]\tLoss: 0.051619\nTrain Epoch: 56 [800/2430 (33%)]\tLoss: 0.063650\nTrain Epoch: 56 [1000/2430 (41%)]\tLoss: 0.035540\nTrain Epoch: 56 [1200/2430 (49%)]\tLoss: 0.046319\nTrain Epoch: 56 [1400/2430 (58%)]\tLoss: 0.061367\nTrain Epoch: 56 [1600/2430 (66%)]\tLoss: 0.040500\nTrain Epoch: 56 [1800/2430 (74%)]\tLoss: 0.028661\nTrain Epoch: 56 [2000/2430 (82%)]\tLoss: 0.051186\nTrain Epoch: 56 [2200/2430 (91%)]\tLoss: 0.071610\nTrain Epoch: 56 [2400/2430 (99%)]\tLoss: 0.024299\n\nevaluating...\nTest set:\tAverage loss: 0.9694, Average CER: 0.179743 Average WER: 0.6760\n\nTrain Epoch: 57 [0/2430 (0%)]\tLoss: 0.118093\nTrain Epoch: 57 [200/2430 (8%)]\tLoss: 0.047991\nTrain Epoch: 57 [400/2430 (16%)]\tLoss: 0.073310\nTrain Epoch: 57 [600/2430 (25%)]\tLoss: 0.096652\nTrain Epoch: 57 [800/2430 (33%)]\tLoss: 0.050838\nTrain Epoch: 57 [1000/2430 (41%)]\tLoss: 0.028816\nTrain Epoch: 57 [1200/2430 (49%)]\tLoss: 0.080071\nTrain Epoch: 57 [1400/2430 (58%)]\tLoss: 0.038244\nTrain Epoch: 57 [1600/2430 (66%)]\tLoss: 0.102797\nTrain Epoch: 57 [1800/2430 (74%)]\tLoss: 0.084425\nTrain Epoch: 57 [2000/2430 (82%)]\tLoss: 0.016609\nTrain Epoch: 57 [2200/2430 (91%)]\tLoss: 0.063957\nTrain Epoch: 57 [2400/2430 (99%)]\tLoss: 0.109537\n\nevaluating...\nTest set:\tAverage loss: 1.0711, Average CER: 0.184451 Average WER: 0.6697\n\nTrain Epoch: 58 [0/2430 (0%)]\tLoss: 0.048115\nTrain Epoch: 58 [200/2430 (8%)]\tLoss: 0.083738\nTrain Epoch: 58 [400/2430 (16%)]\tLoss: 0.136037\nTrain Epoch: 58 [600/2430 (25%)]\tLoss: 0.163369\nTrain Epoch: 58 [800/2430 (33%)]\tLoss: 0.071360\nTrain Epoch: 58 [1000/2430 (41%)]\tLoss: 0.021931\nTrain Epoch: 58 [1200/2430 (49%)]\tLoss: 0.023000\nTrain Epoch: 58 [1400/2430 (58%)]\tLoss: 0.094924\nTrain Epoch: 58 [1600/2430 (66%)]\tLoss: 0.107751\nTrain Epoch: 58 [1800/2430 (74%)]\tLoss: 0.101463\nTrain Epoch: 58 [2000/2430 (82%)]\tLoss: 0.088664\nTrain Epoch: 58 [2200/2430 (91%)]\tLoss: 0.153827\nTrain Epoch: 58 [2400/2430 (99%)]\tLoss: 0.096242\n\nevaluating...\nTest set:\tAverage loss: 1.0195, Average CER: 0.180923 Average WER: 0.6820\n\nTrain Epoch: 59 [0/2430 (0%)]\tLoss: 0.037455\nTrain Epoch: 59 [200/2430 (8%)]\tLoss: 0.012728\nTrain Epoch: 59 [400/2430 (16%)]\tLoss: 0.048927\nTrain Epoch: 59 [600/2430 (25%)]\tLoss: 0.063861\nTrain Epoch: 59 [800/2430 (33%)]\tLoss: 0.033595\nTrain Epoch: 59 [1000/2430 (41%)]\tLoss: 0.050414\nTrain Epoch: 59 [1200/2430 (49%)]\tLoss: 0.050921\nTrain Epoch: 59 [1400/2430 (58%)]\tLoss: 0.054236\nTrain Epoch: 59 [1600/2430 (66%)]\tLoss: 0.054333\nTrain Epoch: 59 [1800/2430 (74%)]\tLoss: 0.027395\nTrain Epoch: 59 [2000/2430 (82%)]\tLoss: 0.054910\nTrain Epoch: 59 [2200/2430 (91%)]\tLoss: 0.080733\nTrain Epoch: 59 [2400/2430 (99%)]\tLoss: 0.030791\n\nevaluating...\nTest set:\tAverage loss: 1.0126, Average CER: 0.182811 Average WER: 0.6804\n\nTrain Epoch: 60 [0/2430 (0%)]\tLoss: 0.025465\nTrain Epoch: 60 [200/2430 (8%)]\tLoss: 0.028158\nTrain Epoch: 60 [400/2430 (16%)]\tLoss: 0.047431\nTrain Epoch: 60 [600/2430 (25%)]\tLoss: 0.127275\nTrain Epoch: 60 [800/2430 (33%)]\tLoss: 0.045454\nTrain Epoch: 60 [1000/2430 (41%)]\tLoss: 0.044801\nTrain Epoch: 60 [1200/2430 (49%)]\tLoss: 0.035726\nTrain Epoch: 60 [1400/2430 (58%)]\tLoss: 0.057149\nTrain Epoch: 60 [1600/2430 (66%)]\tLoss: 0.072857\nTrain Epoch: 60 [1800/2430 (74%)]\tLoss: 0.072007\nTrain Epoch: 60 [2000/2430 (82%)]\tLoss: 0.188504\nTrain Epoch: 60 [2200/2430 (91%)]\tLoss: 0.052322\nTrain Epoch: 60 [2400/2430 (99%)]\tLoss: 0.021013\n\nevaluating...\nTest set:\tAverage loss: 1.0761, Average CER: 0.185310 Average WER: 0.7010\n\nTrain Epoch: 61 [0/2430 (0%)]\tLoss: 0.038341\nTrain Epoch: 61 [200/2430 (8%)]\tLoss: 0.042502\nTrain Epoch: 61 [400/2430 (16%)]\tLoss: 0.034632\nTrain Epoch: 61 [600/2430 (25%)]\tLoss: 0.133083\nTrain Epoch: 61 [800/2430 (33%)]\tLoss: 0.014780\nTrain Epoch: 61 [1000/2430 (41%)]\tLoss: 0.046497\nTrain Epoch: 61 [1200/2430 (49%)]\tLoss: 0.040894\nTrain Epoch: 61 [1400/2430 (58%)]\tLoss: 0.085470\nTrain Epoch: 61 [1600/2430 (66%)]\tLoss: 0.073457\nTrain Epoch: 61 [1800/2430 (74%)]\tLoss: 0.056948\nTrain Epoch: 61 [2000/2430 (82%)]\tLoss: 0.032678\nTrain Epoch: 61 [2200/2430 (91%)]\tLoss: 0.058707\nTrain Epoch: 61 [2400/2430 (99%)]\tLoss: 0.080540\n\nevaluating...\nTest set:\tAverage loss: 1.0797, Average CER: 0.182209 Average WER: 0.6616\n\nTrain Epoch: 62 [0/2430 (0%)]\tLoss: 0.009955\nTrain Epoch: 62 [200/2430 (8%)]\tLoss: 0.055525\nTrain Epoch: 62 [400/2430 (16%)]\tLoss: 0.093917\nTrain Epoch: 62 [600/2430 (25%)]\tLoss: 0.033065\nTrain Epoch: 62 [800/2430 (33%)]\tLoss: 0.041308\nTrain Epoch: 62 [1000/2430 (41%)]\tLoss: 0.038823\nTrain Epoch: 62 [1200/2430 (49%)]\tLoss: 0.027122\nTrain Epoch: 62 [1400/2430 (58%)]\tLoss: 0.039759\nTrain Epoch: 62 [1600/2430 (66%)]\tLoss: 0.040154\nTrain Epoch: 62 [1800/2430 (74%)]\tLoss: 0.037955\nTrain Epoch: 62 [2000/2430 (82%)]\tLoss: 0.076145\nTrain Epoch: 62 [2200/2430 (91%)]\tLoss: 0.084461\nTrain Epoch: 62 [2400/2430 (99%)]\tLoss: 0.105298\n\nevaluating...\nTest set:\tAverage loss: 1.0624, Average CER: 0.177426 Average WER: 0.6742\n\nTrain Epoch: 63 [0/2430 (0%)]\tLoss: 0.069174\nTrain Epoch: 63 [200/2430 (8%)]\tLoss: 0.039157\nTrain Epoch: 63 [400/2430 (16%)]\tLoss: 0.065855\nTrain Epoch: 63 [600/2430 (25%)]\tLoss: 0.036186\nTrain Epoch: 63 [800/2430 (33%)]\tLoss: 0.030975\nTrain Epoch: 63 [1000/2430 (41%)]\tLoss: 0.031927\nTrain Epoch: 63 [1200/2430 (49%)]\tLoss: 0.070847\nTrain Epoch: 63 [1400/2430 (58%)]\tLoss: 0.042240\nTrain Epoch: 63 [1600/2430 (66%)]\tLoss: 0.061850\nTrain Epoch: 63 [1800/2430 (74%)]\tLoss: 0.063886\nTrain Epoch: 63 [2000/2430 (82%)]\tLoss: 0.047296\nTrain Epoch: 63 [2200/2430 (91%)]\tLoss: 0.026228\nTrain Epoch: 63 [2400/2430 (99%)]\tLoss: 0.058451\n\nevaluating...\nTest set:\tAverage loss: 1.0641, Average CER: 0.182883 Average WER: 0.6827\n\nTrain Epoch: 64 [0/2430 (0%)]\tLoss: 0.075460\nTrain Epoch: 64 [200/2430 (8%)]\tLoss: 0.069999\nTrain Epoch: 64 [400/2430 (16%)]\tLoss: 0.043768\nTrain Epoch: 64 [600/2430 (25%)]\tLoss: 0.105088\nTrain Epoch: 64 [800/2430 (33%)]\tLoss: 0.041056\nTrain Epoch: 64 [1000/2430 (41%)]\tLoss: 0.085644\nTrain Epoch: 64 [1200/2430 (49%)]\tLoss: 0.119838\nTrain Epoch: 64 [1400/2430 (58%)]\tLoss: 0.040110\nTrain Epoch: 64 [1600/2430 (66%)]\tLoss: 0.135259\nTrain Epoch: 64 [1800/2430 (74%)]\tLoss: 0.071318\nTrain Epoch: 64 [2000/2430 (82%)]\tLoss: 0.105244\nTrain Epoch: 64 [2200/2430 (91%)]\tLoss: 0.042372\nTrain Epoch: 64 [2400/2430 (99%)]\tLoss: 0.072255\n\nevaluating...\nTest set:\tAverage loss: 1.0147, Average CER: 0.176550 Average WER: 0.6766\n\nTrain Epoch: 65 [0/2430 (0%)]\tLoss: 0.082545\nTrain Epoch: 65 [200/2430 (8%)]\tLoss: 0.083751\nTrain Epoch: 65 [400/2430 (16%)]\tLoss: 0.087301\nTrain Epoch: 65 [600/2430 (25%)]\tLoss: 0.019893\nTrain Epoch: 65 [800/2430 (33%)]\tLoss: 0.053607\nTrain Epoch: 65 [1000/2430 (41%)]\tLoss: 0.142448\nTrain Epoch: 65 [1200/2430 (49%)]\tLoss: 0.095904\nTrain Epoch: 65 [1400/2430 (58%)]\tLoss: 0.042004\nTrain Epoch: 65 [1600/2430 (66%)]\tLoss: 0.081622\nTrain Epoch: 65 [1800/2430 (74%)]\tLoss: 0.177098\nTrain Epoch: 65 [2000/2430 (82%)]\tLoss: 0.026073\nTrain Epoch: 65 [2200/2430 (91%)]\tLoss: 0.137577\nTrain Epoch: 65 [2400/2430 (99%)]\tLoss: 0.099046\n\nevaluating...\nTest set:\tAverage loss: 1.0962, Average CER: 0.180252 Average WER: 0.6819\n\nTrain Epoch: 66 [0/2430 (0%)]\tLoss: 0.029935\nTrain Epoch: 66 [200/2430 (8%)]\tLoss: 0.069485\nTrain Epoch: 66 [400/2430 (16%)]\tLoss: 0.087529\nTrain Epoch: 66 [600/2430 (25%)]\tLoss: 0.052842\nTrain Epoch: 66 [800/2430 (33%)]\tLoss: 0.036021\nTrain Epoch: 66 [1000/2430 (41%)]\tLoss: 0.107456\nTrain Epoch: 66 [1200/2430 (49%)]\tLoss: 0.051480\nTrain Epoch: 66 [1400/2430 (58%)]\tLoss: 0.045679\nTrain Epoch: 66 [1600/2430 (66%)]\tLoss: 0.068588\nTrain Epoch: 66 [1800/2430 (74%)]\tLoss: 0.058621\nTrain Epoch: 66 [2000/2430 (82%)]\tLoss: 0.031437\nTrain Epoch: 66 [2200/2430 (91%)]\tLoss: 0.047297\nTrain Epoch: 66 [2400/2430 (99%)]\tLoss: 0.032111\n\nevaluating...\nTest set:\tAverage loss: 1.0562, Average CER: 0.179078 Average WER: 0.6759\n\nTrain Epoch: 67 [0/2430 (0%)]\tLoss: 0.025417\nTrain Epoch: 67 [200/2430 (8%)]\tLoss: 0.047449\nTrain Epoch: 67 [400/2430 (16%)]\tLoss: 0.047035\nTrain Epoch: 67 [600/2430 (25%)]\tLoss: 0.024260\nTrain Epoch: 67 [800/2430 (33%)]\tLoss: 0.016877\nTrain Epoch: 67 [1000/2430 (41%)]\tLoss: 0.032382\nTrain Epoch: 67 [1200/2430 (49%)]\tLoss: 0.354154\nTrain Epoch: 67 [1400/2430 (58%)]\tLoss: 0.044058\nTrain Epoch: 67 [1600/2430 (66%)]\tLoss: 0.029419\nTrain Epoch: 67 [1800/2430 (74%)]\tLoss: 0.066420\nTrain Epoch: 67 [2000/2430 (82%)]\tLoss: 0.039689\nTrain Epoch: 67 [2200/2430 (91%)]\tLoss: 0.030100\nTrain Epoch: 67 [2400/2430 (99%)]\tLoss: 0.028231\n\nevaluating...\nTest set:\tAverage loss: 1.0984, Average CER: 0.179012 Average WER: 0.6727\n\nTrain Epoch: 68 [0/2430 (0%)]\tLoss: 0.049968\nTrain Epoch: 68 [200/2430 (8%)]\tLoss: 0.023192\nTrain Epoch: 68 [400/2430 (16%)]\tLoss: 0.009465\nTrain Epoch: 68 [600/2430 (25%)]\tLoss: 0.041168\nTrain Epoch: 68 [800/2430 (33%)]\tLoss: 0.032849\nTrain Epoch: 68 [1000/2430 (41%)]\tLoss: 0.050755\nTrain Epoch: 68 [1200/2430 (49%)]\tLoss: 0.032723\nTrain Epoch: 68 [1400/2430 (58%)]\tLoss: 0.049429\nTrain Epoch: 68 [1600/2430 (66%)]\tLoss: 0.062803\nTrain Epoch: 68 [1800/2430 (74%)]\tLoss: 0.036695\nTrain Epoch: 68 [2000/2430 (82%)]\tLoss: 0.096060\nTrain Epoch: 68 [2200/2430 (91%)]\tLoss: 0.005753\nTrain Epoch: 68 [2400/2430 (99%)]\tLoss: 0.044283\n\nevaluating...\nTest set:\tAverage loss: 1.0836, Average CER: 0.168445 Average WER: 0.6671\n\nTrain Epoch: 69 [0/2430 (0%)]\tLoss: 0.017092\nTrain Epoch: 69 [200/2430 (8%)]\tLoss: 0.017571\nTrain Epoch: 69 [400/2430 (16%)]\tLoss: 0.031122\nTrain Epoch: 69 [600/2430 (25%)]\tLoss: 0.026896\nTrain Epoch: 69 [800/2430 (33%)]\tLoss: 0.012758\nTrain Epoch: 69 [1000/2430 (41%)]\tLoss: 0.032498\nTrain Epoch: 69 [1200/2430 (49%)]\tLoss: 0.063424\nTrain Epoch: 69 [1400/2430 (58%)]\tLoss: 0.108968\nTrain Epoch: 69 [1600/2430 (66%)]\tLoss: 0.010899\nTrain Epoch: 69 [1800/2430 (74%)]\tLoss: 0.016297\nTrain Epoch: 69 [2000/2430 (82%)]\tLoss: 0.020476\nTrain Epoch: 69 [2200/2430 (91%)]\tLoss: 0.047853\nTrain Epoch: 69 [2400/2430 (99%)]\tLoss: 0.006926\n\nevaluating...\nTest set:\tAverage loss: 1.0689, Average CER: 0.165614 Average WER: 0.6318\n\nTrain Epoch: 70 [0/2430 (0%)]\tLoss: 0.028433\nTrain Epoch: 70 [200/2430 (8%)]\tLoss: 0.017508\nTrain Epoch: 70 [400/2430 (16%)]\tLoss: 0.035137\nTrain Epoch: 70 [600/2430 (25%)]\tLoss: 0.010080\nTrain Epoch: 70 [800/2430 (33%)]\tLoss: 0.013123\nTrain Epoch: 70 [1000/2430 (41%)]\tLoss: 0.020043\nTrain Epoch: 70 [1200/2430 (49%)]\tLoss: 0.022916\nTrain Epoch: 70 [1400/2430 (58%)]\tLoss: 0.007166\nTrain Epoch: 70 [1600/2430 (66%)]\tLoss: 0.010563\nTrain Epoch: 70 [1800/2430 (74%)]\tLoss: 0.008833\nTrain Epoch: 70 [2000/2430 (82%)]\tLoss: 0.026667\nTrain Epoch: 70 [2200/2430 (91%)]\tLoss: 0.036482\nTrain Epoch: 70 [2400/2430 (99%)]\tLoss: 0.031079\n\nevaluating...\nTest set:\tAverage loss: 1.1039, Average CER: 0.166756 Average WER: 0.6572\n\nTrain Epoch: 71 [0/2430 (0%)]\tLoss: 0.009041\nTrain Epoch: 71 [200/2430 (8%)]\tLoss: 0.013246\nTrain Epoch: 71 [400/2430 (16%)]\tLoss: 0.006396\nTrain Epoch: 71 [600/2430 (25%)]\tLoss: 0.002301\nTrain Epoch: 71 [800/2430 (33%)]\tLoss: 0.011762\nTrain Epoch: 71 [1000/2430 (41%)]\tLoss: 0.030286\nTrain Epoch: 71 [1200/2430 (49%)]\tLoss: 0.008605\nTrain Epoch: 71 [1400/2430 (58%)]\tLoss: 0.003593\nTrain Epoch: 71 [1600/2430 (66%)]\tLoss: 0.018342\nTrain Epoch: 71 [1800/2430 (74%)]\tLoss: 0.022175\nTrain Epoch: 71 [2000/2430 (82%)]\tLoss: 0.026765\nTrain Epoch: 71 [2200/2430 (91%)]\tLoss: 0.011736\nTrain Epoch: 71 [2400/2430 (99%)]\tLoss: 0.034566\n\nevaluating...\nTest set:\tAverage loss: 1.1311, Average CER: 0.173000 Average WER: 0.6512\n\nTrain Epoch: 72 [0/2430 (0%)]\tLoss: 0.020916\nTrain Epoch: 72 [200/2430 (8%)]\tLoss: 0.007180\nTrain Epoch: 72 [400/2430 (16%)]\tLoss: 0.029401\nTrain Epoch: 72 [600/2430 (25%)]\tLoss: 0.036240\nTrain Epoch: 72 [800/2430 (33%)]\tLoss: 0.026182\nTrain Epoch: 72 [1000/2430 (41%)]\tLoss: 0.060409\nTrain Epoch: 72 [1200/2430 (49%)]\tLoss: 0.024732\nTrain Epoch: 72 [1400/2430 (58%)]\tLoss: 0.053504\nTrain Epoch: 72 [1600/2430 (66%)]\tLoss: 0.046497\nTrain Epoch: 72 [1800/2430 (74%)]\tLoss: 0.035130\nTrain Epoch: 72 [2000/2430 (82%)]\tLoss: 0.024936\nTrain Epoch: 72 [2200/2430 (91%)]\tLoss: 0.010210\nTrain Epoch: 72 [2400/2430 (99%)]\tLoss: 0.092188\n\nevaluating...\nTest set:\tAverage loss: 1.2047, Average CER: 0.185456 Average WER: 0.7053\n\nTrain Epoch: 73 [0/2430 (0%)]\tLoss: 0.101166\nTrain Epoch: 73 [200/2430 (8%)]\tLoss: 0.078327\nTrain Epoch: 73 [400/2430 (16%)]\tLoss: 0.039853\nTrain Epoch: 73 [600/2430 (25%)]\tLoss: 0.010275\nTrain Epoch: 73 [800/2430 (33%)]\tLoss: 0.031964\nTrain Epoch: 73 [1000/2430 (41%)]\tLoss: 0.025226\nTrain Epoch: 73 [1200/2430 (49%)]\tLoss: 0.067779\nTrain Epoch: 73 [1400/2430 (58%)]\tLoss: 0.269311\nTrain Epoch: 73 [1600/2430 (66%)]\tLoss: 0.015434\nTrain Epoch: 73 [1800/2430 (74%)]\tLoss: 0.045112\nTrain Epoch: 73 [2000/2430 (82%)]\tLoss: 0.081628\nTrain Epoch: 73 [2200/2430 (91%)]\tLoss: 0.052170\nTrain Epoch: 73 [2400/2430 (99%)]\tLoss: 0.056407\n\nevaluating...\nTest set:\tAverage loss: 1.1712, Average CER: 0.170490 Average WER: 0.6395\n\nTrain Epoch: 74 [0/2430 (0%)]\tLoss: 0.007829\nTrain Epoch: 74 [200/2430 (8%)]\tLoss: 0.004462\nTrain Epoch: 74 [400/2430 (16%)]\tLoss: 0.036951\nTrain Epoch: 74 [600/2430 (25%)]\tLoss: 0.013185\nTrain Epoch: 74 [800/2430 (33%)]\tLoss: 0.037789\nTrain Epoch: 74 [1000/2430 (41%)]\tLoss: 0.053062\nTrain Epoch: 74 [1200/2430 (49%)]\tLoss: 0.022094\nTrain Epoch: 74 [1400/2430 (58%)]\tLoss: 0.026399\nTrain Epoch: 74 [1600/2430 (66%)]\tLoss: 0.035185\nTrain Epoch: 74 [1800/2430 (74%)]\tLoss: 0.016112\nTrain Epoch: 74 [2000/2430 (82%)]\tLoss: 0.051486\nTrain Epoch: 74 [2200/2430 (91%)]\tLoss: 0.039279\nTrain Epoch: 74 [2400/2430 (99%)]\tLoss: 0.013222\n\nevaluating...\nTest set:\tAverage loss: 1.1105, Average CER: 0.164090 Average WER: 0.6445\n\nTrain Epoch: 75 [0/2430 (0%)]\tLoss: 0.004236\nTrain Epoch: 75 [200/2430 (8%)]\tLoss: 0.007298\nTrain Epoch: 75 [400/2430 (16%)]\tLoss: 0.039042\nTrain Epoch: 75 [600/2430 (25%)]\tLoss: 0.042980\nTrain Epoch: 75 [800/2430 (33%)]\tLoss: 0.031127\nTrain Epoch: 75 [1000/2430 (41%)]\tLoss: 0.020363\nTrain Epoch: 75 [1200/2430 (49%)]\tLoss: 0.009377\nTrain Epoch: 75 [1400/2430 (58%)]\tLoss: 0.028183\nTrain Epoch: 75 [1600/2430 (66%)]\tLoss: 0.028611\nTrain Epoch: 75 [1800/2430 (74%)]\tLoss: 0.014030\nTrain Epoch: 75 [2000/2430 (82%)]\tLoss: 0.012367\nTrain Epoch: 75 [2200/2430 (91%)]\tLoss: 0.012565\nTrain Epoch: 75 [2400/2430 (99%)]\tLoss: 0.013754\n\nevaluating...\nTest set:\tAverage loss: 1.1711, Average CER: 0.170679 Average WER: 0.6520\n\nTrain Epoch: 76 [0/2430 (0%)]\tLoss: 0.021628\nTrain Epoch: 76 [200/2430 (8%)]\tLoss: 0.030332\nTrain Epoch: 76 [400/2430 (16%)]\tLoss: 0.008275\nTrain Epoch: 76 [600/2430 (25%)]\tLoss: 0.011547\nTrain Epoch: 76 [800/2430 (33%)]\tLoss: 0.006523\nTrain Epoch: 76 [1000/2430 (41%)]\tLoss: 0.009338\nTrain Epoch: 76 [1200/2430 (49%)]\tLoss: 0.030693\nTrain Epoch: 76 [1400/2430 (58%)]\tLoss: 0.051747\nTrain Epoch: 76 [1600/2430 (66%)]\tLoss: 0.021370\nTrain Epoch: 76 [1800/2430 (74%)]\tLoss: 0.019861\nTrain Epoch: 76 [2000/2430 (82%)]\tLoss: 0.005203\nTrain Epoch: 76 [2200/2430 (91%)]\tLoss: 0.001501\nTrain Epoch: 76 [2400/2430 (99%)]\tLoss: 0.015655\n\nevaluating...\nTest set:\tAverage loss: 1.1072, Average CER: 0.166440 Average WER: 0.6363\n\nTrain Epoch: 77 [0/2430 (0%)]\tLoss: 0.005311\nTrain Epoch: 77 [200/2430 (8%)]\tLoss: 0.008507\nTrain Epoch: 77 [400/2430 (16%)]\tLoss: 0.003483\nTrain Epoch: 77 [600/2430 (25%)]\tLoss: 0.002436\nTrain Epoch: 77 [800/2430 (33%)]\tLoss: 0.005651\nTrain Epoch: 77 [1000/2430 (41%)]\tLoss: 0.046012\nTrain Epoch: 77 [1200/2430 (49%)]\tLoss: 0.010631\nTrain Epoch: 77 [1400/2430 (58%)]\tLoss: 0.001377\nTrain Epoch: 77 [1600/2430 (66%)]\tLoss: 0.014517\nTrain Epoch: 77 [1800/2430 (74%)]\tLoss: 0.017138\nTrain Epoch: 77 [2000/2430 (82%)]\tLoss: 0.001537\nTrain Epoch: 77 [2200/2430 (91%)]\tLoss: 0.020624\nTrain Epoch: 77 [2400/2430 (99%)]\tLoss: 0.018616\n\nevaluating...\nTest set:\tAverage loss: 1.0816, Average CER: 0.163148 Average WER: 0.6454\n\nTrain Epoch: 78 [0/2430 (0%)]\tLoss: 0.000869\nTrain Epoch: 78 [200/2430 (8%)]\tLoss: 0.008867\nTrain Epoch: 78 [400/2430 (16%)]\tLoss: 0.001882\nTrain Epoch: 78 [600/2430 (25%)]\tLoss: 0.007725\nTrain Epoch: 78 [800/2430 (33%)]\tLoss: 0.020208\nTrain Epoch: 78 [1000/2430 (41%)]\tLoss: 0.034497\nTrain Epoch: 78 [1200/2430 (49%)]\tLoss: 0.006571\nTrain Epoch: 78 [1400/2430 (58%)]\tLoss: 0.003181\nTrain Epoch: 78 [1600/2430 (66%)]\tLoss: 0.001952\nTrain Epoch: 78 [1800/2430 (74%)]\tLoss: 0.000852\nTrain Epoch: 78 [2000/2430 (82%)]\tLoss: 0.000839\nTrain Epoch: 78 [2200/2430 (91%)]\tLoss: 0.006639\nTrain Epoch: 78 [2400/2430 (99%)]\tLoss: 0.000845\n\nevaluating...\nTest set:\tAverage loss: 1.1207, Average CER: 0.157166 Average WER: 0.6332\n\nTrain Epoch: 79 [0/2430 (0%)]\tLoss: 0.001051\nTrain Epoch: 79 [200/2430 (8%)]\tLoss: 0.000374\nTrain Epoch: 79 [400/2430 (16%)]\tLoss: 0.007135\nTrain Epoch: 79 [600/2430 (25%)]\tLoss: 0.035358\nTrain Epoch: 79 [800/2430 (33%)]\tLoss: 0.001746\nTrain Epoch: 79 [1000/2430 (41%)]\tLoss: 0.001450\nTrain Epoch: 79 [1200/2430 (49%)]\tLoss: 0.009467\nTrain Epoch: 79 [1400/2430 (58%)]\tLoss: 0.006178\nTrain Epoch: 79 [1600/2430 (66%)]\tLoss: 0.001329\nTrain Epoch: 79 [1800/2430 (74%)]\tLoss: 0.002698\nTrain Epoch: 79 [2000/2430 (82%)]\tLoss: 0.006859\nTrain Epoch: 79 [2200/2430 (91%)]\tLoss: 0.005831\nTrain Epoch: 79 [2400/2430 (99%)]\tLoss: 0.013833\n\nevaluating...\nTest set:\tAverage loss: 1.1580, Average CER: 0.156768 Average WER: 0.6226\n\nTrain Epoch: 80 [0/2430 (0%)]\tLoss: 0.001515\nTrain Epoch: 80 [200/2430 (8%)]\tLoss: 0.005415\nTrain Epoch: 80 [400/2430 (16%)]\tLoss: 0.005411\nTrain Epoch: 80 [600/2430 (25%)]\tLoss: 0.002172\nTrain Epoch: 80 [800/2430 (33%)]\tLoss: 0.014276\nTrain Epoch: 80 [1000/2430 (41%)]\tLoss: 0.058298\nTrain Epoch: 80 [1200/2430 (49%)]\tLoss: 0.002868\nTrain Epoch: 80 [1400/2430 (58%)]\tLoss: 0.005783\nTrain Epoch: 80 [1600/2430 (66%)]\tLoss: 0.016806\nTrain Epoch: 80 [1800/2430 (74%)]\tLoss: 0.005073\nTrain Epoch: 80 [2000/2430 (82%)]\tLoss: 0.001868\nTrain Epoch: 80 [2200/2430 (91%)]\tLoss: 0.001059\nTrain Epoch: 80 [2400/2430 (99%)]\tLoss: 0.006298\n\nevaluating...\nTest set:\tAverage loss: 1.1204, Average CER: 0.155615 Average WER: 0.6066\n\nTrain Epoch: 81 [0/2430 (0%)]\tLoss: 0.004436\nTrain Epoch: 81 [200/2430 (8%)]\tLoss: 0.001534\nTrain Epoch: 81 [400/2430 (16%)]\tLoss: 0.001852\nTrain Epoch: 81 [600/2430 (25%)]\tLoss: 0.000611\nTrain Epoch: 81 [800/2430 (33%)]\tLoss: 0.001099\nTrain Epoch: 81 [1000/2430 (41%)]\tLoss: 0.000760\nTrain Epoch: 81 [1200/2430 (49%)]\tLoss: 0.002983\nTrain Epoch: 81 [1400/2430 (58%)]\tLoss: 0.001545\nTrain Epoch: 81 [1600/2430 (66%)]\tLoss: 0.001754\nTrain Epoch: 81 [1800/2430 (74%)]\tLoss: 0.000284\nTrain Epoch: 81 [2000/2430 (82%)]\tLoss: 0.000841\nTrain Epoch: 81 [2200/2430 (91%)]\tLoss: 0.000530\nTrain Epoch: 81 [2400/2430 (99%)]\tLoss: 0.001364\n\nevaluating...\nTest set:\tAverage loss: 1.1171, Average CER: 0.156645 Average WER: 0.6156\n\nTrain Epoch: 82 [0/2430 (0%)]\tLoss: 0.001899\nTrain Epoch: 82 [200/2430 (8%)]\tLoss: 0.000676\nTrain Epoch: 82 [400/2430 (16%)]\tLoss: 0.000429\nTrain Epoch: 82 [600/2430 (25%)]\tLoss: 0.000867\nTrain Epoch: 82 [800/2430 (33%)]\tLoss: 0.002058\nTrain Epoch: 82 [1000/2430 (41%)]\tLoss: 0.005246\nTrain Epoch: 82 [1200/2430 (49%)]\tLoss: 0.001542\nTrain Epoch: 82 [1400/2430 (58%)]\tLoss: 0.001353\nTrain Epoch: 82 [1600/2430 (66%)]\tLoss: 0.006011\nTrain Epoch: 82 [1800/2430 (74%)]\tLoss: 0.003603\nTrain Epoch: 82 [2000/2430 (82%)]\tLoss: 0.006402\nTrain Epoch: 82 [2200/2430 (91%)]\tLoss: 0.015388\nTrain Epoch: 82 [2400/2430 (99%)]\tLoss: 0.001643\n\nevaluating...\nTest set:\tAverage loss: 1.1640, Average CER: 0.159923 Average WER: 0.6376\n\nTrain Epoch: 83 [0/2430 (0%)]\tLoss: 0.009814\nTrain Epoch: 83 [200/2430 (8%)]\tLoss: 0.000497\nTrain Epoch: 83 [400/2430 (16%)]\tLoss: 0.008962\nTrain Epoch: 83 [600/2430 (25%)]\tLoss: 0.075356\nTrain Epoch: 83 [800/2430 (33%)]\tLoss: 0.011274\nTrain Epoch: 83 [1000/2430 (41%)]\tLoss: 0.014909\nTrain Epoch: 83 [1200/2430 (49%)]\tLoss: 0.002721\nTrain Epoch: 83 [1400/2430 (58%)]\tLoss: 0.008015\nTrain Epoch: 83 [1600/2430 (66%)]\tLoss: 0.003797\nTrain Epoch: 83 [1800/2430 (74%)]\tLoss: 0.003665\nTrain Epoch: 83 [2000/2430 (82%)]\tLoss: 0.010086\nTrain Epoch: 83 [2200/2430 (91%)]\tLoss: 0.000643\nTrain Epoch: 83 [2400/2430 (99%)]\tLoss: 0.037093\n\nevaluating...\nTest set:\tAverage loss: 1.1686, Average CER: 0.160958 Average WER: 0.6484\n\nTrain Epoch: 84 [0/2430 (0%)]\tLoss: 0.024570\nTrain Epoch: 84 [200/2430 (8%)]\tLoss: 0.006222\nTrain Epoch: 84 [400/2430 (16%)]\tLoss: 0.007187\nTrain Epoch: 84 [600/2430 (25%)]\tLoss: 0.008610\nTrain Epoch: 84 [800/2430 (33%)]\tLoss: 0.033810\nTrain Epoch: 84 [1000/2430 (41%)]\tLoss: 0.020715\nTrain Epoch: 84 [1200/2430 (49%)]\tLoss: 0.046474\nTrain Epoch: 84 [1400/2430 (58%)]\tLoss: 0.008136\nTrain Epoch: 84 [1600/2430 (66%)]\tLoss: 0.016972\nTrain Epoch: 84 [1800/2430 (74%)]\tLoss: 0.011780\nTrain Epoch: 84 [2000/2430 (82%)]\tLoss: 0.010484\nTrain Epoch: 84 [2200/2430 (91%)]\tLoss: 0.008100\nTrain Epoch: 84 [2400/2430 (99%)]\tLoss: 0.008458\n\nevaluating...\nTest set:\tAverage loss: 1.1320, Average CER: 0.154748 Average WER: 0.6037\n\nTrain Epoch: 85 [0/2430 (0%)]\tLoss: 0.000516\nTrain Epoch: 85 [200/2430 (8%)]\tLoss: 0.004954\nTrain Epoch: 85 [400/2430 (16%)]\tLoss: 0.018318\nTrain Epoch: 85 [600/2430 (25%)]\tLoss: 0.001840\nTrain Epoch: 85 [800/2430 (33%)]\tLoss: 0.000612\nTrain Epoch: 85 [1000/2430 (41%)]\tLoss: 0.001782\nTrain Epoch: 85 [1200/2430 (49%)]\tLoss: 0.001655\nTrain Epoch: 85 [1400/2430 (58%)]\tLoss: 0.002586\nTrain Epoch: 85 [1600/2430 (66%)]\tLoss: 0.009568\nTrain Epoch: 85 [1800/2430 (74%)]\tLoss: 0.000889\nTrain Epoch: 85 [2000/2430 (82%)]\tLoss: 0.001320\nTrain Epoch: 85 [2200/2430 (91%)]\tLoss: 0.005107\nTrain Epoch: 85 [2400/2430 (99%)]\tLoss: 0.011959\n\nevaluating...\nTest set:\tAverage loss: 1.1548, Average CER: 0.153807 Average WER: 0.6091\n\nTrain Epoch: 86 [0/2430 (0%)]\tLoss: 0.008519\nTrain Epoch: 86 [200/2430 (8%)]\tLoss: 0.007925\nTrain Epoch: 86 [400/2430 (16%)]\tLoss: 0.004841\nTrain Epoch: 86 [600/2430 (25%)]\tLoss: 0.001436\nTrain Epoch: 86 [800/2430 (33%)]\tLoss: 0.000850\nTrain Epoch: 86 [1000/2430 (41%)]\tLoss: 0.001981\nTrain Epoch: 86 [1200/2430 (49%)]\tLoss: 0.000542\nTrain Epoch: 86 [1400/2430 (58%)]\tLoss: 0.000620\nTrain Epoch: 86 [1600/2430 (66%)]\tLoss: 0.008096\nTrain Epoch: 86 [1800/2430 (74%)]\tLoss: 0.002859\nTrain Epoch: 86 [2000/2430 (82%)]\tLoss: 0.000791\nTrain Epoch: 86 [2200/2430 (91%)]\tLoss: 0.005497\nTrain Epoch: 86 [2400/2430 (99%)]\tLoss: 0.006092\n\nevaluating...\nTest set:\tAverage loss: 1.1756, Average CER: 0.154704 Average WER: 0.6187\n\nTrain Epoch: 87 [0/2430 (0%)]\tLoss: 0.000419\nTrain Epoch: 87 [200/2430 (8%)]\tLoss: 0.000460\nTrain Epoch: 87 [400/2430 (16%)]\tLoss: 0.000545\nTrain Epoch: 87 [600/2430 (25%)]\tLoss: 0.000370\nTrain Epoch: 87 [800/2430 (33%)]\tLoss: 0.000492\nTrain Epoch: 87 [1000/2430 (41%)]\tLoss: 0.003092\nTrain Epoch: 87 [1200/2430 (49%)]\tLoss: 0.000690\nTrain Epoch: 87 [1400/2430 (58%)]\tLoss: 0.001467\nTrain Epoch: 87 [1600/2430 (66%)]\tLoss: 0.000346\nTrain Epoch: 87 [1800/2430 (74%)]\tLoss: 0.002140\nTrain Epoch: 87 [2000/2430 (82%)]\tLoss: 0.000967\nTrain Epoch: 87 [2200/2430 (91%)]\tLoss: 0.000529\nTrain Epoch: 87 [2400/2430 (99%)]\tLoss: 0.002016\n\nevaluating...\nTest set:\tAverage loss: 1.1747, Average CER: 0.150623 Average WER: 0.5946\n\nTrain Epoch: 88 [0/2430 (0%)]\tLoss: 0.000162\nTrain Epoch: 88 [200/2430 (8%)]\tLoss: 0.000320\nTrain Epoch: 88 [400/2430 (16%)]\tLoss: 0.005319\nTrain Epoch: 88 [600/2430 (25%)]\tLoss: 0.000609\nTrain Epoch: 88 [800/2430 (33%)]\tLoss: 0.000169\nTrain Epoch: 88 [1000/2430 (41%)]\tLoss: 0.001420\nTrain Epoch: 88 [1200/2430 (49%)]\tLoss: 0.000598\nTrain Epoch: 88 [1400/2430 (58%)]\tLoss: 0.000338\nTrain Epoch: 88 [1600/2430 (66%)]\tLoss: 0.000260\nTrain Epoch: 88 [1800/2430 (74%)]\tLoss: 0.000119\nTrain Epoch: 88 [2000/2430 (82%)]\tLoss: 0.000340\nTrain Epoch: 88 [2200/2430 (91%)]\tLoss: 0.000365\nTrain Epoch: 88 [2400/2430 (99%)]\tLoss: 0.000815\n\nevaluating...\nTest set:\tAverage loss: 1.1690, Average CER: 0.150269 Average WER: 0.5985\n\nTrain Epoch: 89 [0/2430 (0%)]\tLoss: 0.000197\nTrain Epoch: 89 [200/2430 (8%)]\tLoss: 0.000540\nTrain Epoch: 89 [400/2430 (16%)]\tLoss: 0.000294\nTrain Epoch: 89 [600/2430 (25%)]\tLoss: 0.000250\nTrain Epoch: 89 [800/2430 (33%)]\tLoss: 0.000290\nTrain Epoch: 89 [1000/2430 (41%)]\tLoss: 0.000368\nTrain Epoch: 89 [1200/2430 (49%)]\tLoss: 0.000472\nTrain Epoch: 89 [1400/2430 (58%)]\tLoss: 0.000652\nTrain Epoch: 89 [1600/2430 (66%)]\tLoss: 0.000319\nTrain Epoch: 89 [1800/2430 (74%)]\tLoss: 0.000931\nTrain Epoch: 89 [2000/2430 (82%)]\tLoss: 0.000254\nTrain Epoch: 89 [2200/2430 (91%)]\tLoss: 0.001681\nTrain Epoch: 89 [2400/2430 (99%)]\tLoss: 0.000457\n\nevaluating...\nTest set:\tAverage loss: 1.1786, Average CER: 0.150567 Average WER: 0.5949\n\nTrain Epoch: 90 [0/2430 (0%)]\tLoss: 0.000293\nTrain Epoch: 90 [200/2430 (8%)]\tLoss: 0.000593\nTrain Epoch: 90 [400/2430 (16%)]\tLoss: 0.000528\nTrain Epoch: 90 [600/2430 (25%)]\tLoss: 0.006615\nTrain Epoch: 90 [800/2430 (33%)]\tLoss: 0.000237\nTrain Epoch: 90 [1000/2430 (41%)]\tLoss: 0.000247\nTrain Epoch: 90 [1200/2430 (49%)]\tLoss: 0.000204\nTrain Epoch: 90 [1400/2430 (58%)]\tLoss: 0.000448\nTrain Epoch: 90 [1600/2430 (66%)]\tLoss: 0.000338\nTrain Epoch: 90 [1800/2430 (74%)]\tLoss: 0.000659\nTrain Epoch: 90 [2000/2430 (82%)]\tLoss: 0.000354\nTrain Epoch: 90 [2200/2430 (91%)]\tLoss: 0.000303\nTrain Epoch: 90 [2400/2430 (99%)]\tLoss: 0.001646\n\nevaluating...\nTest set:\tAverage loss: 1.1934, Average CER: 0.150810 Average WER: 0.5939\n\nTrain Epoch: 91 [0/2430 (0%)]\tLoss: 0.000128\nTrain Epoch: 91 [200/2430 (8%)]\tLoss: 0.000036\nTrain Epoch: 91 [400/2430 (16%)]\tLoss: 0.000501\nTrain Epoch: 91 [600/2430 (25%)]\tLoss: 0.000133\nTrain Epoch: 91 [800/2430 (33%)]\tLoss: 0.000637\nTrain Epoch: 91 [1000/2430 (41%)]\tLoss: 0.000379\nTrain Epoch: 91 [1200/2430 (49%)]\tLoss: 0.000205\nTrain Epoch: 91 [1400/2430 (58%)]\tLoss: 0.000153\nTrain Epoch: 91 [1600/2430 (66%)]\tLoss: 0.000347\nTrain Epoch: 91 [1800/2430 (74%)]\tLoss: 0.000164\nTrain Epoch: 91 [2000/2430 (82%)]\tLoss: 0.003669\nTrain Epoch: 91 [2200/2430 (91%)]\tLoss: 0.000076\nTrain Epoch: 91 [2400/2430 (99%)]\tLoss: 0.000156\n\nevaluating...\nTest set:\tAverage loss: 1.2087, Average CER: 0.152694 Average WER: 0.6060\n\nTrain Epoch: 92 [0/2430 (0%)]\tLoss: 0.000612\nTrain Epoch: 92 [200/2430 (8%)]\tLoss: 0.000137\nTrain Epoch: 92 [400/2430 (16%)]\tLoss: 0.000371\nTrain Epoch: 92 [600/2430 (25%)]\tLoss: 0.000129\nTrain Epoch: 92 [800/2430 (33%)]\tLoss: 0.000932\nTrain Epoch: 92 [1000/2430 (41%)]\tLoss: 0.000145\nTrain Epoch: 92 [1200/2430 (49%)]\tLoss: 0.000217\nTrain Epoch: 92 [1400/2430 (58%)]\tLoss: 0.000275\nTrain Epoch: 92 [1600/2430 (66%)]\tLoss: 0.000314\nTrain Epoch: 92 [1800/2430 (74%)]\tLoss: 0.000950\nTrain Epoch: 92 [2000/2430 (82%)]\tLoss: 0.000237\nTrain Epoch: 92 [2200/2430 (91%)]\tLoss: 0.000075\nTrain Epoch: 92 [2400/2430 (99%)]\tLoss: 0.000118\n\nevaluating...\nTest set:\tAverage loss: 1.1958, Average CER: 0.152269 Average WER: 0.5980\n\nTrain Epoch: 93 [0/2430 (0%)]\tLoss: 0.000177\nTrain Epoch: 93 [200/2430 (8%)]\tLoss: 0.000258\nTrain Epoch: 93 [400/2430 (16%)]\tLoss: 0.000092\nTrain Epoch: 93 [600/2430 (25%)]\tLoss: 0.000157\nTrain Epoch: 93 [800/2430 (33%)]\tLoss: 0.000532\nTrain Epoch: 93 [1000/2430 (41%)]\tLoss: 0.000820\nTrain Epoch: 93 [1200/2430 (49%)]\tLoss: 0.000109\nTrain Epoch: 93 [1400/2430 (58%)]\tLoss: 0.000437\nTrain Epoch: 93 [1600/2430 (66%)]\tLoss: 0.000047\nTrain Epoch: 93 [1800/2430 (74%)]\tLoss: 0.000145\nTrain Epoch: 93 [2000/2430 (82%)]\tLoss: 0.000170\nTrain Epoch: 93 [2200/2430 (91%)]\tLoss: 0.000068\nTrain Epoch: 93 [2400/2430 (99%)]\tLoss: 0.000754\n\nevaluating...\nTest set:\tAverage loss: 1.1920, Average CER: 0.151423 Average WER: 0.5989\n\nTrain Epoch: 94 [0/2430 (0%)]\tLoss: 0.000144\nTrain Epoch: 94 [200/2430 (8%)]\tLoss: 0.000470\nTrain Epoch: 94 [400/2430 (16%)]\tLoss: 0.000288\nTrain Epoch: 94 [600/2430 (25%)]\tLoss: 0.000128\nTrain Epoch: 94 [800/2430 (33%)]\tLoss: 0.000225\nTrain Epoch: 94 [1000/2430 (41%)]\tLoss: 0.000126\nTrain Epoch: 94 [1200/2430 (49%)]\tLoss: 0.000223\nTrain Epoch: 94 [1400/2430 (58%)]\tLoss: 0.000050\nTrain Epoch: 94 [1600/2430 (66%)]\tLoss: 0.000184\nTrain Epoch: 94 [1800/2430 (74%)]\tLoss: 0.000343\nTrain Epoch: 94 [2000/2430 (82%)]\tLoss: 0.000091\nTrain Epoch: 94 [2200/2430 (91%)]\tLoss: 0.000206\nTrain Epoch: 94 [2400/2430 (99%)]\tLoss: 0.000385\n\nevaluating...\nTest set:\tAverage loss: 1.1957, Average CER: 0.150725 Average WER: 0.5975\n\nTrain Epoch: 95 [0/2430 (0%)]\tLoss: 0.000301\nTrain Epoch: 95 [200/2430 (8%)]\tLoss: 0.000101\nTrain Epoch: 95 [400/2430 (16%)]\tLoss: 0.000092\nTrain Epoch: 95 [600/2430 (25%)]\tLoss: 0.000077\nTrain Epoch: 95 [800/2430 (33%)]\tLoss: 0.000293\nTrain Epoch: 95 [1000/2430 (41%)]\tLoss: 0.000089\nTrain Epoch: 95 [1200/2430 (49%)]\tLoss: 0.000117\nTrain Epoch: 95 [1400/2430 (58%)]\tLoss: 0.000227\nTrain Epoch: 95 [1600/2430 (66%)]\tLoss: 0.000076\nTrain Epoch: 95 [1800/2430 (74%)]\tLoss: 0.000154\nTrain Epoch: 95 [2000/2430 (82%)]\tLoss: 0.000092\nTrain Epoch: 95 [2200/2430 (91%)]\tLoss: 0.000048\nTrain Epoch: 95 [2400/2430 (99%)]\tLoss: 0.000107\n\nevaluating...\nTest set:\tAverage loss: 1.1942, Average CER: 0.149123 Average WER: 0.5926\n\nTrain Epoch: 96 [0/2430 (0%)]\tLoss: 0.000092\nTrain Epoch: 96 [200/2430 (8%)]\tLoss: 0.000108\nTrain Epoch: 96 [400/2430 (16%)]\tLoss: 0.000187\nTrain Epoch: 96 [600/2430 (25%)]\tLoss: 0.000399\nTrain Epoch: 96 [800/2430 (33%)]\tLoss: 0.006993\nTrain Epoch: 96 [1000/2430 (41%)]\tLoss: 0.000124\nTrain Epoch: 96 [1200/2430 (49%)]\tLoss: 0.000306\nTrain Epoch: 96 [1400/2430 (58%)]\tLoss: 0.000072\nTrain Epoch: 96 [1600/2430 (66%)]\tLoss: 0.000332\nTrain Epoch: 96 [1800/2430 (74%)]\tLoss: 0.000043\nTrain Epoch: 96 [2000/2430 (82%)]\tLoss: 0.000077\nTrain Epoch: 96 [2200/2430 (91%)]\tLoss: 0.000921\nTrain Epoch: 96 [2400/2430 (99%)]\tLoss: 0.000160\n\nevaluating...\nTest set:\tAverage loss: 1.1980, Average CER: 0.149091 Average WER: 0.5925\n\nTrain Epoch: 97 [0/2430 (0%)]\tLoss: 0.000141\nTrain Epoch: 97 [200/2430 (8%)]\tLoss: 0.000300\nTrain Epoch: 97 [400/2430 (16%)]\tLoss: 0.000312\nTrain Epoch: 97 [600/2430 (25%)]\tLoss: 0.000097\nTrain Epoch: 97 [800/2430 (33%)]\tLoss: 0.000088\nTrain Epoch: 97 [1000/2430 (41%)]\tLoss: 0.000188\nTrain Epoch: 97 [1200/2430 (49%)]\tLoss: 0.000134\nTrain Epoch: 97 [1400/2430 (58%)]\tLoss: 0.000236\nTrain Epoch: 97 [1600/2430 (66%)]\tLoss: 0.000858\nTrain Epoch: 97 [1800/2430 (74%)]\tLoss: 0.000133\nTrain Epoch: 97 [2000/2430 (82%)]\tLoss: 0.000189\nTrain Epoch: 97 [2200/2430 (91%)]\tLoss: 0.000096\nTrain Epoch: 97 [2400/2430 (99%)]\tLoss: 0.000118\n\nevaluating...\nTest set:\tAverage loss: 1.2000, Average CER: 0.150224 Average WER: 0.5994\n\nTrain Epoch: 98 [0/2430 (0%)]\tLoss: 0.000123\nTrain Epoch: 98 [200/2430 (8%)]\tLoss: 0.000068\nTrain Epoch: 98 [400/2430 (16%)]\tLoss: 0.000288\nTrain Epoch: 98 [600/2430 (25%)]\tLoss: 0.000109\nTrain Epoch: 98 [800/2430 (33%)]\tLoss: 0.000063\nTrain Epoch: 98 [1000/2430 (41%)]\tLoss: 0.000099\nTrain Epoch: 98 [1200/2430 (49%)]\tLoss: 0.000040\nTrain Epoch: 98 [1400/2430 (58%)]\tLoss: 0.000118\nTrain Epoch: 98 [1600/2430 (66%)]\tLoss: 0.000072\nTrain Epoch: 98 [1800/2430 (74%)]\tLoss: 0.000138\nTrain Epoch: 98 [2000/2430 (82%)]\tLoss: 0.000200\nTrain Epoch: 98 [2200/2430 (91%)]\tLoss: 0.000252\nTrain Epoch: 98 [2400/2430 (99%)]\tLoss: 0.000075\n\nevaluating...\nTest set:\tAverage loss: 1.2046, Average CER: 0.149283 Average WER: 0.5962\n\nTrain Epoch: 99 [0/2430 (0%)]\tLoss: 0.000059\nTrain Epoch: 99 [200/2430 (8%)]\tLoss: 0.000062\nTrain Epoch: 99 [400/2430 (16%)]\tLoss: 0.000439\nTrain Epoch: 99 [600/2430 (25%)]\tLoss: 0.000147\nTrain Epoch: 99 [800/2430 (33%)]\tLoss: 0.000252\nTrain Epoch: 99 [1000/2430 (41%)]\tLoss: 0.000141\nTrain Epoch: 99 [1200/2430 (49%)]\tLoss: 0.000157\nTrain Epoch: 99 [1400/2430 (58%)]\tLoss: 0.000090\nTrain Epoch: 99 [1600/2430 (66%)]\tLoss: 0.000108\nTrain Epoch: 99 [1800/2430 (74%)]\tLoss: 0.000040\nTrain Epoch: 99 [2000/2430 (82%)]\tLoss: 0.000119\nTrain Epoch: 99 [2200/2430 (91%)]\tLoss: 0.000162\nTrain Epoch: 99 [2400/2430 (99%)]\tLoss: 0.000130\n\nevaluating...\nTest set:\tAverage loss: 1.2059, Average CER: 0.150002 Average WER: 0.5977\n\nTrain Epoch: 100 [0/2430 (0%)]\tLoss: 0.000157\nTrain Epoch: 100 [200/2430 (8%)]\tLoss: 0.000228\nTrain Epoch: 100 [400/2430 (16%)]\tLoss: 0.000082\nTrain Epoch: 100 [600/2430 (25%)]\tLoss: 0.000099\nTrain Epoch: 100 [800/2430 (33%)]\tLoss: 0.000096\nTrain Epoch: 100 [1000/2430 (41%)]\tLoss: 0.000176\nTrain Epoch: 100 [1200/2430 (49%)]\tLoss: 0.000082\nTrain Epoch: 100 [1400/2430 (58%)]\tLoss: 0.000078\nTrain Epoch: 100 [1600/2430 (66%)]\tLoss: 0.000105\nTrain Epoch: 100 [1800/2430 (74%)]\tLoss: 0.000110\nTrain Epoch: 100 [2000/2430 (82%)]\tLoss: 0.000125\nTrain Epoch: 100 [2200/2430 (91%)]\tLoss: 0.000121\nTrain Epoch: 100 [2400/2430 (99%)]\tLoss: 0.000198\n\nevaluating...\nTest set:\tAverage loss: 1.2068, Average CER: 0.149455 Average WER: 0.5947\n\nCPU times: user 45min 1s, sys: 37.9 s, total: 45min 39s\nWall time: 45min 28s\n","output_type":"stream"}]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cpu\")\n\nmodel = torch.load('/kaggle/working/model.pt')\n\n#1543 1882 1372\n\nmodel.to(device)\npredict(model, '/kaggle/input/upd-speech/mono_voice/1964.wav', device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:26:24.339488Z","iopub.execute_input":"2023-05-24T10:26:24.340405Z","iopub.status.idle":"2023-05-24T10:26:24.350954Z","shell.execute_reply.started":"2023-05-24T10:26:24.340364Z","shell.execute_reply":"2023-05-24T10:26:24.349831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:16:11.401175Z","iopub.execute_input":"2023-05-13T16:16:11.401879Z","iopub.status.idle":"2023-05-13T16:16:11.430020Z","shell.execute_reply.started":"2023-05-13T16:16:11.401838Z","shell.execute_reply":"2023-05-13T16:16:11.428960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(directory):\n    total_duration = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            filepath = os.path.join(directory, filename)\n            with wave.open(filepath, 'r') as wav_file:\n                frames = wav_file.getnframes()\n                rate = wav_file.getframerate()\n                duration = frames / float(rate)\n                total_duration += duration\n    return total_duration\n\ndirectory = '/kaggle/input/upd-speech/mono_voice'\ntotal_duration = get_wav_duration(directory)\nprint('Total duration of WAV files:', total_duration, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:15.415086Z","iopub.execute_input":"2023-07-05T10:09:15.415876Z","iopub.status.idle":"2023-07-05T10:09:18.755936Z","shell.execute_reply.started":"2023-07-05T10:09:15.415836Z","shell.execute_reply":"2023-07-05T10:09:18.754693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return '{:02d}:{:02d}:{:02d}'.format(int(hours), int(minutes), int(seconds))\nseconds = 3661\nformatted_time = format_time(total_duration)\nprint(formatted_time)  # Output: '01:01:01'","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:23.353548Z","iopub.execute_input":"2023-07-05T10:09:23.354296Z","iopub.status.idle":"2023-07-05T10:09:23.361628Z","shell.execute_reply.started":"2023-07-05T10:09:23.354254Z","shell.execute_reply":"2023-07-05T10:09:23.360431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}