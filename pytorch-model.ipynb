{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5321255,"sourceType":"datasetVersion","datasetId":3091651},{"sourceId":5618710,"sourceType":"datasetVersion","datasetId":3230790},{"sourceId":5677279,"sourceType":"datasetVersion","datasetId":2989949},{"sourceId":5677298,"sourceType":"datasetVersion","datasetId":3213578},{"sourceId":5677449,"sourceType":"datasetVersion","datasetId":3071831},{"sourceId":5760288,"sourceType":"datasetVersion","datasetId":3311237},{"sourceId":8003942,"sourceType":"datasetVersion","datasetId":4230886}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np \nimport matplotlib\nfrom transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-04-06T03:57:59.877077Z","iopub.execute_input":"2024-04-06T03:57:59.877966Z","iopub.status.idle":"2024-04-06T03:58:04.192265Z","shell.execute_reply.started":"2024-04-06T03:57:59.877917Z","shell.execute_reply":"2024-04-06T03:58:04.191006Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        int_sequence = []\n        for c in text:\n            ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MFCC(n_mfcc=20)\n)\n\n\nvalid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//3)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=34, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2024-04-06T03:58:04.197327Z","iopub.execute_input":"2024-04-06T03:58:04.198142Z","iopub.status.idle":"2024-04-06T03:58:04.361115Z","shell.execute_reply.started":"2024-04-06T03:58:04.198101Z","shell.execute_reply":"2024-04-06T03:58:04.359747Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"class BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-06T03:58:04.362797Z","iopub.execute_input":"2024-04-06T03:58:04.363163Z","iopub.status.idle":"2024-04-06T03:58:04.371796Z","shell.execute_reply.started":"2024-04-06T03:58:04.363129Z","shell.execute_reply":"2024-04-06T03:58:04.370846Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\"\"\"import pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/rus-speech/Speeches.xlsx')\ny = [sentence for sentence in file['Русская речь']]\n\ndir_name = \"/kaggle/input/upd-speech/mono_voice/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in range(1, 2001):\n    file_name = f'{e}.wav'\n    sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    X.append(torch.Tensor(sampl))\n    if i % 100 == 0:\n        print(i)\n    i += 1\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-06T03:58:04.373866Z","iopub.execute_input":"2024-04-06T03:58:04.374713Z","iopub.status.idle":"2024-04-06T03:58:04.393822Z","shell.execute_reply.started":"2024-04-06T03:58:04.374671Z","shell.execute_reply":"2024-04-06T03:58:04.392698Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'import pandas as pd\\nimport librosa\\n\\nfile = pd.read_excel(\\'/kaggle/input/rus-speech/Speeches.xlsx\\')\\ny = [sentence for sentence in file[\\'Русская речь\\']]\\n\\ndir_name = \"/kaggle/input/upd-speech/mono_voice/\"\\nfiles_in_dir = os.listdir(dir_name)\\n\\nX = []\\ni = 1\\n\\nfor e in range(1, 2001):\\n    file_name = f\\'{e}.wav\\'\\n    sampl = librosa.load(dir_name + file_name, sr=16000)[0]\\n    sampl = sampl[np.newaxis, :]\\n    X.append(torch.Tensor(sampl))\\n    if i % 100 == 0:\\n        print(i)\\n    i += 1'"},"metadata":{}}]},{"cell_type":"code","source":"#Поменял там, где происходит загрузка, сохраняется id звукового файла, а потом в excel файле по колонке old_id ищется текст\n#И того звук и текст к нему\n\nimport pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches v1.xlsx')\n#y = [sentence for sentence in file['text']]\ny = []\ndir_name = \"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in os.listdir(\"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"):\n    file_name = e\n    for old_id in range(0, 2073):\n        if file_name.startswith(str(file['old_id'][old_id]) + '.'):\n            y.extend([''.join(file['text'][old_id])])\n            sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n            sampl = sampl[np.newaxis, :]\n            X.append(torch.Tensor(sampl))\n            break","metadata":{"execution":{"iopub.status.busy":"2024-04-06T03:58:04.395084Z","iopub.execute_input":"2024-04-06T03:58:04.395357Z","iopub.status.idle":"2024-04-06T04:00:10.221958Z","shell.execute_reply.started":"2024-04-06T03:58:04.395330Z","shell.execute_reply":"2024-04-06T04:00:10.220388Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\"\"\"import pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches v1.xlsx')\n#y = [sentence for sentence in file['text']]\ny = []\ndir_name = \"/kaggle/input/2700-audio/Disorder Russian Speech/Disorder Russian Speech/Disorder Russian Speech/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in os.listdir(\"/kaggle/input/2700-audio/Disorder Russian Speech/Disorder Russian Speech/Disorder Russian Speech/\"):\n    file_name = e\n    for old_id in range(0, 2073):\n        if file_name.startswith(str(file['old_id'][old_id]) + '.'):\n            y.extend([''.join(file['text'][old_id])])\n            sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n            sampl = sampl[np.newaxis, :]\n            X.append(torch.Tensor(sampl))\n            break\n    if i % 100 == 0:\n        print(i)\n    i += 1\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:28:28.873601Z","iopub.execute_input":"2024-04-02T14:28:28.874749Z","iopub.status.idle":"2024-04-02T14:31:55.214061Z","shell.execute_reply.started":"2024-04-02T14:28:28.874698Z","shell.execute_reply":"2024-04-02T14:31:55.212873Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\npairs = list(zip(X, y))\nrandom.shuffle(pairs)\nX, y = zip(*pairs)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T04:01:05.834345Z","iopub.execute_input":"2024-04-06T04:01:05.835587Z","iopub.status.idle":"2024-04-06T04:01:05.846026Z","shell.execute_reply.started":"2024-04-06T04:01:05.835539Z","shell.execute_reply":"2024-04-06T04:01:05.844956Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"y[540]","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:52:44.157357Z","iopub.execute_input":"2024-04-02T14:52:44.158056Z","iopub.status.idle":"2024-04-02T14:52:44.164608Z","shell.execute_reply.started":"2024-04-02T14:52:44.158015Z","shell.execute_reply":"2024-04-02T14:52:44.163563Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'Оно стучится'"},"metadata":{}}]},{"cell_type":"code","source":"type(X[3210])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:54:36.379616Z","iopub.execute_input":"2024-04-02T07:54:36.380346Z","iopub.status.idle":"2024-04-02T07:54:36.386805Z","shell.execute_reply.started":"2024-04-02T07:54:36.380311Z","shell.execute_reply":"2024-04-02T07:54:36.385581Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"torch.Tensor"},"metadata":{}}]},{"cell_type":"code","source":"torchaudio.save('/kaggle/working/audio.wav', X[540], 16000)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:52:46.826831Z","iopub.execute_input":"2024-04-02T14:52:46.827791Z","iopub.status.idle":"2024-04-02T14:52:46.834792Z","shell.execute_reply.started":"2024-04-02T14:52:46.827746Z","shell.execute_reply":"2024-04-02T14:52:46.833811Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"waveform, sample_rate = torchaudio.load('/kaggle/working/audio.wav')  # Загрузка аудиофайла\ntorchaudio.play(waveform, sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:22:15.368540Z","iopub.execute_input":"2024-04-02T07:22:15.368969Z","iopub.status.idle":"2024-04-02T07:22:15.395250Z","shell.execute_reply.started":"2024-04-02T07:22:15.368930Z","shell.execute_reply":"2024-04-02T07:22:15.393666Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1470391849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/audio.wav'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Загрузка аудиофайла\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'torchaudio' has no attribute 'play'"],"ename":"AttributeError","evalue":"module 'torchaudio' has no attribute 'play'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('4', 'четыре').replace('Р-220', 'р двести двадцать').replace('6', 'шесть').replace(\"-\", \" \")\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    sentence = \" \".join(sentence.split())\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T04:01:12.092026Z","iopub.execute_input":"2024-04-06T04:01:12.092399Z","iopub.status.idle":"2024-04-06T04:01:12.124168Z","shell.execute_reply.started":"2024-04-06T04:01:12.092369Z","shell.execute_reply":"2024-04-06T04:01:12.122954Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\"\"\"#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nX_train = X[:1800]\nX_test = X[1800:]\ny_train = y[:1800]\ny_test = y[1800:]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-06T04:01:14.043096Z","iopub.execute_input":"2024-04-06T04:01:14.043748Z","iopub.status.idle":"2024-04-06T04:01:14.149824Z","shell.execute_reply.started":"2024-04-06T04:01:14.043710Z","shell.execute_reply":"2024-04-06T04:01:14.148789Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\\nX_train = X[:1800]\\nX_test = X[1800:]\\ny_train = y[:1800]\\ny_test = y[1800:]'"},"metadata":{}}]},{"cell_type":"code","source":"X_train = X[:2430]\nX_test = X[2430:]\ny_train = y[:2430]\ny_test = y[2430:]","metadata":{"execution":{"iopub.status.busy":"2024-04-06T04:01:14.265011Z","iopub.execute_input":"2024-04-06T04:01:14.265669Z","iopub.status.idle":"2024-04-06T04:01:14.270699Z","shell.execute_reply.started":"2024-04-06T04:01:14.265634Z","shell.execute_reply":"2024-04-06T04:01:14.269695Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2024-04-06T04:01:14.813455Z","iopub.execute_input":"2024-04-06T04:01:14.814253Z","iopub.status.idle":"2024-04-06T04:01:14.822197Z","shell.execute_reply.started":"2024-04-06T04:01:14.814215Z","shell.execute_reply":"2024-04-06T04:01:14.820792Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 32, kernel_size=(4,4), stride=(3,3), padding=(2,2)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.Conv2d(128, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n        )\n        \n        self.fc_1 = nn.Sequential(\n            nn.Linear(896, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n        )\n        \n        self.BiGRU_1 = BidirectionalGRU(270, 270, 0, True)\n        self.BiGRU_2 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_3 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_4 = BidirectionalGRU(540, 270, 0.5, True)\n        \n        self.fc_2 = nn.Sequential(\n            nn.Linear(540, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.fc_1(x)\n        x = self.BiGRU_1(x)\n        x = self.BiGRU_2(x)\n        x = self.BiGRU_3(x)\n        x = self.BiGRU_4(x)\n        x = self.fc_2(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-06T04:01:16.223792Z","iopub.execute_input":"2024-04-06T04:01:16.224751Z","iopub.status.idle":"2024-04-06T04:01:16.241250Z","shell.execute_reply.started":"2024-04-06T04:01:16.224706Z","shell.execute_reply":"2024-04-06T04:01:16.239945Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"TOKENIZERS_PARALLELISM=True","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:14:12.214440Z","iopub.execute_input":"2024-04-04T16:14:12.215402Z","iopub.status.idle":"2024-04-04T16:14:12.220648Z","shell.execute_reply.started":"2024-04-04T16:14:12.215355Z","shell.execute_reply":"2024-04-04T16:14:12.219629Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Зададим название выбронной модели из хаба\nMODEL_NAME = 'UrukHan/t5-russian-spell'\nMAX_INPUT = 256\n\n# Загрузка модели и токенизатора\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n\n# Входные данные (можно массив фраз или текст)\ninput_sequences = 'сеглдыя хорош ден'   # или можно использовать одиночные фразы:  input_sequences = 'сеглдыя хорош ден'","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:13:34.768051Z","iopub.execute_input":"2024-04-04T16:13:34.769406Z","iopub.status.idle":"2024-04-04T16:13:50.721738Z","shell.execute_reply.started":"2024-04-04T16:13:34.769360Z","shell.execute_reply":"2024-04-04T16:13:50.720460Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8d90e6b8aab4dbebaaf8f2e33490fad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/1.00M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2e978d0b27c49f48a82e07229677f87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/2.63M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a90dbe303254b05a8ce181746974387"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c674444c40e24357aad817af6b00b10a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c45b1acb77f44b9952e2ad72bb332cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e9d12f5750b4c2985e2abb2d2b275ea"}},"metadata":{}}]},{"cell_type":"code","source":"task_prefix = \"Spell correct: \"                 # Токенизирование данных\nif type(input_sequences) != list: input_sequences = [input_sequences]\nencoded = tokenizer(\n  [task_prefix + sequence for sequence in input_sequences],\n  padding=\"longest\",\n  max_length=MAX_INPUT,\n  truncation=True,\n  return_tensors=\"pt\",\n)   # # Прогнозирование","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:13:50.724069Z","iopub.execute_input":"2024-04-04T16:13:50.724428Z","iopub.status.idle":"2024-04-04T16:13:50.736359Z","shell.execute_reply.started":"2024-04-04T16:13:50.724380Z","shell.execute_reply":"2024-04-04T16:13:50.735263Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"predicts = model.generate(**encoded.to(device))","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:13:52.768924Z","iopub.execute_input":"2024-04-04T16:13:52.770010Z","iopub.status.idle":"2024-04-04T16:13:56.523644Z","shell.execute_reply.started":"2024-04-04T16:13:52.769955Z","shell.execute_reply":"2024-04-04T16:13:56.522572Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py:1292: UserWarning: Using `max_length`'s default (256) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  UserWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]  # Декодируем данные","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:14:16.919739Z","iopub.execute_input":"2024-04-04T16:14:16.920586Z","iopub.status.idle":"2024-04-04T16:14:19.686824Z","shell.execute_reply.started":"2024-04-04T16:14:16.920544Z","shell.execute_reply":"2024-04-04T16:14:19.685743Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'Сегодня хорош день.'"},"metadata":{}}]},{"cell_type":"code","source":"TOKENIZERS_PARALLELISM=True","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:09:00.016844Z","iopub.execute_input":"2024-04-04T16:09:00.017705Z","iopub.status.idle":"2024-04-04T16:09:00.022526Z","shell.execute_reply.started":"2024-04-04T16:09:00.017664Z","shell.execute_reply":"2024-04-04T16:09:00.021428Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Зададим название выбронной модели из хаба\nMODEL_NAME = 'UrukHan/t5-russian-spell'\nMAX_INPUT = 256\n\n# Загрузка модели и токенизатора\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\ncorrector = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T04:01:25.759934Z","iopub.execute_input":"2024-04-06T04:01:25.760871Z","iopub.status.idle":"2024-04-06T04:01:47.049752Z","shell.execute_reply.started":"2024-04-06T04:01:25.760834Z","shell.execute_reply":"2024-04-06T04:01:47.048577Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"711fea152555473d8c63ef359013b87e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/1.00M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58d406bba5ab49c08cdf3fa9d161ee03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/2.63M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"818731939ecc4201852ab3b20e18a6d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6edb92899bf41f2b45ee0c1f59d68ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac5e43e6beaf4611be38d6b4cf58cac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9978e5113db4524b775e6ce41edfce1"}},"metadata":{}}]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        for j in range(len(decoded_preds)):\n    \n            input_sequences = decoded_preds[j]\n            \n            task_prefix = \"Spell correct: \"\n            \n            #TOKENIZERS_PARALLELISM=True\n            \n            if type(input_sequences) != list: input_sequences = [input_sequences]\n            encoded = tokenizer(\n              [task_prefix + sequence for sequence in input_sequences],\n              padding=\"longest\",\n              max_length=MAX_INPUT,\n              truncation=True,\n              return_tensors=\"pt\",\n            )\n            \n            predicts = corrector.generate(**encoded.to(device))\n            # # Прогнозирование\n            \n            input_sequences = tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]\n            \n            train_cer.append(cer(decoded_targets[j], input_sequences))\n            train_wer.append(wer(decoded_targets[j], input_sequences))\n    \n    avg_cer = sum(train_cer)/len(train_cer)\n    avg_wer = sum(train_wer)/len(train_wer)\n            \n    print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(train_loss, avg_cer, avg_wer))\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n            \n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n            \n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                \n                input_sequences = decoded_preds[j]\n                \n                task_prefix = \"Spell correct: \"\n                \n                if type(input_sequences) != list: input_sequences = [input_sequences]\n                encoded = tokenizer(\n                  [task_prefix + sequence for sequence in input_sequences],\n                  padding=\"longest\",\n                  max_length=MAX_INPUT,\n                  truncation=True,\n                  return_tensors=\"pt\",\n                )\n\n                predicts = corrector.generate(**encoded.to(device))   # # Прогнозирование\n                \n                input_sequences = tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]\n                \n                test_cer.append(cer(decoded_targets[j], input_sequences))\n                test_wer.append(wer(decoded_targets[j], input_sequences))\n    \n   \n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model.pt')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T04:02:08.253026Z","iopub.execute_input":"2024-04-06T04:02:08.253643Z","iopub.status.idle":"2024-04-06T04:02:08.288853Z","shell.execute_reply.started":"2024-04-06T04:02:08.253601Z","shell.execute_reply":"2024-04-06T04:02:08.287944Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        for j in range(len(decoded_preds)):\n            train_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n            train_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n    avg_cer = sum(train_cer)/len(train_cer)\n    avg_wer = sum(train_wer)/len(train_wer)\n            \n    print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(train_loss, avg_cer, avg_wer))\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n            \n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n            \n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n   \n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model.pt')","metadata":{"execution":{"iopub.status.busy":"2024-04-04T15:08:19.102512Z","iopub.execute_input":"2024-04-04T15:08:19.103352Z","iopub.status.idle":"2024-04-04T15:08:19.133682Z","shell.execute_reply.started":"2024-04-04T15:08:19.103283Z","shell.execute_reply":"2024-04-04T15:08:19.132398Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict(model, file_name, device):\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    sampl = librosa.load(file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    print(spectrogram_tensor.size())\n\n    with torch.no_grad():\n        spectrogram_tensor.to(device)\n        output = model(spectrogram_tensor)\n        print(output.size())\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-06T04:02:16.033559Z","iopub.execute_input":"2024-04-06T04:02:16.034491Z","iopub.status.idle":"2024-04-06T04:02:16.044865Z","shell.execute_reply.started":"2024-04-06T04:02:16.034450Z","shell.execute_reply":"2024-04-06T04:02:16.043716Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"%%time \nlearning_rate = 0.001\nbatch_size = 20\nepochs = 200\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-06T04:02:18.402649Z","iopub.execute_input":"2024-04-06T04:02:18.403494Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel1(\n  (conv): Sequential(\n    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Conv2d(1, 32, kernel_size=(4, 4), stride=(3, 3), padding=(2, 2))\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): GELU(approximate='none')\n    (4): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): GELU(approximate='none')\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): GELU(approximate='none')\n  )\n  (fc_1): Sequential(\n    (0): Linear(in_features=896, out_features=270, bias=True)\n    (1): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=270, out_features=270, bias=True)\n    (4): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (5): GELU(approximate='none')\n    (6): Linear(in_features=270, out_features=270, bias=True)\n    (7): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (8): GELU(approximate='none')\n  )\n  (BiGRU_1): BidirectionalGRU(\n    (BiGRU): GRU(270, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_2): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_3): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_4): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (fc_2): Sequential(\n    (0): Linear(in_features=540, out_features=35, bias=True)\n  )\n  (softmax): LogSoftmax(dim=2)\n)\nNum Model Parameters 5422923\nTrain Epoch: 1 [0/2430 (0%)]\tLoss: 18.544142\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py:1292: UserWarning: Using `max_length`'s default (256) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  UserWarning,\n","output_type":"stream"},{"name":"stdout","text":"Train Epoch: 1 [400/2430 (16%)]\tLoss: 3.783701\nTrain Epoch: 1 [800/2430 (33%)]\tLoss: 3.692682\nTrain Epoch: 1 [1200/2430 (49%)]\tLoss: 3.508759\nTrain Epoch: 1 [1600/2430 (66%)]\tLoss: 3.398024\nTrain Epoch: 1 [2000/2430 (82%)]\tLoss: 3.355687\nTrain Epoch: 1 [2400/2430 (98%)]\tLoss: 3.294183\nTrain set:\tAverage loss: 4.3755, Average CER: 1.550245 Average WER: 2.2454\n\n\nevaluating...\nTest set:\tAverage loss: 3.3050, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 2 [0/2430 (0%)]\tLoss: 3.241162\nTrain Epoch: 2 [400/2430 (16%)]\tLoss: 3.309129\nTrain Epoch: 2 [800/2430 (33%)]\tLoss: 3.344159\nTrain Epoch: 2 [1200/2430 (49%)]\tLoss: 3.266047\nTrain Epoch: 2 [1600/2430 (66%)]\tLoss: 3.293651\nTrain Epoch: 2 [2000/2430 (82%)]\tLoss: 3.290851\nTrain Epoch: 2 [2400/2430 (98%)]\tLoss: 3.306544\nTrain set:\tAverage loss: 3.2983, Average CER: 1.000000 Average WER: 1.0000\n\n\nevaluating...\nTest set:\tAverage loss: 3.2867, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 3 [0/2430 (0%)]\tLoss: 3.267120\nTrain Epoch: 3 [400/2430 (16%)]\tLoss: 3.252027\nTrain Epoch: 3 [800/2430 (33%)]\tLoss: 3.208513\nTrain Epoch: 3 [1200/2430 (49%)]\tLoss: 3.270213\nTrain Epoch: 3 [1600/2430 (66%)]\tLoss: 3.242117\nTrain Epoch: 3 [2000/2430 (82%)]\tLoss: 3.246315\nTrain Epoch: 3 [2400/2430 (98%)]\tLoss: 3.178853\nTrain set:\tAverage loss: 3.2273, Average CER: 1.000000 Average WER: 1.0000\n\n\nevaluating...\nTest set:\tAverage loss: 3.1234, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 4 [0/2430 (0%)]\tLoss: 3.214369\nTrain Epoch: 4 [400/2430 (16%)]\tLoss: 3.098092\nTrain Epoch: 4 [800/2430 (33%)]\tLoss: 3.008423\nTrain Epoch: 4 [1200/2430 (49%)]\tLoss: 3.032790\nTrain Epoch: 4 [1600/2430 (66%)]\tLoss: 2.886902\nTrain Epoch: 4 [2000/2430 (82%)]\tLoss: 2.830026\nTrain Epoch: 4 [2400/2430 (98%)]\tLoss: 2.729499\nTrain set:\tAverage loss: 2.9773, Average CER: 0.999961 Average WER: 1.0000\n\n\nevaluating...\nTest set:\tAverage loss: 2.7441, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 5 [0/2430 (0%)]\tLoss: 2.715666\nTrain Epoch: 5 [400/2430 (16%)]\tLoss: 2.725784\nTrain Epoch: 5 [800/2430 (33%)]\tLoss: 2.616856\nTrain Epoch: 5 [1200/2430 (49%)]\tLoss: 2.414215\nTrain Epoch: 5 [1600/2430 (66%)]\tLoss: 2.354939\nTrain Epoch: 5 [2000/2430 (82%)]\tLoss: 2.349642\nTrain Epoch: 5 [2400/2430 (98%)]\tLoss: 2.155490\nTrain set:\tAverage loss: 2.4750, Average CER: 0.987120 Average WER: 1.0070\n\n\nevaluating...\nTest set:\tAverage loss: 2.1322, Average CER: 0.925383 Average WER: 1.0231\n\nTrain Epoch: 6 [0/2430 (0%)]\tLoss: 2.175934\nTrain Epoch: 6 [400/2430 (16%)]\tLoss: 1.972745\nTrain Epoch: 6 [800/2430 (33%)]\tLoss: 1.957134\nTrain Epoch: 6 [1200/2430 (49%)]\tLoss: 1.830543\nTrain Epoch: 6 [1600/2430 (66%)]\tLoss: 1.817176\nTrain Epoch: 6 [2000/2430 (82%)]\tLoss: 1.682653\nTrain Epoch: 6 [2400/2430 (98%)]\tLoss: 1.695280\nTrain set:\tAverage loss: 1.8492, Average CER: 0.830210 Average WER: 1.0163\n\n\nevaluating...\nTest set:\tAverage loss: 1.5971, Average CER: 0.679064 Average WER: 1.0032\n\nTrain Epoch: 7 [0/2430 (0%)]\tLoss: 1.499320\nTrain Epoch: 7 [400/2430 (16%)]\tLoss: 1.292691\nTrain Epoch: 7 [800/2430 (33%)]\tLoss: 1.485161\nTrain Epoch: 7 [1200/2430 (49%)]\tLoss: 1.511835\nTrain Epoch: 7 [1600/2430 (66%)]\tLoss: 1.334727\nTrain Epoch: 7 [2000/2430 (82%)]\tLoss: 1.379011\nTrain Epoch: 7 [2400/2430 (98%)]\tLoss: 1.307935\nTrain set:\tAverage loss: 1.4070, Average CER: 0.638495 Average WER: 0.9975\n\n\nevaluating...\nTest set:\tAverage loss: 1.3418, Average CER: 0.608649 Average WER: 1.0016\n\nTrain Epoch: 8 [0/2430 (0%)]\tLoss: 1.222022\nTrain Epoch: 8 [400/2430 (16%)]\tLoss: 1.118157\nTrain Epoch: 8 [800/2430 (33%)]\tLoss: 0.926198\nTrain Epoch: 8 [1200/2430 (49%)]\tLoss: 1.115786\nTrain Epoch: 8 [1600/2430 (66%)]\tLoss: 0.984412\nTrain Epoch: 8 [2000/2430 (82%)]\tLoss: 1.122176\nTrain Epoch: 8 [2400/2430 (98%)]\tLoss: 1.116913\nTrain set:\tAverage loss: 1.1266, Average CER: 0.565929 Average WER: 0.9675\n\n\nevaluating...\nTest set:\tAverage loss: 1.1671, Average CER: 0.502180 Average WER: 0.9677\n\nTrain Epoch: 9 [0/2430 (0%)]\tLoss: 1.092450\nTrain Epoch: 9 [400/2430 (16%)]\tLoss: 0.826645\nTrain Epoch: 9 [800/2430 (33%)]\tLoss: 0.841510\nTrain Epoch: 9 [1200/2430 (49%)]\tLoss: 0.851623\nTrain Epoch: 9 [1600/2430 (66%)]\tLoss: 0.854493\nTrain Epoch: 9 [2000/2430 (82%)]\tLoss: 0.932618\nTrain Epoch: 9 [2400/2430 (98%)]\tLoss: 0.850363\nTrain set:\tAverage loss: 0.9266, Average CER: 0.488296 Average WER: 0.9257\n\n\nevaluating...\nTest set:\tAverage loss: 1.0766, Average CER: 0.487052 Average WER: 0.9260\n\nTrain Epoch: 10 [0/2430 (0%)]\tLoss: 0.873437\nTrain Epoch: 10 [400/2430 (16%)]\tLoss: 1.011755\nTrain Epoch: 10 [800/2430 (33%)]\tLoss: 0.767068\nTrain Epoch: 10 [1200/2430 (49%)]\tLoss: 0.926038\nTrain Epoch: 10 [1600/2430 (66%)]\tLoss: 0.936447\nTrain Epoch: 10 [2000/2430 (82%)]\tLoss: 0.630437\nTrain Epoch: 10 [2400/2430 (98%)]\tLoss: 0.644091\nTrain set:\tAverage loss: 0.7902, Average CER: 0.450879 Average WER: 0.9030\n\n\nevaluating...\nTest set:\tAverage loss: 1.0233, Average CER: 0.480800 Average WER: 0.9367\n\nTrain Epoch: 11 [0/2430 (0%)]\tLoss: 0.639958\nTrain Epoch: 11 [400/2430 (16%)]\tLoss: 0.555588\nTrain Epoch: 11 [800/2430 (33%)]\tLoss: 0.609572\nTrain Epoch: 11 [1200/2430 (49%)]\tLoss: 0.654503\nTrain Epoch: 11 [1600/2430 (66%)]\tLoss: 0.594929\nTrain Epoch: 11 [2000/2430 (82%)]\tLoss: 0.763455\nTrain Epoch: 11 [2400/2430 (98%)]\tLoss: 0.645488\nTrain set:\tAverage loss: 0.6795, Average CER: 0.398983 Average WER: 0.8735\n\n\nevaluating...\nTest set:\tAverage loss: 0.9943, Average CER: 0.476798 Average WER: 0.9152\n\nTrain Epoch: 12 [0/2430 (0%)]\tLoss: 0.467485\nTrain Epoch: 12 [400/2430 (16%)]\tLoss: 0.590039\nTrain Epoch: 12 [800/2430 (33%)]\tLoss: 0.533066\nTrain Epoch: 12 [1200/2430 (49%)]\tLoss: 0.636880\nTrain Epoch: 12 [1600/2430 (66%)]\tLoss: 0.573742\nTrain Epoch: 12 [2000/2430 (82%)]\tLoss: 0.632788\nTrain Epoch: 12 [2400/2430 (98%)]\tLoss: 0.653727\nTrain set:\tAverage loss: 0.5821, Average CER: 0.370115 Average WER: 0.8344\n\n\nevaluating...\nTest set:\tAverage loss: 0.9714, Average CER: 0.433590 Average WER: 0.8877\n\nTrain Epoch: 13 [0/2430 (0%)]\tLoss: 0.499723\nTrain Epoch: 13 [400/2430 (16%)]\tLoss: 0.439866\nTrain Epoch: 13 [800/2430 (33%)]\tLoss: 0.474642\nTrain Epoch: 13 [1200/2430 (49%)]\tLoss: 0.460783\nTrain Epoch: 13 [1600/2430 (66%)]\tLoss: 0.524842\nTrain Epoch: 13 [2000/2430 (82%)]\tLoss: 0.556584\nTrain Epoch: 13 [2400/2430 (98%)]\tLoss: 0.596365\nTrain set:\tAverage loss: 0.5151, Average CER: 0.330891 Average WER: 0.8179\n\n\nevaluating...\nTest set:\tAverage loss: 0.9945, Average CER: 0.440704 Average WER: 0.8944\n\nTrain Epoch: 14 [0/2430 (0%)]\tLoss: 0.443754\nTrain Epoch: 14 [400/2430 (16%)]\tLoss: 0.520834\nTrain Epoch: 14 [800/2430 (33%)]\tLoss: 0.520517\nTrain Epoch: 14 [1200/2430 (49%)]\tLoss: 0.388102\nTrain Epoch: 14 [1600/2430 (66%)]\tLoss: 0.461279\nTrain Epoch: 14 [2000/2430 (82%)]\tLoss: 0.362757\nTrain Epoch: 14 [2400/2430 (98%)]\tLoss: 0.512797\nTrain set:\tAverage loss: 0.4506, Average CER: 0.292625 Average WER: 0.8131\n\n\nevaluating...\nTest set:\tAverage loss: 0.9759, Average CER: 0.456077 Average WER: 0.9187\n\nTrain Epoch: 15 [0/2430 (0%)]\tLoss: 0.346545\nTrain Epoch: 15 [400/2430 (16%)]\tLoss: 0.243758\nTrain Epoch: 15 [800/2430 (33%)]\tLoss: 0.411738\nTrain Epoch: 15 [1200/2430 (49%)]\tLoss: 0.396445\nTrain Epoch: 15 [1600/2430 (66%)]\tLoss: 0.475046\nTrain Epoch: 15 [2000/2430 (82%)]\tLoss: 0.303775\nTrain Epoch: 15 [2400/2430 (98%)]\tLoss: 0.382166\nTrain set:\tAverage loss: 0.3890, Average CER: 0.263417 Average WER: 0.7751\n\n\nevaluating...\nTest set:\tAverage loss: 0.9781, Average CER: 0.429522 Average WER: 0.8904\n\nTrain Epoch: 16 [0/2430 (0%)]\tLoss: 0.357908\nTrain Epoch: 16 [400/2430 (16%)]\tLoss: 0.247895\nTrain Epoch: 16 [800/2430 (33%)]\tLoss: 0.289391\nTrain Epoch: 16 [1200/2430 (49%)]\tLoss: 0.420347\nTrain Epoch: 16 [1600/2430 (66%)]\tLoss: 0.296479\nTrain Epoch: 16 [2000/2430 (82%)]\tLoss: 0.330510\nTrain Epoch: 16 [2400/2430 (98%)]\tLoss: 0.293788\nTrain set:\tAverage loss: 0.3338, Average CER: 0.246681 Average WER: 0.7605\n\n\nevaluating...\nTest set:\tAverage loss: 1.0240, Average CER: 0.458108 Average WER: 0.9378\n\nTrain Epoch: 17 [0/2430 (0%)]\tLoss: 0.270751\nTrain Epoch: 17 [400/2430 (16%)]\tLoss: 0.292247\nTrain Epoch: 17 [800/2430 (33%)]\tLoss: 0.289446\nTrain Epoch: 17 [1200/2430 (49%)]\tLoss: 0.228083\nTrain Epoch: 17 [1600/2430 (66%)]\tLoss: 0.215717\nTrain Epoch: 17 [2000/2430 (82%)]\tLoss: 0.250718\nTrain Epoch: 17 [2400/2430 (98%)]\tLoss: 0.322049\nTrain set:\tAverage loss: 0.2983, Average CER: 0.238279 Average WER: 0.7592\n\n\nevaluating...\nTest set:\tAverage loss: 1.0235, Average CER: 0.439810 Average WER: 0.8770\n\nTrain Epoch: 18 [0/2430 (0%)]\tLoss: 0.224279\nTrain Epoch: 18 [400/2430 (16%)]\tLoss: 0.193541\nTrain Epoch: 18 [800/2430 (33%)]\tLoss: 0.281238\nTrain Epoch: 18 [1200/2430 (49%)]\tLoss: 0.330179\nTrain Epoch: 18 [1600/2430 (66%)]\tLoss: 0.214277\nTrain Epoch: 18 [2000/2430 (82%)]\tLoss: 0.259419\nTrain Epoch: 18 [2400/2430 (98%)]\tLoss: 0.285759\nTrain set:\tAverage loss: 0.2692, Average CER: 0.226553 Average WER: 0.7454\n\n\nevaluating...\nTest set:\tAverage loss: 1.0444, Average CER: 0.478387 Average WER: 0.9055\n\nTrain Epoch: 19 [0/2430 (0%)]\tLoss: 0.180385\nTrain Epoch: 19 [400/2430 (16%)]\tLoss: 0.196029\nTrain Epoch: 19 [800/2430 (33%)]\tLoss: 0.245056\nTrain Epoch: 19 [1200/2430 (49%)]\tLoss: 0.203773\nTrain Epoch: 19 [1600/2430 (66%)]\tLoss: 0.200585\nTrain Epoch: 19 [2000/2430 (82%)]\tLoss: 0.235882\nTrain Epoch: 19 [2400/2430 (98%)]\tLoss: 0.247038\nTrain set:\tAverage loss: 0.2199, Average CER: 0.203149 Average WER: 0.7260\n\n\nevaluating...\nTest set:\tAverage loss: 1.0851, Average CER: 0.577815 Average WER: 0.9266\n\nTrain Epoch: 20 [0/2430 (0%)]\tLoss: 0.268320\nTrain Epoch: 20 [400/2430 (16%)]\tLoss: 0.317386\nTrain Epoch: 20 [800/2430 (33%)]\tLoss: 0.199551\nTrain Epoch: 20 [1200/2430 (49%)]\tLoss: 0.254747\nTrain Epoch: 20 [1600/2430 (66%)]\tLoss: 0.230828\nTrain Epoch: 20 [2000/2430 (82%)]\tLoss: 0.224554\nTrain Epoch: 20 [2400/2430 (98%)]\tLoss: 0.213437\nTrain set:\tAverage loss: 0.2223, Average CER: 0.203147 Average WER: 0.7232\n\n\nevaluating...\nTest set:\tAverage loss: 1.1744, Average CER: 0.457212 Average WER: 0.9574\n\nTrain Epoch: 21 [0/2430 (0%)]\tLoss: 0.306804\nTrain Epoch: 21 [400/2430 (16%)]\tLoss: 0.366643\nTrain Epoch: 21 [800/2430 (33%)]\tLoss: 0.169072\nTrain Epoch: 21 [1200/2430 (49%)]\tLoss: 0.283114\nTrain Epoch: 21 [1600/2430 (66%)]\tLoss: 0.205448\nTrain Epoch: 21 [2000/2430 (82%)]\tLoss: 0.353436\nTrain Epoch: 21 [2400/2430 (98%)]\tLoss: 0.515422\nTrain set:\tAverage loss: 0.2364, Average CER: 0.223634 Average WER: 0.7262\n\n\nevaluating...\nTest set:\tAverage loss: 1.1933, Average CER: 0.517641 Average WER: 0.9302\n\nTrain Epoch: 22 [0/2430 (0%)]\tLoss: 0.246191\nTrain Epoch: 22 [400/2430 (16%)]\tLoss: 0.303462\nTrain Epoch: 22 [800/2430 (33%)]\tLoss: 0.288742\nTrain Epoch: 22 [1200/2430 (49%)]\tLoss: 0.300942\nTrain Epoch: 22 [1600/2430 (66%)]\tLoss: 0.185412\nTrain Epoch: 22 [2000/2430 (82%)]\tLoss: 0.201756\nTrain Epoch: 22 [2400/2430 (98%)]\tLoss: 0.239767\nTrain set:\tAverage loss: 0.2569, Average CER: 0.227713 Average WER: 0.7408\n\n\nevaluating...\nTest set:\tAverage loss: 1.1265, Average CER: 0.474961 Average WER: 0.9071\n\nTrain Epoch: 23 [0/2430 (0%)]\tLoss: 0.227304\nTrain Epoch: 23 [400/2430 (16%)]\tLoss: 0.206191\nTrain Epoch: 23 [800/2430 (33%)]\tLoss: 0.168983\nTrain Epoch: 23 [1200/2430 (49%)]\tLoss: 0.152698\nTrain Epoch: 23 [1600/2430 (66%)]\tLoss: 0.209212\nTrain Epoch: 23 [2000/2430 (82%)]\tLoss: 0.115361\nTrain Epoch: 23 [2400/2430 (98%)]\tLoss: 0.214887\nTrain set:\tAverage loss: 0.1802, Average CER: 0.195330 Average WER: 0.7144\n\n\nevaluating...\nTest set:\tAverage loss: 1.1109, Average CER: 0.465623 Average WER: 0.8684\n\nTrain Epoch: 24 [0/2430 (0%)]\tLoss: 0.191244\nTrain Epoch: 24 [400/2430 (16%)]\tLoss: 0.105054\nTrain Epoch: 24 [800/2430 (33%)]\tLoss: 0.104302\nTrain Epoch: 24 [1200/2430 (49%)]\tLoss: 0.144443\nTrain Epoch: 24 [1600/2430 (66%)]\tLoss: 0.168205\nTrain Epoch: 24 [2000/2430 (82%)]\tLoss: 0.129679\nTrain Epoch: 24 [2400/2430 (98%)]\tLoss: 0.227826\nTrain set:\tAverage loss: 0.1302, Average CER: 0.176008 Average WER: 0.7168\n\n\nevaluating...\nTest set:\tAverage loss: 1.1112, Average CER: 0.438152 Average WER: 0.9180\n\nTrain Epoch: 25 [0/2430 (0%)]\tLoss: 0.077058\nTrain Epoch: 25 [400/2430 (16%)]\tLoss: 0.115773\nTrain Epoch: 25 [800/2430 (33%)]\tLoss: 0.141962\nTrain Epoch: 25 [1200/2430 (49%)]\tLoss: 0.182614\nTrain Epoch: 25 [1600/2430 (66%)]\tLoss: 0.137140\nTrain Epoch: 25 [2000/2430 (82%)]\tLoss: 0.178238\nTrain Epoch: 25 [2400/2430 (98%)]\tLoss: 0.346771\nTrain set:\tAverage loss: 0.1772, Average CER: 0.192559 Average WER: 0.7096\n\n\nevaluating...\nTest set:\tAverage loss: 1.2329, Average CER: 0.454379 Average WER: 0.9177\n\nTrain Epoch: 26 [0/2430 (0%)]\tLoss: 0.347945\nTrain Epoch: 26 [400/2430 (16%)]\tLoss: 0.245638\nTrain Epoch: 26 [800/2430 (33%)]\tLoss: 0.153106\nTrain Epoch: 26 [1200/2430 (49%)]\tLoss: 0.199080\nTrain Epoch: 26 [1600/2430 (66%)]\tLoss: 0.148644\nTrain Epoch: 26 [2000/2430 (82%)]\tLoss: 0.215539\nTrain Epoch: 26 [2400/2430 (98%)]\tLoss: 0.185747\nTrain set:\tAverage loss: 0.2148, Average CER: 0.214323 Average WER: 0.7206\n\n\nevaluating...\nTest set:\tAverage loss: 1.0836, Average CER: 0.421733 Average WER: 0.8801\n\nTrain Epoch: 27 [0/2430 (0%)]\tLoss: 0.089793\nTrain Epoch: 27 [400/2430 (16%)]\tLoss: 0.142049\nTrain Epoch: 27 [800/2430 (33%)]\tLoss: 0.147474\nTrain Epoch: 27 [1200/2430 (49%)]\tLoss: 0.171740\nTrain Epoch: 27 [1600/2430 (66%)]\tLoss: 0.212653\nTrain Epoch: 27 [2000/2430 (82%)]\tLoss: 0.123571\nTrain Epoch: 27 [2400/2430 (98%)]\tLoss: 0.133225\nTrain set:\tAverage loss: 0.1476, Average CER: 0.184188 Average WER: 0.7030\n\n\nevaluating...\nTest set:\tAverage loss: 1.1903, Average CER: 0.519113 Average WER: 0.9093\n\nTrain Epoch: 28 [0/2430 (0%)]\tLoss: 0.107552\nTrain Epoch: 28 [400/2430 (16%)]\tLoss: 0.146767\nTrain Epoch: 28 [800/2430 (33%)]\tLoss: 0.151942\nTrain Epoch: 28 [1200/2430 (49%)]\tLoss: 0.163293\nTrain Epoch: 28 [1600/2430 (66%)]\tLoss: 0.252238\nTrain Epoch: 28 [2000/2430 (82%)]\tLoss: 0.196287\nTrain Epoch: 28 [2400/2430 (98%)]\tLoss: 0.190699\nTrain set:\tAverage loss: 0.1750, Average CER: 0.201106 Average WER: 0.7144\n\n\nevaluating...\nTest set:\tAverage loss: 1.0929, Average CER: 0.420273 Average WER: 0.8878\n\nTrain Epoch: 29 [0/2430 (0%)]\tLoss: 0.120555\nTrain Epoch: 29 [400/2430 (16%)]\tLoss: 0.125590\nTrain Epoch: 29 [800/2430 (33%)]\tLoss: 0.070404\nTrain Epoch: 29 [1200/2430 (49%)]\tLoss: 0.094987\nTrain Epoch: 29 [1600/2430 (66%)]\tLoss: 0.100614\nTrain Epoch: 29 [2000/2430 (82%)]\tLoss: 0.136464\nTrain Epoch: 29 [2400/2430 (98%)]\tLoss: 0.186333\nTrain set:\tAverage loss: 0.1320, Average CER: 0.169844 Average WER: 0.6945\n\n\nevaluating...\nTest set:\tAverage loss: 1.1914, Average CER: 0.446366 Average WER: 0.8885\n\nTrain Epoch: 30 [0/2430 (0%)]\tLoss: 0.095341\nTrain Epoch: 30 [400/2430 (16%)]\tLoss: 0.083998\nTrain Epoch: 30 [800/2430 (33%)]\tLoss: 0.073326\nTrain Epoch: 30 [1200/2430 (49%)]\tLoss: 0.115922\nTrain Epoch: 30 [1600/2430 (66%)]\tLoss: 0.142505\nTrain Epoch: 30 [2000/2430 (82%)]\tLoss: 0.129024\nTrain Epoch: 30 [2400/2430 (98%)]\tLoss: 0.092391\nTrain set:\tAverage loss: 0.1182, Average CER: 0.176255 Average WER: 0.6882\n\n\nevaluating...\nTest set:\tAverage loss: 1.1778, Average CER: 0.420558 Average WER: 0.9030\n\nTrain Epoch: 31 [0/2430 (0%)]\tLoss: 0.046028\nTrain Epoch: 31 [400/2430 (16%)]\tLoss: 0.068397\nTrain Epoch: 31 [800/2430 (33%)]\tLoss: 0.121219\nTrain Epoch: 31 [1200/2430 (49%)]\tLoss: 0.186819\nTrain Epoch: 31 [1600/2430 (66%)]\tLoss: 0.128622\nTrain Epoch: 31 [2000/2430 (82%)]\tLoss: 0.189451\nTrain Epoch: 31 [2400/2430 (98%)]\tLoss: 0.214175\nTrain set:\tAverage loss: 0.1561, Average CER: 0.180303 Average WER: 0.6998\n\n\nevaluating...\nTest set:\tAverage loss: 1.1747, Average CER: 0.411914 Average WER: 0.9088\n\nTrain Epoch: 32 [0/2430 (0%)]\tLoss: 0.219521\nTrain Epoch: 32 [400/2430 (16%)]\tLoss: 0.145855\nTrain Epoch: 32 [800/2430 (33%)]\tLoss: 0.198739\nTrain Epoch: 32 [1200/2430 (49%)]\tLoss: 0.195455\nTrain Epoch: 32 [1600/2430 (66%)]\tLoss: 0.222475\nTrain Epoch: 32 [2000/2430 (82%)]\tLoss: 0.155021\nTrain Epoch: 32 [2400/2430 (98%)]\tLoss: 0.198002\nTrain set:\tAverage loss: 0.1968, Average CER: 0.205506 Average WER: 0.7147\n\n\nevaluating...\nTest set:\tAverage loss: 1.1370, Average CER: 0.407452 Average WER: 0.8961\n\nTrain Epoch: 33 [0/2430 (0%)]\tLoss: 0.104479\nTrain Epoch: 33 [400/2430 (16%)]\tLoss: 0.172786\nTrain Epoch: 33 [800/2430 (33%)]\tLoss: 0.113722\nTrain Epoch: 33 [1200/2430 (49%)]\tLoss: 0.106755\nTrain Epoch: 33 [1600/2430 (66%)]\tLoss: 0.110355\nTrain Epoch: 33 [2000/2430 (82%)]\tLoss: 0.106356\nTrain Epoch: 33 [2400/2430 (98%)]\tLoss: 0.159563\nTrain set:\tAverage loss: 0.1258, Average CER: 0.188906 Average WER: 0.6958\n\n\nevaluating...\nTest set:\tAverage loss: 1.1473, Average CER: 0.441274 Average WER: 0.8982\n\nTrain Epoch: 34 [0/2430 (0%)]\tLoss: 0.040618\nTrain Epoch: 34 [400/2430 (16%)]\tLoss: 0.046776\nTrain Epoch: 34 [800/2430 (33%)]\tLoss: 0.111614\nTrain Epoch: 34 [1200/2430 (49%)]\tLoss: 0.107716\nTrain Epoch: 34 [1600/2430 (66%)]\tLoss: 0.115199\nTrain Epoch: 34 [2000/2430 (82%)]\tLoss: 0.127527\nTrain Epoch: 34 [2400/2430 (98%)]\tLoss: 0.174272\nTrain set:\tAverage loss: 0.1188, Average CER: 0.176144 Average WER: 0.6889\n\n\nevaluating...\nTest set:\tAverage loss: 1.2267, Average CER: 0.439245 Average WER: 0.9150\n\nTrain Epoch: 35 [0/2430 (0%)]\tLoss: 0.121418\nTrain Epoch: 35 [400/2430 (16%)]\tLoss: 0.150595\nTrain Epoch: 35 [800/2430 (33%)]\tLoss: 0.110643\nTrain Epoch: 35 [1200/2430 (49%)]\tLoss: 0.160610\nTrain Epoch: 35 [1600/2430 (66%)]\tLoss: 0.120603\nTrain Epoch: 35 [2000/2430 (82%)]\tLoss: 0.227154\nTrain Epoch: 35 [2400/2430 (98%)]\tLoss: 0.195921\nTrain set:\tAverage loss: 0.1527, Average CER: 0.184134 Average WER: 0.7025\n\n\nevaluating...\nTest set:\tAverage loss: 1.2224, Average CER: 0.449924 Average WER: 0.9514\n\nTrain Epoch: 36 [0/2430 (0%)]\tLoss: 0.259448\nTrain Epoch: 36 [400/2430 (16%)]\tLoss: 0.221221\nTrain Epoch: 36 [800/2430 (33%)]\tLoss: 0.250045\nTrain Epoch: 36 [1200/2430 (49%)]\tLoss: 0.182494\nTrain Epoch: 36 [1600/2430 (66%)]\tLoss: 0.208259\nTrain Epoch: 36 [2000/2430 (82%)]\tLoss: 0.242083\nTrain Epoch: 36 [2400/2430 (98%)]\tLoss: 0.159956\nTrain set:\tAverage loss: 0.1739, Average CER: 0.190454 Average WER: 0.7153\n\n\nevaluating...\nTest set:\tAverage loss: 1.1468, Average CER: 0.421600 Average WER: 0.9058\n\nTrain Epoch: 37 [0/2430 (0%)]\tLoss: 0.105094\nTrain Epoch: 37 [400/2430 (16%)]\tLoss: 0.101063\nTrain Epoch: 37 [800/2430 (33%)]\tLoss: 0.081925\nTrain Epoch: 37 [1200/2430 (49%)]\tLoss: 0.117053\nTrain Epoch: 37 [1600/2430 (66%)]\tLoss: 0.116874\nTrain Epoch: 37 [2000/2430 (82%)]\tLoss: 0.062401\nTrain Epoch: 37 [2400/2430 (98%)]\tLoss: 0.105834\nTrain set:\tAverage loss: 0.1222, Average CER: 0.171459 Average WER: 0.6907\n\n\nevaluating...\nTest set:\tAverage loss: 1.1368, Average CER: 0.394376 Average WER: 0.8810\n\nTrain Epoch: 38 [0/2430 (0%)]\tLoss: 0.087363\nTrain Epoch: 38 [400/2430 (16%)]\tLoss: 0.068678\nTrain Epoch: 38 [800/2430 (33%)]\tLoss: 0.090503\nTrain Epoch: 38 [1200/2430 (49%)]\tLoss: 0.101148\nTrain Epoch: 38 [1600/2430 (66%)]\tLoss: 0.105692\nTrain Epoch: 38 [2000/2430 (82%)]\tLoss: 0.077180\nTrain Epoch: 38 [2400/2430 (98%)]\tLoss: 0.115218\nTrain set:\tAverage loss: 0.0970, Average CER: 0.164131 Average WER: 0.6846\n\n\nevaluating...\nTest set:\tAverage loss: 1.2235, Average CER: 0.427670 Average WER: 0.8960\n\nTrain Epoch: 39 [0/2430 (0%)]\tLoss: 0.109095\nTrain Epoch: 39 [400/2430 (16%)]\tLoss: 0.107626\nTrain Epoch: 39 [800/2430 (33%)]\tLoss: 0.070567\nTrain Epoch: 39 [1200/2430 (49%)]\tLoss: 0.126991\nTrain Epoch: 39 [1600/2430 (66%)]\tLoss: 0.068547\nTrain Epoch: 39 [2000/2430 (82%)]\tLoss: 0.154627\nTrain Epoch: 39 [2400/2430 (98%)]\tLoss: 0.247636\nTrain set:\tAverage loss: 0.1146, Average CER: 0.614230 Average WER: 0.6949\n\n\nevaluating...\nTest set:\tAverage loss: 1.2102, Average CER: 1.039202 Average WER: 0.9663\n\nTrain Epoch: 40 [0/2430 (0%)]\tLoss: 0.136239\nTrain Epoch: 40 [400/2430 (16%)]\tLoss: 0.123998\nTrain Epoch: 40 [800/2430 (33%)]\tLoss: 0.136513\nTrain Epoch: 40 [1200/2430 (49%)]\tLoss: 0.137292\nTrain Epoch: 40 [1600/2430 (66%)]\tLoss: 0.071772\nTrain Epoch: 40 [2000/2430 (82%)]\tLoss: 0.139761\nTrain Epoch: 40 [2400/2430 (98%)]\tLoss: 0.134649\nTrain set:\tAverage loss: 0.1354, Average CER: 0.321668 Average WER: 0.6954\n\n\nevaluating...\nTest set:\tAverage loss: 1.1517, Average CER: 0.380956 Average WER: 0.8743\n\nTrain Epoch: 41 [0/2430 (0%)]\tLoss: 0.124247\nTrain Epoch: 41 [400/2430 (16%)]\tLoss: 0.068273\nTrain Epoch: 41 [800/2430 (33%)]\tLoss: 0.091711\nTrain Epoch: 41 [1200/2430 (49%)]\tLoss: 0.119178\nTrain Epoch: 41 [1600/2430 (66%)]\tLoss: 0.145135\nTrain Epoch: 41 [2000/2430 (82%)]\tLoss: 0.168624\nTrain Epoch: 41 [2400/2430 (98%)]\tLoss: 0.163915\nTrain set:\tAverage loss: 0.1057, Average CER: 0.168049 Average WER: 0.6885\n\n\nevaluating...\nTest set:\tAverage loss: 1.1441, Average CER: 0.410443 Average WER: 0.8730\n\nTrain Epoch: 42 [0/2430 (0%)]\tLoss: 0.099240\nTrain Epoch: 42 [400/2430 (16%)]\tLoss: 0.147009\nTrain Epoch: 42 [800/2430 (33%)]\tLoss: 0.053168\nTrain Epoch: 42 [1200/2430 (49%)]\tLoss: 0.089217\nTrain Epoch: 42 [1600/2430 (66%)]\tLoss: 0.139788\nTrain Epoch: 42 [2000/2430 (82%)]\tLoss: 0.070315\nTrain Epoch: 42 [2400/2430 (98%)]\tLoss: 0.139466\nTrain set:\tAverage loss: 0.1093, Average CER: 0.165956 Average WER: 0.6886\n\n\nevaluating...\nTest set:\tAverage loss: 1.3502, Average CER: 0.409214 Average WER: 0.8909\n\nTrain Epoch: 43 [0/2430 (0%)]\tLoss: 0.125113\nTrain Epoch: 43 [400/2430 (16%)]\tLoss: 0.106370\nTrain Epoch: 43 [800/2430 (33%)]\tLoss: 0.126899\nTrain Epoch: 43 [1200/2430 (49%)]\tLoss: 0.102843\nTrain Epoch: 43 [1600/2430 (66%)]\tLoss: 0.101401\nTrain Epoch: 43 [2000/2430 (82%)]\tLoss: 0.166165\nTrain Epoch: 43 [2400/2430 (98%)]\tLoss: 0.172495\nTrain set:\tAverage loss: 0.1394, Average CER: 0.178969 Average WER: 0.6982\n\n\nevaluating...\nTest set:\tAverage loss: 1.1303, Average CER: 0.393037 Average WER: 0.8510\n\nTrain Epoch: 44 [0/2430 (0%)]\tLoss: 0.113916\nTrain Epoch: 44 [400/2430 (16%)]\tLoss: 0.090887\nTrain Epoch: 44 [800/2430 (33%)]\tLoss: 0.103329\nTrain Epoch: 44 [1200/2430 (49%)]\tLoss: 0.149705\nTrain Epoch: 44 [1600/2430 (66%)]\tLoss: 0.138742\nTrain Epoch: 44 [2000/2430 (82%)]\tLoss: 0.088777\nTrain Epoch: 44 [2400/2430 (98%)]\tLoss: 0.100116\nTrain set:\tAverage loss: 0.1280, Average CER: 0.179825 Average WER: 0.7010\n\n\nevaluating...\nTest set:\tAverage loss: 1.2012, Average CER: 0.419487 Average WER: 0.8642\n\nTrain Epoch: 45 [0/2430 (0%)]\tLoss: 0.272531\nTrain Epoch: 45 [400/2430 (16%)]\tLoss: 0.178561\nTrain Epoch: 45 [800/2430 (33%)]\tLoss: 0.199771\nTrain Epoch: 45 [1200/2430 (49%)]\tLoss: 0.153694\nTrain Epoch: 45 [1600/2430 (66%)]\tLoss: 0.144403\nTrain Epoch: 45 [2000/2430 (82%)]\tLoss: 0.138930\nTrain Epoch: 45 [2400/2430 (98%)]\tLoss: 0.189226\nTrain set:\tAverage loss: 0.1574, Average CER: 0.185285 Average WER: 0.7063\n\n\nevaluating...\nTest set:\tAverage loss: 1.1784, Average CER: 0.455660 Average WER: 0.9236\n\nTrain Epoch: 46 [0/2430 (0%)]\tLoss: 0.181934\nTrain Epoch: 46 [400/2430 (16%)]\tLoss: 0.129465\n","output_type":"stream"}]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cpu\")\n\nmodel = torch.load('/kaggle/working/model.pt')\n\n#1543 1882 1372\n\nmodel.to(device)\npredict(model, '/kaggle/input/upd-speech/mono_voice/1964.wav', device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:26:24.339488Z","iopub.execute_input":"2023-05-24T10:26:24.340405Z","iopub.status.idle":"2023-05-24T10:26:24.350954Z","shell.execute_reply.started":"2023-05-24T10:26:24.340364Z","shell.execute_reply":"2023-05-24T10:26:24.349831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:16:11.401175Z","iopub.execute_input":"2023-05-13T16:16:11.401879Z","iopub.status.idle":"2023-05-13T16:16:11.430020Z","shell.execute_reply.started":"2023-05-13T16:16:11.401838Z","shell.execute_reply":"2023-05-13T16:16:11.428960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(directory):\n    total_duration = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            filepath = os.path.join(directory, filename)\n            with wave.open(filepath, 'r') as wav_file:\n                frames = wav_file.getnframes()\n                rate = wav_file.getframerate()\n                duration = frames / float(rate)\n                total_duration += duration\n    return total_duration\n\ndirectory = '/kaggle/input/upd-speech/mono_voice'\ntotal_duration = get_wav_duration(directory)\nprint('Total duration of WAV files:', total_duration, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:15.415086Z","iopub.execute_input":"2023-07-05T10:09:15.415876Z","iopub.status.idle":"2023-07-05T10:09:18.755936Z","shell.execute_reply.started":"2023-07-05T10:09:15.415836Z","shell.execute_reply":"2023-07-05T10:09:18.754693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return '{:02d}:{:02d}:{:02d}'.format(int(hours), int(minutes), int(seconds))\nseconds = 3661\nformatted_time = format_time(total_duration)\nprint(formatted_time)  # Output: '01:01:01'","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:23.353548Z","iopub.execute_input":"2023-07-05T10:09:23.354296Z","iopub.status.idle":"2023-07-05T10:09:23.361628Z","shell.execute_reply.started":"2023-07-05T10:09:23.354254Z","shell.execute_reply":"2023-07-05T10:09:23.360431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}