{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5321255,"sourceType":"datasetVersion","datasetId":3091651},{"sourceId":5618710,"sourceType":"datasetVersion","datasetId":3230790},{"sourceId":5677279,"sourceType":"datasetVersion","datasetId":2989949},{"sourceId":5677449,"sourceType":"datasetVersion","datasetId":3071831},{"sourceId":5760288,"sourceType":"datasetVersion","datasetId":3311237},{"sourceId":8003942,"sourceType":"datasetVersion","datasetId":4230886},{"sourceId":8065936,"sourceType":"datasetVersion","datasetId":3213578}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np \nimport matplotlib\nfrom transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\"\"\"TO-DO:\n- накрутить в предикт исправление предсказаний корректором ошибок, из train\\test убрать корректор\n- сделать разделение на train/valid/test\n- прогнать модель с корректором на тестовом наборе\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:10:07.145630Z","iopub.execute_input":"2024-04-22T16:10:07.146048Z","iopub.status.idle":"2024-04-22T16:10:12.666590Z","shell.execute_reply.started":"2024-04-22T16:10:07.146008Z","shell.execute_reply":"2024-04-22T16:10:12.665058Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'TO-DO:\\n- накрутить в предикт исправление предсказаний корректором ошибок, из train\\test убрать корректор\\n- сделать разделение на train/valid/test\\n- прогнать модель с корректором на тестовом наборе'"},"metadata":{}}]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        int_sequence = []\n        for c in text:\n            ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MFCC(n_mfcc=20)\n)\n\n\nvalid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//3)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=34, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:10:15.769314Z","iopub.execute_input":"2024-04-22T16:10:15.770040Z","iopub.status.idle":"2024-04-22T16:10:15.911988Z","shell.execute_reply.started":"2024-04-22T16:10:15.769989Z","shell.execute_reply":"2024-04-22T16:10:15.910872Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"class BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:10:16.689971Z","iopub.execute_input":"2024-04-22T16:10:16.690939Z","iopub.status.idle":"2024-04-22T16:10:16.698467Z","shell.execute_reply.started":"2024-04-22T16:10:16.690884Z","shell.execute_reply":"2024-04-22T16:10:16.697241Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Поменял там, где происходит загрузка, сохраняется id звукового файла, а потом в excel файле по колонке old_id ищется текст\n#И того звук и текст к нему\n\nimport pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches v1.xlsx')\n#y = [sentence for sentence in file['text']]\ny = []\ndir_name = \"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in os.listdir(\"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"):\n    file_name = e\n    for old_id in range(0, 2073):\n        if file_name.startswith(str(file['old_id'][old_id]) + '.'):\n            y.extend([''.join(file['text'][old_id])])\n            sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n            sampl = sampl[np.newaxis, :]\n            X.append(torch.Tensor(sampl))\n            break","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:10:18.390241Z","iopub.execute_input":"2024-04-22T16:10:18.390676Z","iopub.status.idle":"2024-04-22T16:12:07.506456Z","shell.execute_reply.started":"2024-04-22T16:10:18.390634Z","shell.execute_reply":"2024-04-22T16:12:07.504979Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import random\npairs = list(zip(X, y))\nrandom.shuffle(pairs)\nX, y = zip(*pairs)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:12:11.408985Z","iopub.execute_input":"2024-04-22T16:12:11.409975Z","iopub.status.idle":"2024-04-22T16:12:11.421094Z","shell.execute_reply.started":"2024-04-22T16:12:11.409922Z","shell.execute_reply":"2024-04-22T16:12:11.419187Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"y[:3]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:12:12.598260Z","iopub.execute_input":"2024-04-22T16:12:12.598689Z","iopub.status.idle":"2024-04-22T16:12:12.607644Z","shell.execute_reply.started":"2024-04-22T16:12:12.598651Z","shell.execute_reply":"2024-04-22T16:12:12.606252Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('Ты свободная в эти выходные?',\n 'Моя посылка уже доставлена или ещё нет?',\n 'Бабушка мне межзвездное пространство')"},"metadata":{}}]},{"cell_type":"code","source":"X[:3]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:12:12.820921Z","iopub.execute_input":"2024-04-22T16:12:12.821414Z","iopub.status.idle":"2024-04-22T16:12:12.848803Z","shell.execute_reply.started":"2024-04-22T16:12:12.821365Z","shell.execute_reply":"2024-04-22T16:12:12.847517Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0657, 0.0521, 0.0373]]),\n tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0033, -0.0087,  0.0000]]),\n tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0011, 0.0000]]))"},"metadata":{}}]},{"cell_type":"code","source":"torchaudio.save('/kaggle/working/audio.wav', X[540], 16000)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:52:46.826831Z","iopub.execute_input":"2024-04-02T14:52:46.827791Z","iopub.status.idle":"2024-04-02T14:52:46.834792Z","shell.execute_reply.started":"2024-04-02T14:52:46.827746Z","shell.execute_reply":"2024-04-02T14:52:46.833811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"waveform, sample_rate = torchaudio.load('/kaggle/working/audio.wav')  # Загрузка аудиофайла\ntorchaudio.play(waveform, sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:22:15.368540Z","iopub.execute_input":"2024-04-02T07:22:15.368969Z","iopub.status.idle":"2024-04-02T07:22:15.395250Z","shell.execute_reply.started":"2024-04-02T07:22:15.368930Z","shell.execute_reply":"2024-04-02T07:22:15.393666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('4', 'четыре').replace('Р-220', 'р двести двадцать').replace('6', 'шесть').replace(\"-\", \" \")\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    sentence = \" \".join(sentence.split())\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:12:17.637588Z","iopub.execute_input":"2024-04-22T16:12:17.638100Z","iopub.status.idle":"2024-04-22T16:12:17.674215Z","shell.execute_reply.started":"2024-04-22T16:12:17.638048Z","shell.execute_reply":"2024-04-22T16:12:17.672926Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\"\"\"#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nX_train = X[:1800]\nX_test = X[1800:]\ny_train = y[:1800]\ny_test = y[1800:]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:12:19.996011Z","iopub.execute_input":"2024-04-22T16:12:19.996895Z","iopub.status.idle":"2024-04-22T16:12:20.133738Z","shell.execute_reply.started":"2024-04-22T16:12:19.996840Z","shell.execute_reply":"2024-04-22T16:12:20.132267Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\\nX_train = X[:1800]\\nX_test = X[1800:]\\ny_train = y[:1800]\\ny_test = y[1800:]'"},"metadata":{}}]},{"cell_type":"code","source":"X_train = X[:2430]\nX_test = X[2430:]\ny_train = y[:2430]\ny_test = y[2430:]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:12:20.526879Z","iopub.execute_input":"2024-04-22T16:12:20.527743Z","iopub.status.idle":"2024-04-22T16:12:20.534449Z","shell.execute_reply.started":"2024-04-22T16:12:20.527698Z","shell.execute_reply":"2024-04-22T16:12:20.532844Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:12:25.237249Z","iopub.execute_input":"2024-04-22T16:12:25.237664Z","iopub.status.idle":"2024-04-22T16:12:25.246914Z","shell.execute_reply.started":"2024-04-22T16:12:25.237627Z","shell.execute_reply":"2024-04-22T16:12:25.245519Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 32, kernel_size=(4,4), stride=(3,3), padding=(2,2)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.Conv2d(128, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n        )\n        \n        self.fc_1 = nn.Sequential(\n            nn.Linear(896, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n        )\n        \n        self.BiGRU_1 = BidirectionalGRU(270, 270, 0, True)\n        self.BiGRU_2 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_3 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_4 = BidirectionalGRU(540, 270, 0.5, True)\n        \n        self.fc_2 = nn.Sequential(\n            nn.Linear(540, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.fc_1(x)\n        x = self.BiGRU_1(x)\n        x = self.BiGRU_2(x)\n        x = self.BiGRU_3(x)\n        x = self.BiGRU_4(x)\n        x = self.fc_2(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:12:31.228625Z","iopub.execute_input":"2024-04-22T16:12:31.229092Z","iopub.status.idle":"2024-04-22T16:12:31.245099Z","shell.execute_reply.started":"2024-04-22T16:12:31.229050Z","shell.execute_reply":"2024-04-22T16:12:31.243592Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"TOKENIZERS_PARALLELISM=True","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:14:12.214440Z","iopub.execute_input":"2024-04-04T16:14:12.215402Z","iopub.status.idle":"2024-04-04T16:14:12.220648Z","shell.execute_reply.started":"2024-04-04T16:14:12.215355Z","shell.execute_reply":"2024-04-04T16:14:12.219629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Зададим название выбронной модели из хаба\nMODEL_NAME = 'UrukHan/t5-russian-spell'\nMAX_INPUT = 256\n\n# Загрузка модели и токенизатора\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n\n# Входные данные (можно массив фраз или текст)\ninput_sequences = 'сеглдыя хорош ден'   # или можно использовать одиночные фразы:  input_sequences = 'сеглдыя хорош ден'","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:13:34.768051Z","iopub.execute_input":"2024-04-04T16:13:34.769406Z","iopub.status.idle":"2024-04-04T16:13:50.721738Z","shell.execute_reply.started":"2024-04-04T16:13:34.769360Z","shell.execute_reply":"2024-04-04T16:13:50.720460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"task_prefix = \"Spell correct: \"                 # Токенизирование данных\nif type(input_sequences) != list: input_sequences = [input_sequences]\nencoded = tokenizer(\n  [task_prefix + sequence for sequence in input_sequences],\n  padding=\"longest\",\n  max_length=MAX_INPUT,\n  truncation=True,\n  return_tensors=\"pt\",\n)   # # Прогнозирование","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:13:50.724069Z","iopub.execute_input":"2024-04-04T16:13:50.724428Z","iopub.status.idle":"2024-04-04T16:13:50.736359Z","shell.execute_reply.started":"2024-04-04T16:13:50.724380Z","shell.execute_reply":"2024-04-04T16:13:50.735263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts = model.generate(**encoded.to(device))","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:13:52.768924Z","iopub.execute_input":"2024-04-04T16:13:52.770010Z","iopub.status.idle":"2024-04-04T16:13:56.523644Z","shell.execute_reply.started":"2024-04-04T16:13:52.769955Z","shell.execute_reply":"2024-04-04T16:13:56.522572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]  # Декодируем данные","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:14:16.919739Z","iopub.execute_input":"2024-04-04T16:14:16.920586Z","iopub.status.idle":"2024-04-04T16:14:19.686824Z","shell.execute_reply.started":"2024-04-04T16:14:16.920544Z","shell.execute_reply":"2024-04-04T16:14:19.685743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOKENIZERS_PARALLELISM=True","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:30:30.812195Z","iopub.execute_input":"2024-04-22T14:30:30.813150Z","iopub.status.idle":"2024-04-22T14:30:30.817675Z","shell.execute_reply.started":"2024-04-22T14:30:30.813103Z","shell.execute_reply":"2024-04-22T14:30:30.816503Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Зададим название выбронной модели из хаба\nMODEL_NAME = 'UrukHan/t5-russian-spell'\nMAX_INPUT = 256\n\n# Загрузка модели и токенизатора\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\ncorrector = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:12:41.887371Z","iopub.execute_input":"2024-04-22T16:12:41.887817Z","iopub.status.idle":"2024-04-22T16:13:05.362042Z","shell.execute_reply.started":"2024-04-22T16:12:41.887773Z","shell.execute_reply":"2024-04-22T16:13:05.360672Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e47984a6364659a5fc696ad8d717da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/1.00M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a21b0cd1fc984a7a9cf93ddcaf6684ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/2.63M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff1020757c74874a1d7b10b112f4624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ad92a15e2b64d5aa9c6911ad0432a67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a987a25353744f68de10cbc6ead83b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef4502d79bff4128aa37f14042fd8cbe"}},"metadata":{}}]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        for j in range(len(decoded_preds)):\n            # убрать корректор отсюда и докинуть его в predict\n    \n            input_sequences = decoded_preds[j]\n            \n            task_prefix = \"Spell correct: \"\n            \n            #TOKENIZERS_PARALLELISM=True\n            \n            if type(input_sequences) != list: input_sequences = [input_sequences]\n            encoded = tokenizer(\n              [task_prefix + sequence for sequence in input_sequences],\n              padding=\"longest\",\n              max_length=MAX_INPUT,\n              truncation=True,\n              return_tensors=\"pt\",\n            )\n            \n            predicts = corrector.generate(**encoded.to(device))\n            # # Прогнозирование\n            \n            input_sequences = tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]\n            \n            train_cer.append(cer(decoded_targets[j], input_sequences))\n            train_wer.append(wer(decoded_targets[j], input_sequences))\n    \n    avg_cer = sum(train_cer)/len(train_cer)\n    avg_wer = sum(train_wer)/len(train_wer)\n            \n    print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(train_loss, avg_cer, avg_wer))\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n            \n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n            \n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                # убрать корректор отсюда и докинуть его в predict\n                input_sequences = decoded_preds[j]\n                \n                task_prefix = \"Spell correct: \"\n                \n                if type(input_sequences) != list: input_sequences = [input_sequences]\n                encoded = tokenizer(\n                  [task_prefix + sequence for sequence in input_sequences],\n                  padding=\"longest\",\n                  max_length=MAX_INPUT,\n                  truncation=True,\n                  return_tensors=\"pt\",\n                )\n\n                predicts = corrector.generate(**encoded.to(device))   # # Прогнозирование\n                \n                input_sequences = tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]\n                \n                test_cer.append(cer(decoded_targets[j], input_sequences))\n                test_wer.append(wer(decoded_targets[j], input_sequences))\n    \n   \n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model.pt')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:38:08.116253Z","iopub.execute_input":"2024-04-22T15:38:08.116695Z","iopub.status.idle":"2024-04-22T15:38:08.155610Z","shell.execute_reply.started":"2024-04-22T15:38:08.116648Z","shell.execute_reply":"2024-04-22T15:38:08.154431Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        for j in range(len(decoded_preds)):\n            train_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n            train_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n    avg_cer = sum(train_cer)/len(train_cer)\n    avg_wer = sum(train_wer)/len(train_wer)\n            \n    print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(train_loss, avg_cer, avg_wer))\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n            \n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n            \n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n   \n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model.pt')","metadata":{"execution":{"iopub.status.busy":"2024-04-04T15:08:19.102512Z","iopub.execute_input":"2024-04-04T15:08:19.103352Z","iopub.status.idle":"2024-04-04T15:08:19.133682Z","shell.execute_reply.started":"2024-04-04T15:08:19.103283Z","shell.execute_reply":"2024-04-04T15:08:19.132398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict(model, file_name, device):\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    sampl = librosa.load(file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    print(spectrogram_tensor.size())\n\n    with torch.no_grad():\n        spectrogram_tensor.to(device)\n        output = model(spectrogram_tensor)\n        print(output.size())\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-06T04:02:16.033559Z","iopub.execute_input":"2024-04-06T04:02:16.034491Z","iopub.status.idle":"2024-04-06T04:02:16.044865Z","shell.execute_reply.started":"2024-04-06T04:02:16.034450Z","shell.execute_reply":"2024-04-06T04:02:16.043716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict_with_tensor(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    #sampl = librosa.load(file_name, sr=16000)[0]\n    #sampl = sampl[np.newaxis, :]\n    #sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    print(spectrogram_tensor.size())\n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    print(decodes[0])        \n    input_sequences = decodes[0]\n                \n    task_prefix = \"Spell correct: \"\n\n    if type(input_sequences) != list: input_sequences = [input_sequences]\n    encoded = tokenizer(\n      [task_prefix + sequence for sequence in input_sequences],\n      padding=\"longest\",\n      max_length=MAX_INPUT,\n      truncation=True,\n      return_tensors=\"pt\",\n    )\n\n    predicts = corrector.generate(**encoded.to(needed_device))   # # Прогнозирование\n\n    input_sequences = tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]\n    print(input_sequences)\n\n    return input_sequences","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:20:03.587697Z","iopub.execute_input":"2024-04-22T16:20:03.588197Z","iopub.status.idle":"2024-04-22T16:20:03.602004Z","shell.execute_reply.started":"2024-04-22T16:20:03.588136Z","shell.execute_reply":"2024-04-22T16:20:03.600849Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"%%time \nlearning_rate = 0.002\nbatch_size = 20\nepochs = 100\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-06T04:02:18.402649Z","iopub.execute_input":"2024-04-06T04:02:18.403494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use_cuda = torch.cuda.is_available()\n#device = torch.device(\"cpu\")\nneeded_device = torch.device(\"cpu\")\nmodel = torch.load('/kaggle/input/dop-test-files/model.pt', map_location=torch.device('cpu'))\n\n#1543 1882 1372\n\nmodel.to(needed_device)\nprint(needed_device)\n#predict(model, '/kaggle/input/upd-speech/mono_voice/1964.wav', device)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:14:24.293843Z","iopub.execute_input":"2024-04-22T16:14:24.294537Z","iopub.status.idle":"2024-04-22T16:14:24.700997Z","shell.execute_reply.started":"2024-04-22T16:14:24.294477Z","shell.execute_reply":"2024-04-22T16:14:24.699634Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"d = {'X_test': X_test, 'label': y_test}\ndf_test = pd.DataFrame(data=d)\ndf_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:14:28.324639Z","iopub.execute_input":"2024-04-22T16:14:28.325064Z","iopub.status.idle":"2024-04-22T16:14:33.840390Z","shell.execute_reply.started":"2024-04-22T16:14:28.325025Z","shell.execute_reply":"2024-04-22T16:14:33.839046Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                              X_test  \\\n0  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n1  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n2  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n3  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n4  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n\n                                         label  \n0                   двоюродный брат мне юпитер  \n1                              вода набирается  \n2  без записи можно посетить этого специалиста  \n3                                    дай денег  \n4                     может быть следующий раз  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X_test</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>двоюродный брат мне юпитер</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>вода набирается</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>без записи можно посетить этого специалиста</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>дай денег</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>может быть следующий раз</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def count_test_cer(row, model):\n    prediction = predict_with_tensor(model, row['X_test'])\n    return cer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:15:44.065990Z","iopub.execute_input":"2024-04-22T16:15:44.066467Z","iopub.status.idle":"2024-04-22T16:15:44.073674Z","shell.execute_reply.started":"2024-04-22T16:15:44.066424Z","shell.execute_reply":"2024-04-22T16:15:44.071944Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_test['CER'] = df_test.apply(count_test_cer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:20:15.423046Z","iopub.execute_input":"2024-04-22T16:20:15.423576Z","iopub.status.idle":"2024-04-22T16:21:43.939476Z","shell.execute_reply.started":"2024-04-22T16:20:15.423523Z","shell.execute_reply":"2024-04-22T16:21:43.937389Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 1, 20, 488])\nдвоюродный брат мне юпитер\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py:1292: UserWarning: Using `max_length`'s default (256) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  UserWarning,\n","output_type":"stream"},{"name":"stdout","text":"Двоюродный брат мне Питер.\ntorch.Size([1, 1, 20, 201])\nвода набирается\nВода набирается.\ntorch.Size([1, 1, 20, 631])\nбез записи можно посетить этого специалиста\nБез записи можно посетить этого специалиста\ntorch.Size([1, 1, 20, 136])\nдай денег\nДай денег.\ntorch.Size([1, 1, 20, 423])\nможет быть следующий раз\nМожет быть, следующий раз.\ntorch.Size([1, 1, 20, 553])\nзнакомо чувство когда мечта не сбывается\nЗнакомо чувство, когда мечта не сбывается.\ntorch.Size([1, 1, 20, 376])\nразвелись после лет\nРазвелись после лет.\ntorch.Size([1, 1, 20, 349])\nзверёк\nЗверёк.\ntorch.Size([1, 1, 20, 243])\nверновщесь вдом\nВероятно, в дом.\ntorch.Size([1, 1, 20, 758])\nгустой туман скрывает все вокруг\nГустлый туман скрывает все вокруг.\ntorch.Size([1, 1, 20, 450])\nвыхожу на улицу а там лежит снег\nВыхожу на улицу, а там лежит снег.\ntorch.Size([1, 1, 20, 236])\nкака\nКака.\ntorch.Size([1, 1, 20, 576])\nполка была заставлена игрушками разного сорта\nПолка была заставлена игрушками разного сорта\ntorch.Size([1, 1, 20, 571])\nваши часы отстают\nВаши часы отстают.\ntorch.Size([1, 1, 20, 565])\nнаверное стоит заказать еще одну пиццу\nНаверное, стоит заказать еще одну пиццу.\ntorch.Size([1, 1, 20, 790])\nвзаимная любовь делает жизнь ярче\nВзаимная любовь делает жизнь ярче.\ntorch.Size([1, 1, 20, 540])\nжелаю самого наилучшего\nЖелаю самого наилучшего\ntorch.Size([1, 1, 20, 233])\nя его раскусил\nЯ его раскусил.\ntorch.Size([1, 1, 20, 321])\nчто те бетом понравилос\nЧто тем летом понравилось.\ntorch.Size([1, 1, 20, 516])\nникогда не останавливайтесь на достигнутом\nНикогда не останавливайтесь на достигнутом.\ntorch.Size([1, 1, 20, 158])\nконец пути\nКонец пути.\ntorch.Size([1, 1, 20, 376])\nмы друзья\nМы друзья.\ntorch.Size([1, 1, 20, 376])\nтонкие ление на бумаже\nТонкое ление на бумаге.\ntorch.Size([1, 1, 20, 127])\nпривет\nПривет.\ntorch.Size([1, 1, 20, 456])\nкогда нибудь я стану президентом\nКогда-нибудь я стану президентом.\ntorch.Size([1, 1, 20, 280])\nмашенег\n«Мешенег»\ntorch.Size([1, 1, 20, 394])\nничего хорошего красивого\nНичего хорошего, красивого.\ntorch.Size([1, 1, 20, 349])\nмир во всем мире\nМир во всем мире.\ntorch.Size([1, 1, 20, 730])\nпринимай изменения как шанс на новые возможности\nПринимай изменения как шанс на новые возможности.\ntorch.Size([1, 1, 20, 538])\nсияющий драгоценный камень\nСияющий драгоценный камень.\ntorch.Size([1, 1, 20, 421])\nвереткрылось ознестоиял чловек\nВертолет открылось, поднялся человек.\ntorch.Size([1, 1, 20, 149])\nсоциальная сеть\nСоциальная сеть\ntorch.Size([1, 1, 20, 257])\nтемный коридор\nТемный коридор.\ntorch.Size([1, 1, 20, 221])\nсломанный инструмент\nСломанный инструмент.\ntorch.Size([1, 1, 20, 394])\nзло оказалось сильней\nЗло оказалось сильней.\ntorch.Size([1, 1, 20, 132])\nогурец\nОгурец.\ntorch.Size([1, 1, 20, 303])\nможно мне стакан воды\nМожно мне стакан воды.\ntorch.Size([1, 1, 20, 429])\nя не уверена что это возможно\nЯ не уверена, что это возможно.\ntorch.Size([1, 1, 20, 638])\nвенские вафли с клубникой\nВенские вафли с клубникой.\ntorch.Size([1, 1, 20, 459])\nдавай встретимся завтра\nДавай встретимся завтра.\ntorch.Size([1, 1, 20, 758])\nтелефон с откидывающимся дисплеем\nТелефон с откидывающимся дисплеем.\ntorch.Size([1, 1, 20, 674])\nтвоя поддержка была для меня невероятно важна\nТвоя поддержка была для меня невероятно важна.\ntorch.Size([1, 1, 20, 440])\nавторкнегий любил курить трубку\nАвтор книги любил курить трубку.\ntorch.Size([1, 1, 20, 544])\nкрысы тоже не нравятся\nКрысы тоже не нравятся.\ntorch.Size([1, 1, 20, 223])\nне вижу смысла\nНе вижу смысла.\ntorch.Size([1, 1, 20, 547])\nжизнь иногда сложная штука\nЖизнь иногда сложная штука.\ntorch.Size([1, 1, 20, 418])\nмозна торяни дужее\nМозна Торяни и другие\ntorch.Size([1, 1, 20, 363])\nзабитый гол\nЗабитый гол\ntorch.Size([1, 1, 20, 730])\nвыберите работу по душе чтобы каждый день был ярким\nВыберите работу по душе, чтобы каждый день был ярким\ntorch.Size([1, 1, 20, 490])\nзастывшее молоко в морозилке\nЗастывшее молоко в морозилке.\ntorch.Size([1, 1, 20, 423])\nскользкий тип\nСкользкий тип.\ntorch.Size([1, 1, 20, 347])\nувеличена селезёнка\nУвеличена селезёнка.\ntorch.Size([1, 1, 20, 341])\nстоимость проезда\nСтоимость проезда\ntorch.Size([1, 1, 20, 434])\nгромкая музыка\nГромкая музыка.\ntorch.Size([1, 1, 20, 782])\nкошка мяукает поздно ночью\nКошка мяукает поздно ночью.\ntorch.Size([1, 1, 20, 310])\nконьюктивит\nКонютивит.\ntorch.Size([1, 1, 20, 354])\nветердул сельно сиуга\nВетер дул сельно сига.\ntorch.Size([1, 1, 20, 163])\nсекс\nСекс.\ntorch.Size([1, 1, 20, 321])\nкак долго ждать заказ\nКак долго ждать заказ\ntorch.Size([1, 1, 20, 389])\nмузыка это язык души\nМузыка — это язык души.\ntorch.Size([1, 1, 20, 681])\nработать нужно усердняя если ни нужно умниия\nРаботать нужно усердно, если не нужно ума.\ntorch.Size([1, 1, 20, 383])\nвенские вафли с клубникой\nВенские вафли с клубникой.\ntorch.Size([1, 1, 20, 341])\nспортивный костюм для бега\nСпортивный костюм для бега.\ntorch.Size([1, 1, 20, 402])\nэто оченьнилось ваший стараны\nЭто очень касалось вашей страны.\ntorch.Size([1, 1, 20, 153])\nлатовая связь\n«Латовая связь»\ntorch.Size([1, 1, 20, 204])\nтвердый орех\nтвердый орех.\ntorch.Size([1, 1, 20, 381])\nготов попробовать это блюдо\nГотов попробовать это блюдо.\ntorch.Size([1, 1, 20, 356])\nмне нужно добавить медицина\nМне нужно добавить: медицина.\ntorch.Size([1, 1, 20, 106])\nгалка\nГалка.\ntorch.Size([1, 1, 20, 415])\nразноцветный букет цветов\nРазноцветный букет цветов.\ntorch.Size([1, 1, 20, 423])\nтрепещущий луч\nТребующий луч.\ntorch.Size([1, 1, 20, 352])\nи если я проиграл\nИ если я проиграл.\ntorch.Size([1, 1, 20, 287])\nкристальный град\nКристальный град.\ntorch.Size([1, 1, 20, 642])\nмы бы хотели позавтракать\nМы бы хотели позавтракать.\ntorch.Size([1, 1, 20, 204])\nшелковый платок\nШелковый платок.\ntorch.Size([1, 1, 20, 407])\nсвежий ветер\nСвежий ветер.\ntorch.Size([1, 1, 20, 376])\nжаркое лето вызывает жануду\nЖаркое лето вызывает жару.\ntorch.Size([1, 1, 20, 279])\nжаркий пустырь\nЖаркий пустырь.\ntorch.Size([1, 1, 20, 450])\nесть ли новости из школы\nЕсть ли новости из школы?\ntorch.Size([1, 1, 20, 259])\nтелефон\nТелефон\ntorch.Size([1, 1, 20, 206])\nкафе семейное\nКафе семейное\ntorch.Size([1, 1, 20, 206])\nкороль тосковал\nКороль тосковал.\ntorch.Size([1, 1, 20, 574])\nнам нужно еще порупродута в для ужно\nНам нужно еще пару продуктов для нужд.\ntorch.Size([1, 1, 20, 271])\nчитать стихи\nЧитать стихи\ntorch.Size([1, 1, 20, 356])\nбанковская карта\nБанковская карта\ntorch.Size([1, 1, 20, 294])\nвойна продолжается\nВойна продолжается\ntorch.Size([1, 1, 20, 206])\nмного разрушений\nМного разрушений.\ntorch.Size([1, 1, 20, 582])\nфотографии запрещены в этом месте\nФотографии запрещены в этом месте\ntorch.Size([1, 1, 20, 373])\nлес лишь часть полена\nЛес лишь часть полена.\ntorch.Size([1, 1, 20, 119])\nманеж\nМанеж.\ntorch.Size([1, 1, 20, 727])\nкрасное яблоко на зеленой траве выглядит аппетитно\nКрасное яблоко на зеленой траве выглядит аппетитно.\ntorch.Size([1, 1, 20, 539])\nон сказал что я буду выступать перед детьми\nОн сказал, что я буду выступать перед детьми.\ntorch.Size([1, 1, 20, 311])\nвспомнил геометрия\nВспомнил: геометрия.\ntorch.Size([1, 1, 20, 411])\nкристальная смола\nКристальная смола.\ntorch.Size([1, 1, 20, 394])\nгель для умывания\nГель для умывания\ntorch.Size([1, 1, 20, 635])\nчистый воздух в горах бодрит и освежает\nЧистый воздух в горах бодрит и освежает.\ntorch.Size([1, 1, 20, 607])\nзеленый газон идеальное место для пикника\nЗеленый газон идеальное место для пикника.\ntorch.Size([1, 1, 20, 163])\nтрадиция\nТрадиция.\ntorch.Size([1, 1, 20, 282])\nкуда мы идем дальше\nКуда мы идем дальше?\ntorch.Size([1, 1, 20, 381])\nшум дождя на крыше дома\nШум дождя на крыше дома.\ntorch.Size([1, 1, 20, 106])\nмузей\nМузей.\ntorch.Size([1, 1, 20, 688])\nптица летела на восток согреваемая утренним солнцем\nПтица летела на восток, согреваемая утренним солнцем.\ntorch.Size([1, 1, 20, 423])\nдобавьте это в список покупок\nДобавьте это в список покупок\ntorch.Size([1, 1, 20, 316])\nистория повторяется\nИстория повторяется.\ntorch.Size([1, 1, 20, 363])\nпобывал в чёрной дыре\nПобывал в «чёрной дыре»\ntorch.Size([1, 1, 20, 174])\nзапинка\nЗаправка\ntorch.Size([1, 1, 20, 325])\nбелый свет\nБелый свет.\ntorch.Size([1, 1, 20, 273])\nвремя карьеры\nВремя карьеры\ntorch.Size([1, 1, 20, 206])\nискушение\nУдовлетворение.\ntorch.Size([1, 1, 20, 260])\nдоктор\nДоктор.\ntorch.Size([1, 1, 20, 458])\nпроизнес громко\nпроизнес громко.\ntorch.Size([1, 1, 20, 307])\nбегать по утрам полезно\nБегать по утрам полезно.\ntorch.Size([1, 1, 20, 1028])\nкарандаш взлетел и приземлился на стирашку\nКартошка взлетел и приземлился на стирашку.\ntorch.Size([1, 1, 20, 351])\nходить в зелёном замечательно\nХодить в зелёном замечательно.\ntorch.Size([1, 1, 20, 393])\nвывести собаку на прогулку\nВывести собаку на прогулку.\ntorch.Size([1, 1, 20, 955])\nсекс сам по себе для меня не так важен как эмоциональное понимание\nСекс сам по себе для меня не так важен, как эмоциональное понимание.\ntorch.Size([1, 1, 20, 483])\nпроцессоров у нас нет\nПроцессоров у нас нет.\ntorch.Size([1, 1, 20, 137])\nснежинки\nСнежинки.\ntorch.Size([1, 1, 20, 263])\nмузей\nМузей.\ntorch.Size([1, 1, 20, 709])\nсправедливость важная составляющая общества\nСправедливость важная составляющая общества.\ntorch.Size([1, 1, 20, 311])\nкрысы тоже не нравятся\nКрысы тоже не нравятся.\ntorch.Size([1, 1, 20, 373])\nплинянник\nПлинянник\ntorch.Size([1, 1, 20, 607])\nвчера на работе было напряжённо\n«Вчера на работе было напряжённо.\ntorch.Size([1, 1, 20, 717])\nновый голзе блись кими релниме\nНовый гол забили кими рениме.\ntorch.Size([1, 1, 20, 692])\nпустые обещания хуже чем нет обещаний вовсе\nПустые обещания хуже, чем нет обещаний вовсе.\ntorch.Size([1, 1, 20, 343])\nкровать без защитных реек\nКровать без защитных реек.\ntorch.Size([1, 1, 20, 700])\nсходи в аптеку и купи лекарства\nСходи в аптеку и купи лекарства.\ntorch.Size([1, 1, 20, 333])\nбанковская карта\nБанковская карта\ntorch.Size([1, 1, 20, 342])\nя заболел\nЯ заболел.\ntorch.Size([1, 1, 20, 547])\nя никогда не увижу солнце\nЯ никогда не увижу солнце.\ntorch.Size([1, 1, 20, 291])\nматёрый\nМатёрый\ntorch.Size([1, 1, 20, 486])\nразноцветные краски на палитре\nРазноцветные краски на палитре.\ntorch.Size([1, 1, 20, 387])\nконцерт\nКонцерт.\ntorch.Size([1, 1, 20, 190])\nрадио\nРадио\ntorch.Size([1, 1, 20, 416])\nкак можно добраться до\nКак можно добраться до.\ntorch.Size([1, 1, 20, 528])\nбыло удивительно встретиться с тобой\nБыло удивительно встретиться с тобой.\ntorch.Size([1, 1, 20, 407])\nбабочки порхают\nБабочки порхают.\ntorch.Size([1, 1, 20, 493])\nя думаю тайна в глубоком увлажнениик\nЯ думаю, тайна в глубоком увлажнении.\ntorch.Size([1, 1, 20, 271])\nкрасивые ромашки\nКрасивые ромашки.\ntorch.Size([1, 1, 20, 389])\nбудем ждать другого автомобиля\nБудем ждать другого автомобиля.\ntorch.Size([1, 1, 20, 423])\nбукет алых роз\nБукет алых роз\ntorch.Size([1, 1, 20, 738])\nребенок играет с мячом на пляже\nРебенок играет с мячом на пляже.\ntorch.Size([1, 1, 20, 340])\nвыпить стакан воды утром\nВыпить стакан воды утром.\ntorch.Size([1, 1, 20, 509])\nпринеси мне пожалуйста\nПринеси мне, пожалуйста.\ntorch.Size([1, 1, 20, 396])\nзубная паста\nЗубная паста\ntorch.Size([1, 1, 20, 397])\nзелёный сигнал светофора\nЗелёный сигнал светофора.\ntorch.Size([1, 1, 20, 229])\nуспех\nУспех.\ntorch.Size([1, 1, 20, 451])\nсотрудничество\nСотрудничество\ntorch.Size([1, 1, 20, 385])\nхорской прибой на берегу\nХорской прибой на берегу.\ntorch.Size([1, 1, 20, 419])\nкруглая шляпа на голове\nКруглая шляпа на голове.\ntorch.Size([1, 1, 20, 512])\nбелая кухня блестит после уборки\nБелая кухня блестит после уборки.\ntorch.Size([1, 1, 20, 132])\nнайдёшь\nНайдёшь.\ntorch.Size([1, 1, 20, 256])\nчто вы хотите\nЧто вы хотите.\ntorch.Size([1, 1, 20, 368])\nхателось бы виличить рейтинк\nХотелось бы увеличить рейтинг.\ntorch.Size([1, 1, 20, 180])\nдоброй ноче\nДоброй ночи.\ntorch.Size([1, 1, 20, 469])\nодеяло скомковалось в подъодеялнике\nОблегчение скомкалось в подъялнике.\ntorch.Size([1, 1, 20, 986])\nмне пришлось провести целую ночь чтобы закончить свою работу\nМне пришлось провести целую ночь, чтобы закончить свою работу.\ntorch.Size([1, 1, 20, 410])\nучени е свет ане учения тма\nУченый свет, а не учения ТМА.\ntorch.Size([1, 1, 20, 564])\nв какое время твой рейс из аэропорта\nВ какое время твой рейс из аэропорта.\ntorch.Size([1, 1, 20, 254])\nкакое сегозня часло\nКакое сегодня время.\ntorch.Size([1, 1, 20, 137])\nанальгин\nАнальгин\ntorch.Size([1, 1, 20, 430])\nэто для обучения или экзамена\nЭто для обучения или экзамена.\ntorch.Size([1, 1, 20, 805])\nя обязательно пришлю инструкции по электронной почте\nЯ обязательно пришлю инструкции по электронной почте\ntorch.Size([1, 1, 20, 431])\nбыстрый темп\n«Быстрый темп.\ntorch.Size([1, 1, 20, 244])\nкрасный цветок\nКрасный цветок.\ntorch.Size([1, 1, 20, 294])\nпопасть в просак\nПопасть впросак\ntorch.Size([1, 1, 20, 227])\nрадужный мост\nРадужный мост\ntorch.Size([1, 1, 20, 247])\nкольча серевренное\nКольча, Серебреное\ntorch.Size([1, 1, 20, 356])\nполёт на воздушном шаре\nПолёт на воздушном шаре\ntorch.Size([1, 1, 20, 482])\nигровой ноутбук\nИгровой ноутбук\ntorch.Size([1, 1, 20, 1017])\nчай с лимоном и имбирем в холодную погоду\nЧай с лимоном и имбирем в холодную погоду.\ntorch.Size([1, 1, 20, 817])\nувлечения делают жизнь интереснее\nУвлечения делают жизнь интереснее.\ntorch.Size([1, 1, 20, 376])\nуборка дома\nУборка дома\ntorch.Size([1, 1, 20, 402])\nэто была огромная ошибка\nЭто была огромная ошибка.\ntorch.Size([1, 1, 20, 442])\nребенок играет с мячом на пляже\nРебенок играет с мячом на пляже.\ntorch.Size([1, 1, 20, 582])\nгустой туман скрывает город от глаз\nГустлый туман скрывает город от глаз.\ntorch.Size([1, 1, 20, 237])\nизвлечь урок\nИзвлечь урок.\ntorch.Size([1, 1, 20, 512])\nрадужный закат заливает небо красками\nРадужный закат заливает небо красками.\ntorch.Size([1, 1, 20, 762])\nприятно провести время со старыми друзьями\nПриятно провести время со старыми друзьями.\ntorch.Size([1, 1, 20, 735])\nказна с каждым днем пустела\nКазна с каждым днем пустела.\ntorch.Size([1, 1, 20, 383])\nмягкий шелк\nМягкий шелк.\ntorch.Size([1, 1, 20, 247])\nхарактер\nХарактер\ntorch.Size([1, 1, 20, 482])\nкрасный шарч на севек\nКрасный шар на Севек.\ntorch.Size([1, 1, 20, 206])\nигра втёмную\nИгра втёмную.\ntorch.Size([1, 1, 20, 284])\nноски для альпинизма\nНоски для альпинизма.\ntorch.Size([1, 1, 20, 136])\nкнига\nКнига\ntorch.Size([1, 1, 20, 584])\nключ от комнаты\nКлюч от комнаты.\ntorch.Size([1, 1, 20, 880])\nтакое чувство будто жизнь проходит мимо\nТакое чувство, будто жизнь проходит мимо.\ntorch.Size([1, 1, 20, 817])\nлучши быть о тночим чем б плгой компаи\nЛучше быть точным, чем плохой компании.\ntorch.Size([1, 1, 20, 232])\nфлаг аргентины\nФлаг Аргентины\ntorch.Size([1, 1, 20, 695])\nзаботиться о других лучший способ накопления счастья\nЗаботиться о других — лучший способ накопления счастья.\ntorch.Size([1, 1, 20, 337])\nгде взять направление\nГде взять направление.\ntorch.Size([1, 1, 20, 188])\nбелый свет\nБелый свет.\ntorch.Size([1, 1, 20, 371])\nпоиск своего пути в жизни\nПоиск своего пути в жизни.\ntorch.Size([1, 1, 20, 555])\nу меня очень болят ноги после долгой прогулки\nУ меня очень болят ноги после долгой прогулки.\ntorch.Size([1, 1, 20, 326])\nофициант на полставки\nОфициант на полставки.\ntorch.Size([1, 1, 20, 311])\nпочему все так сложно\nПочему все так сложно?\ntorch.Size([1, 1, 20, 946])\nдумайте на чистом листке каждый день своей жизни\nДумайте на чистом листке каждый день своей жизни.\ntorch.Size([1, 1, 20, 693])\nзабудь об этом и живи дальше\nЗабудь об этом и живи дальше.\ntorch.Size([1, 1, 20, 475])\nлётная школа\nЛётная школа.\ntorch.Size([1, 1, 20, 532])\nможешь подарить мне книгу на день рождения\nМожешь подарить мне книгу на день рождения.\ntorch.Size([1, 1, 20, 802])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3591262002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CER'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_test_cer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/489653466.py\u001b[0m in \u001b[0;36mcount_test_cer\u001b[0;34m(row, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_test_cer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_with_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/3191974626.py\u001b[0m in \u001b[0;36mpredict_with_tensor\u001b[0;34m(model, sampl)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mspectrogram_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeded_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrogram_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0marg_maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/3194618677.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBiGRU_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBiGRU_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBiGRU_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/1854614499.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBiGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 956\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"df_test['CER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:19:11.946256Z","iopub.execute_input":"2024-04-22T16:19:11.947099Z","iopub.status.idle":"2024-04-22T16:19:11.957112Z","shell.execute_reply.started":"2024-04-22T16:19:11.947042Z","shell.execute_reply":"2024-04-22T16:19:11.955572Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"0.14745328612741512"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:26:24.339488Z","iopub.execute_input":"2023-05-24T10:26:24.340405Z","iopub.status.idle":"2023-05-24T10:26:24.350954Z","shell.execute_reply.started":"2023-05-24T10:26:24.340364Z","shell.execute_reply":"2023-05-24T10:26:24.349831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:16:11.401175Z","iopub.execute_input":"2023-05-13T16:16:11.401879Z","iopub.status.idle":"2023-05-13T16:16:11.430020Z","shell.execute_reply.started":"2023-05-13T16:16:11.401838Z","shell.execute_reply":"2023-05-13T16:16:11.428960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(directory):\n    total_duration = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            filepath = os.path.join(directory, filename)\n            with wave.open(filepath, 'r') as wav_file:\n                frames = wav_file.getnframes()\n                rate = wav_file.getframerate()\n                duration = frames / float(rate)\n                total_duration += duration\n    return total_duration\n\ndirectory = '/kaggle/input/upd-speech/mono_voice'\ntotal_duration = get_wav_duration(directory)\nprint('Total duration of WAV files:', total_duration, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:15.415086Z","iopub.execute_input":"2023-07-05T10:09:15.415876Z","iopub.status.idle":"2023-07-05T10:09:18.755936Z","shell.execute_reply.started":"2023-07-05T10:09:15.415836Z","shell.execute_reply":"2023-07-05T10:09:18.754693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return '{:02d}:{:02d}:{:02d}'.format(int(hours), int(minutes), int(seconds))\nseconds = 3661\nformatted_time = format_time(total_duration)\nprint(formatted_time)  # Output: '01:01:01'","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:23.353548Z","iopub.execute_input":"2023-07-05T10:09:23.354296Z","iopub.status.idle":"2023-07-05T10:09:23.361628Z","shell.execute_reply.started":"2023-07-05T10:09:23.354254Z","shell.execute_reply":"2023-07-05T10:09:23.360431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}