{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5321255,"sourceType":"datasetVersion","datasetId":3091651},{"sourceId":5618710,"sourceType":"datasetVersion","datasetId":3230790},{"sourceId":5677279,"sourceType":"datasetVersion","datasetId":2989949},{"sourceId":5677449,"sourceType":"datasetVersion","datasetId":3071831},{"sourceId":5760288,"sourceType":"datasetVersion","datasetId":3311237},{"sourceId":8515857,"sourceType":"datasetVersion","datasetId":3213578},{"sourceId":8537485,"sourceType":"datasetVersion","datasetId":5099750},{"sourceId":8537499,"sourceType":"datasetVersion","datasetId":5099761},{"sourceId":8537514,"sourceType":"datasetVersion","datasetId":5099772},{"sourceId":8537530,"sourceType":"datasetVersion","datasetId":5099776},{"sourceId":8560616,"sourceType":"datasetVersion","datasetId":4230886},{"sourceId":8562283,"sourceType":"datasetVersion","datasetId":5118180}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np \nimport matplotlib\nfrom transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:37:12.615395Z","iopub.execute_input":"2024-05-30T17:37:12.616154Z","iopub.status.idle":"2024-05-30T17:37:17.686220Z","shell.execute_reply.started":"2024-05-30T17:37:12.616113Z","shell.execute_reply":"2024-05-30T17:37:17.685078Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        int_sequence = []\n        for c in text:\n            ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MFCC(n_mfcc=20)\n)\n\n\nvalid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//3)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=34, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:37:17.688640Z","iopub.execute_input":"2024-05-30T17:37:17.689401Z","iopub.status.idle":"2024-05-30T17:37:17.870819Z","shell.execute_reply.started":"2024-05-30T17:37:17.689362Z","shell.execute_reply":"2024-05-30T17:37:17.869734Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"class BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:37:17.872501Z","iopub.execute_input":"2024-05-30T17:37:17.873137Z","iopub.status.idle":"2024-05-30T17:37:17.881894Z","shell.execute_reply.started":"2024-05-30T17:37:17.873101Z","shell.execute_reply":"2024-05-30T17:37:17.880948Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Поменял там, где происходит загрузка, сохраняется id звукового файла, а потом в excel файле по колонке old_id ищется текст\n#И того звук и текст к нему\n\nimport pandas as pd\nimport librosa\n\n# file = pd.read_excel('/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches v1.xlsx')\n# #y = [sentence for sentence in file['text']]\n# y = []\n# dir_name = \"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"\n# files_in_dir = os.listdir(dir_name)\n\n# X = []\n# i = 1\n\n# for e in os.listdir(\"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"):\n#     file_name = e\n#     for old_id in range(0, 2073):\n#         if file_name.startswith(str(file['old_id'][old_id]) + '.'):\n#             y.extend([''.join(file['text'][old_id])])\n#             sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n#             sampl = sampl[np.newaxis, :]\n#             X.append(torch.Tensor(sampl))\n#             break\n\nfile = pd.read_excel('/kaggle/input/dataset-with-3-speakers/Speeches v1.xlsx')\n#y = [sentence for sentence in file['text']]\ny = []\ndir_name = \"/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in os.listdir(\"/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/\"):\n    file_name = e\n    for old_id in range(0, 2073):\n        if file_name.startswith(str(file['old_id'][old_id])[:-1]):\n            y.extend([''.join(file['text'][old_id])])\n            sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n            sampl = sampl[np.newaxis, :]\n            X.append(torch.Tensor(sampl))\n            break","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:43:14.083749Z","iopub.execute_input":"2024-05-30T17:43:14.084667Z","iopub.status.idle":"2024-05-30T17:45:18.127472Z","shell.execute_reply.started":"2024-05-30T17:43:14.084610Z","shell.execute_reply":"2024-05-30T17:45:18.126551Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import random\npairs = list(zip(X, y))\nrandom.Random(20024).shuffle(pairs)\nX, y = zip(*pairs)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:45:33.123816Z","iopub.execute_input":"2024-05-30T17:45:33.124265Z","iopub.status.idle":"2024-05-30T17:45:33.137591Z","shell.execute_reply.started":"2024-05-30T17:45:33.124225Z","shell.execute_reply":"2024-05-30T17:45:33.136545Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"y[:3]","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:45:35.808959Z","iopub.execute_input":"2024-05-30T17:45:35.809408Z","iopub.status.idle":"2024-05-30T17:45:35.817327Z","shell.execute_reply.started":"2024-05-30T17:45:35.809369Z","shell.execute_reply":"2024-05-30T17:45:35.816248Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('Где то есть куча копий',\n 'Твоя поддержка была для меня невероятно важна',\n 'Купить лекарства от насморка')"},"metadata":{}}]},{"cell_type":"code","source":"X[:3]","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:45:39.242465Z","iopub.execute_input":"2024-05-30T17:45:39.243142Z","iopub.status.idle":"2024-05-30T17:45:39.271382Z","shell.execute_reply.started":"2024-05-30T17:45:39.243105Z","shell.execute_reply":"2024-05-30T17:45:39.270421Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -4.3147e-05,\n          -4.8934e-05, -5.3347e-05]]),\n tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0008,  0.0014,  0.0000]]),\n tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0025, 0.0000]]))"},"metadata":{}}]},{"cell_type":"code","source":"torchaudio.save('/kaggle/working/audio.wav', X[540], 16000)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:52:46.826831Z","iopub.execute_input":"2024-04-02T14:52:46.827791Z","iopub.status.idle":"2024-04-02T14:52:46.834792Z","shell.execute_reply.started":"2024-04-02T14:52:46.827746Z","shell.execute_reply":"2024-04-02T14:52:46.833811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"waveform, sample_rate = torchaudio.load('/kaggle/working/audio.wav')  # Загрузка аудиофайла\ntorchaudio.play(waveform, sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:22:15.368540Z","iopub.execute_input":"2024-04-02T07:22:15.368969Z","iopub.status.idle":"2024-04-02T07:22:15.395250Z","shell.execute_reply.started":"2024-04-02T07:22:15.368930Z","shell.execute_reply":"2024-04-02T07:22:15.393666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('4', 'четыре').replace('Р-220', 'р двести двадцать').replace('6', 'шесть').replace(\"-\", \" \")\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    sentence = \" \".join(sentence.split())\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:45:46.207410Z","iopub.execute_input":"2024-05-30T17:45:46.208391Z","iopub.status.idle":"2024-05-30T17:45:46.244225Z","shell.execute_reply.started":"2024-05-30T17:45:46.208350Z","shell.execute_reply":"2024-05-30T17:45:46.243078Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"X_train = X[:2800]\nX_test = X[2800:]\ny_train = y[:2800]\ny_test = y[2800:]","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:46:45.658803Z","iopub.execute_input":"2024-05-30T17:46:45.659225Z","iopub.status.idle":"2024-05-30T17:46:45.664806Z","shell.execute_reply.started":"2024-05-30T17:46:45.659185Z","shell.execute_reply":"2024-05-30T17:46:45.663726Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:46:48.346884Z","iopub.execute_input":"2024-05-30T17:46:48.347284Z","iopub.status.idle":"2024-05-30T17:46:48.354286Z","shell.execute_reply.started":"2024-05-30T17:46:48.347235Z","shell.execute_reply":"2024-05-30T17:46:48.353165Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 32, kernel_size=(4,4), stride=(3,3), padding=(2,2)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.Conv2d(128, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n        )\n        \n        self.fc_1 = nn.Sequential(\n            nn.Linear(896, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n        )\n        \n        self.BiGRU_1 = BidirectionalGRU(270, 270, 0, True)\n        self.BiGRU_2 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_3 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_4 = BidirectionalGRU(540, 270, 0.5, True)\n        \n        self.fc_2 = nn.Sequential(\n            nn.Linear(540, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.fc_1(x)\n        x = self.BiGRU_1(x)\n        x = self.BiGRU_2(x)\n        x = self.BiGRU_3(x)\n        x = self.BiGRU_4(x)\n        x = self.fc_2(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:46:48.356423Z","iopub.execute_input":"2024-05-30T17:46:48.356780Z","iopub.status.idle":"2024-05-30T17:46:48.374187Z","shell.execute_reply.started":"2024-05-30T17:46:48.356742Z","shell.execute_reply":"2024-05-30T17:46:48.373012Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Зададим название выбронной модели из хаба\nMODEL_NAME = 'UrukHan/t5-russian-spell'\nMAX_INPUT = 256\n\n# Загрузка модели и токенизатора\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\ncorrector = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T13:52:10.590361Z","iopub.execute_input":"2024-05-25T13:52:10.590760Z","iopub.status.idle":"2024-05-25T13:52:36.615013Z","shell.execute_reply.started":"2024-05-25T13:52:10.590725Z","shell.execute_reply":"2024-05-25T13:52:36.613682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        \"\"\"decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        for j in range(len(decoded_preds)):\n            train_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n            train_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n    avg_cer = sum(train_cer)/len(train_cer)\n    avg_wer = sum(train_wer)/len(train_wer)\n            \n    print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(train_loss, avg_cer, avg_wer))\"\"\"\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n            \n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n            \n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n   \n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model_for_correction_test.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:46:50.837794Z","iopub.execute_input":"2024-05-30T17:46:50.838914Z","iopub.status.idle":"2024-05-30T17:46:50.867630Z","shell.execute_reply.started":"2024-05-30T17:46:50.838868Z","shell.execute_reply":"2024-05-30T17:46:50.866532Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict(model, file_name, device):\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    sampl = librosa.load(file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    print(spectrogram_tensor.size())\n\n    with torch.no_grad():\n        spectrogram_tensor.to(device)\n        output = model(spectrogram_tensor)\n        print(output.size())\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:46:53.130229Z","iopub.execute_input":"2024-05-30T17:46:53.131016Z","iopub.status.idle":"2024-05-30T17:46:53.141282Z","shell.execute_reply.started":"2024-05-30T17:46:53.130970Z","shell.execute_reply":"2024-05-30T17:46:53.140305Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict_with_tensor(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    #sampl = librosa.load(file_name, sr=16000)[0]\n    #sampl = sampl[np.newaxis, :]\n    #sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    input_sequences = decodes[0]\n                \n    task_prefix = \"Spell correct: \"\n\n    if type(input_sequences) != list: input_sequences = [input_sequences]\n    encoded = tokenizer(\n      [task_prefix + sequence for sequence in input_sequences],\n      padding=\"longest\",\n      max_length=MAX_INPUT,\n      truncation=True,\n      return_tensors=\"pt\",\n    )\n\n    predicts = corrector.generate(**encoded.to(needed_device))   # # Прогнозирование\n\n    input_sequences = tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]\n    input_sequences = remove_characters(input_sequences)\n\n    return input_sequences\n\n    #return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-28T11:58:04.311976Z","iopub.execute_input":"2024-05-28T11:58:04.312769Z","iopub.status.idle":"2024-05-28T11:58:04.328174Z","shell.execute_reply.started":"2024-05-28T11:58:04.312711Z","shell.execute_reply":"2024-05-28T11:58:04.326851Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import hunspell\nimport os\n\n#пробуем hunspell для исправления ошибок\ndef load_hunspell_russian_dict():\n    dict_path = \"/kaggle/input/dop-test-files\"  \n    \n    ru_dic = os.path.join(dict_path, \"ru_RU_big.dic\")\n    ru_aff = os.path.join(dict_path, \"ru_RU_big.aff\")\n    \n    # Create Hunspell instance for Russian\n    hunspell_instance = hunspell.HunSpell(ru_dic, ru_aff)\n    return hunspell_instance\n\ndef correct_mistakes(text, hunspell_instance):\n    corrected_text = []\n    words = text.split()\n    \n    for word in words:\n        if hunspell_instance.spell(word):\n            corrected_text.append(word)\n        else:\n            suggestions = hunspell_instance.suggest(word)\n            if suggestions:\n                corrected_text.append(suggestions[0])  # Choose the first suggestion\n            else:\n                corrected_text.append(word)  # No suggestion, keep the original word\n    \n    return \" \".join(corrected_text)\n\n# Example usage\nhunspell_instance = load_hunspell_russian_dict()\n#text = \"Привет, как дила?\"\n#corrected_text = correct_mistakes(text, hunspell_instance)\n#print(corrected_text)\n\ndef predict_with_tensor_v2(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    corrected_output = correct_mistakes(decodes[0], hunspell_instance)\n\n    #return decodes[0]\n    return corrected_output","metadata":{"execution":{"iopub.status.busy":"2024-05-28T11:58:04.330590Z","iopub.execute_input":"2024-05-28T11:58:04.331096Z","iopub.status.idle":"2024-05-28T11:58:04.516304Z","shell.execute_reply.started":"2024-05-28T11:58:04.331042Z","shell.execute_reply":"2024-05-28T11:58:04.514989Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"!pip install safetensors","metadata":{"execution":{"iopub.status.busy":"2024-05-28T11:58:04.517908Z","iopub.execute_input":"2024-05-28T11:58:04.518412Z","iopub.status.idle":"2024-05-28T11:58:20.419403Z","shell.execute_reply.started":"2024-05-28T11:58:04.518360Z","shell.execute_reply":"2024-05-28T11:58:20.417894Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Collecting safetensors\n  Downloading safetensors-0.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: safetensors\nSuccessfully installed safetensors-0.4.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-28T11:58:20.420913Z","iopub.execute_input":"2024-05-28T11:58:20.421317Z","iopub.status.idle":"2024-05-28T11:58:48.000457Z","shell.execute_reply.started":"2024-05-28T11:58:20.421279Z","shell.execute_reply":"2024-05-28T11:58:47.999163Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.4)\nCollecting transformers\n  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nCollecting huggingface-hub<1.0,>=0.14.1\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nInstalling collected packages: huggingface-hub, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.13.3\n    Uninstalling huggingface-hub-0.13.3:\n      Successfully uninstalled huggingface-hub-0.13.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.27.4\n    Uninstalling transformers-4.27.4:\n      Successfully uninstalled transformers-4.27.4\nSuccessfully installed huggingface-hub-0.16.4 transformers-4.30.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-28T12:02:14.404069Z","iopub.execute_input":"2024-05-28T12:02:14.404576Z","iopub.status.idle":"2024-05-28T12:02:28.936170Z","shell.execute_reply.started":"2024-05-28T12:02:14.404535Z","shell.execute_reply":"2024-05-28T12:02:28.934415Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.30.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntoken = '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'\n\n# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# # Load the model and tokenizer\n# model = AutoModelForSequenceClassification.from_pretrained(\"<your-username>/<your-model-name>\")\n# tokenizer = AutoTokenizer.from_pretrained(\"<your-username>/<your-model-name>\")\n\n# Use the model for inference\n#os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n\nmodel = T5ForConditionalGeneration.from_pretrained('NickChudo/t5_small_fine_tuned_10_epochs_model', \n                                                   use_auth_token=token, \n                                                   from_tf=False)\ntokenizer = T5Tokenizer.from_pretrained('NickChudo/t5_small_fine_tuned_10_epochs_tokenizer', \n                                                   use_auth_token=token,\n                                                   from_tf=False)\n\ndef correct_mistakes(text, model, tokenizer):\n    input_text = \"correct: \" + text\n    inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n    \n    outputs = model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return corrected_text\n\n\n#corrected_text = correct_mistakes(text, model, tokenizer)\n\ndef predict_with_tensor_v3(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    corrected_output = correct_mistakes(decodes[0], model, tokenizer)\n\n    #return decodes[0]\n    return corrected_output","metadata":{"execution":{"iopub.status.busy":"2024-05-28T12:02:33.052229Z","iopub.execute_input":"2024-05-28T12:02:33.052757Z","iopub.status.idle":"2024-05-28T12:02:33.249652Z","shell.execute_reply.started":"2024-05-28T12:02:33.052707Z","shell.execute_reply":"2024-05-28T12:02:33.247587Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mpytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_LAYERNORM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_pruneable_heads_and_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_linear_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 49\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mDUMMY_INPUTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'is_optimum_available' from 'transformers.utils' (/opt/conda/lib/python3.7/site-packages/transformers/utils/__init__.py)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_28/878385006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'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'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# from transformers import AutoModelForSequenceClassification, AutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):\ncannot import name 'is_optimum_available' from 'transformers.utils' (/opt/conda/lib/python3.7/site-packages/transformers/utils/__init__.py)"],"ename":"RuntimeError","evalue":"Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):\ncannot import name 'is_optimum_available' from 'transformers.utils' (/opt/conda/lib/python3.7/site-packages/transformers/utils/__init__.py)","output_type":"error"}]},{"cell_type":"code","source":"%%time\nlearning_rate = 0.001\nbatch_size = 20\nepochs = 50\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-30T17:47:28.020803Z","iopub.execute_input":"2024-05-30T17:47:28.021798Z","iopub.status.idle":"2024-05-30T18:18:32.056995Z","shell.execute_reply.started":"2024-05-30T17:47:28.021756Z","shell.execute_reply":"2024-05-30T18:18:32.055665Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel1(\n  (conv): Sequential(\n    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Conv2d(1, 32, kernel_size=(4, 4), stride=(3, 3), padding=(2, 2))\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): GELU(approximate='none')\n    (4): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): GELU(approximate='none')\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): GELU(approximate='none')\n  )\n  (fc_1): Sequential(\n    (0): Linear(in_features=896, out_features=270, bias=True)\n    (1): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=270, out_features=270, bias=True)\n    (4): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (5): GELU(approximate='none')\n    (6): Linear(in_features=270, out_features=270, bias=True)\n    (7): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (8): GELU(approximate='none')\n  )\n  (BiGRU_1): BidirectionalGRU(\n    (BiGRU): GRU(270, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_2): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_3): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_4): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (fc_2): Sequential(\n    (0): Linear(in_features=540, out_features=35, bias=True)\n  )\n  (softmax): LogSoftmax(dim=2)\n)\nNum Model Parameters 5422923\nTrain Epoch: 1 [0/2800 (0%)]\tLoss: 23.564430\nTrain Epoch: 1 [400/2800 (14%)]\tLoss: 4.763329\nTrain Epoch: 1 [800/2800 (29%)]\tLoss: 3.805254\nTrain Epoch: 1 [1200/2800 (43%)]\tLoss: 3.501035\nTrain Epoch: 1 [1600/2800 (57%)]\tLoss: 3.350905\nTrain Epoch: 1 [2000/2800 (71%)]\tLoss: 3.424886\nTrain Epoch: 1 [2400/2800 (86%)]\tLoss: 3.421442\n\nevaluating...\nTest set:\tAverage loss: 3.3427, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 2 [0/2800 (0%)]\tLoss: 3.320343\nTrain Epoch: 2 [400/2800 (14%)]\tLoss: 3.333100\nTrain Epoch: 2 [800/2800 (29%)]\tLoss: 3.382358\nTrain Epoch: 2 [1200/2800 (43%)]\tLoss: 3.442983\nTrain Epoch: 2 [1600/2800 (57%)]\tLoss: 3.283073\nTrain Epoch: 2 [2000/2800 (71%)]\tLoss: 3.328892\nTrain Epoch: 2 [2400/2800 (86%)]\tLoss: 3.318636\n\nevaluating...\nTest set:\tAverage loss: 3.3249, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 3 [0/2800 (0%)]\tLoss: 3.245684\nTrain Epoch: 3 [400/2800 (14%)]\tLoss: 3.288821\nTrain Epoch: 3 [800/2800 (29%)]\tLoss: 3.232763\nTrain Epoch: 3 [1200/2800 (43%)]\tLoss: 3.311104\nTrain Epoch: 3 [1600/2800 (57%)]\tLoss: 3.285082\nTrain Epoch: 3 [2000/2800 (71%)]\tLoss: 3.255735\nTrain Epoch: 3 [2400/2800 (86%)]\tLoss: 3.162341\n\nevaluating...\nTest set:\tAverage loss: 3.1658, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 4 [0/2800 (0%)]\tLoss: 3.208689\nTrain Epoch: 4 [400/2800 (14%)]\tLoss: 3.004833\nTrain Epoch: 4 [800/2800 (29%)]\tLoss: 2.947220\nTrain Epoch: 4 [1200/2800 (43%)]\tLoss: 2.978397\nTrain Epoch: 4 [1600/2800 (57%)]\tLoss: 2.945376\nTrain Epoch: 4 [2000/2800 (71%)]\tLoss: 2.902392\nTrain Epoch: 4 [2400/2800 (86%)]\tLoss: 2.640207\n\nevaluating...\nTest set:\tAverage loss: 2.6509, Average CER: 0.997510 Average WER: 1.0000\n\nTrain Epoch: 5 [0/2800 (0%)]\tLoss: 2.631624\nTrain Epoch: 5 [400/2800 (14%)]\tLoss: 2.390594\nTrain Epoch: 5 [800/2800 (29%)]\tLoss: 2.349914\nTrain Epoch: 5 [1200/2800 (43%)]\tLoss: 2.302738\nTrain Epoch: 5 [1600/2800 (57%)]\tLoss: 2.773915\nTrain Epoch: 5 [2000/2800 (71%)]\tLoss: 2.035028\nTrain Epoch: 5 [2400/2800 (86%)]\tLoss: 2.078445\n\nevaluating...\nTest set:\tAverage loss: 2.0279, Average CER: 0.656414 Average WER: 1.0000\n\nTrain Epoch: 6 [0/2800 (0%)]\tLoss: 2.081351\nTrain Epoch: 6 [400/2800 (14%)]\tLoss: 2.114649\nTrain Epoch: 6 [800/2800 (29%)]\tLoss: 2.063817\nTrain Epoch: 6 [1200/2800 (43%)]\tLoss: 1.940452\nTrain Epoch: 6 [1600/2800 (57%)]\tLoss: 1.726544\nTrain Epoch: 6 [2000/2800 (71%)]\tLoss: 1.837270\nTrain Epoch: 6 [2400/2800 (86%)]\tLoss: 2.154772\n\nevaluating...\nTest set:\tAverage loss: 1.7130, Average CER: 0.519264 Average WER: 0.9739\n\nTrain Epoch: 7 [0/2800 (0%)]\tLoss: 2.080396\nTrain Epoch: 7 [400/2800 (14%)]\tLoss: 1.495740\nTrain Epoch: 7 [800/2800 (29%)]\tLoss: 1.728143\nTrain Epoch: 7 [1200/2800 (43%)]\tLoss: 1.109609\nTrain Epoch: 7 [1600/2800 (57%)]\tLoss: 1.241726\nTrain Epoch: 7 [2000/2800 (71%)]\tLoss: 1.835782\nTrain Epoch: 7 [2400/2800 (86%)]\tLoss: 1.347360\n\nevaluating...\nTest set:\tAverage loss: 1.5665, Average CER: 0.475662 Average WER: 0.9663\n\nTrain Epoch: 8 [0/2800 (0%)]\tLoss: 1.575499\nTrain Epoch: 8 [400/2800 (14%)]\tLoss: 1.900507\nTrain Epoch: 8 [800/2800 (29%)]\tLoss: 1.530831\nTrain Epoch: 8 [1200/2800 (43%)]\tLoss: 1.354536\nTrain Epoch: 8 [1600/2800 (57%)]\tLoss: 1.125008\nTrain Epoch: 8 [2000/2800 (71%)]\tLoss: 1.164957\nTrain Epoch: 8 [2400/2800 (86%)]\tLoss: 1.444020\n\nevaluating...\nTest set:\tAverage loss: 1.4756, Average CER: 0.448221 Average WER: 0.9260\n\nTrain Epoch: 9 [0/2800 (0%)]\tLoss: 1.522908\nTrain Epoch: 9 [400/2800 (14%)]\tLoss: 1.713925\nTrain Epoch: 9 [800/2800 (29%)]\tLoss: 2.007764\nTrain Epoch: 9 [1200/2800 (43%)]\tLoss: 1.148398\nTrain Epoch: 9 [1600/2800 (57%)]\tLoss: 1.052816\nTrain Epoch: 9 [2000/2800 (71%)]\tLoss: 1.434991\nTrain Epoch: 9 [2400/2800 (86%)]\tLoss: 1.044020\n\nevaluating...\nTest set:\tAverage loss: 1.5013, Average CER: 0.446390 Average WER: 0.9407\n\nTrain Epoch: 10 [0/2800 (0%)]\tLoss: 1.560414\nTrain Epoch: 10 [400/2800 (14%)]\tLoss: 1.207166\nTrain Epoch: 10 [800/2800 (29%)]\tLoss: 1.084964\nTrain Epoch: 10 [1200/2800 (43%)]\tLoss: 1.246724\nTrain Epoch: 10 [1600/2800 (57%)]\tLoss: 1.089695\nTrain Epoch: 10 [2000/2800 (71%)]\tLoss: 1.014303\nTrain Epoch: 10 [2400/2800 (86%)]\tLoss: 1.750570\n\nevaluating...\nTest set:\tAverage loss: 1.4280, Average CER: 0.427530 Average WER: 0.9067\n\nTrain Epoch: 11 [0/2800 (0%)]\tLoss: 1.063371\nTrain Epoch: 11 [400/2800 (14%)]\tLoss: 1.000061\nTrain Epoch: 11 [800/2800 (29%)]\tLoss: 1.090006\nTrain Epoch: 11 [1200/2800 (43%)]\tLoss: 1.238083\nTrain Epoch: 11 [1600/2800 (57%)]\tLoss: 1.160702\nTrain Epoch: 11 [2000/2800 (71%)]\tLoss: 1.321692\nTrain Epoch: 11 [2400/2800 (86%)]\tLoss: 1.333508\n\nevaluating...\nTest set:\tAverage loss: 1.3864, Average CER: 0.419440 Average WER: 0.8913\n\nTrain Epoch: 12 [0/2800 (0%)]\tLoss: 0.736657\nTrain Epoch: 12 [400/2800 (14%)]\tLoss: 0.829999\nTrain Epoch: 12 [800/2800 (29%)]\tLoss: 0.895997\nTrain Epoch: 12 [1200/2800 (43%)]\tLoss: 1.172876\nTrain Epoch: 12 [1600/2800 (57%)]\tLoss: 0.761551\nTrain Epoch: 12 [2000/2800 (71%)]\tLoss: 1.080882\nTrain Epoch: 12 [2400/2800 (86%)]\tLoss: 1.378611\n\nevaluating...\nTest set:\tAverage loss: 1.4050, Average CER: 0.414696 Average WER: 0.8718\n\nTrain Epoch: 13 [0/2800 (0%)]\tLoss: 1.197737\nTrain Epoch: 13 [400/2800 (14%)]\tLoss: 0.968624\nTrain Epoch: 13 [800/2800 (29%)]\tLoss: 0.874536\nTrain Epoch: 13 [1200/2800 (43%)]\tLoss: 0.715538\nTrain Epoch: 13 [1600/2800 (57%)]\tLoss: 0.998108\nTrain Epoch: 13 [2000/2800 (71%)]\tLoss: 1.555794\nTrain Epoch: 13 [2400/2800 (86%)]\tLoss: 1.098242\n\nevaluating...\nTest set:\tAverage loss: 1.3861, Average CER: 0.406105 Average WER: 0.9015\n\nTrain Epoch: 14 [0/2800 (0%)]\tLoss: 1.176441\nTrain Epoch: 14 [400/2800 (14%)]\tLoss: 0.824145\nTrain Epoch: 14 [800/2800 (29%)]\tLoss: 1.304078\nTrain Epoch: 14 [1200/2800 (43%)]\tLoss: 1.003527\nTrain Epoch: 14 [1600/2800 (57%)]\tLoss: 1.193766\nTrain Epoch: 14 [2000/2800 (71%)]\tLoss: 0.974548\nTrain Epoch: 14 [2400/2800 (86%)]\tLoss: 0.893958\n\nevaluating...\nTest set:\tAverage loss: 1.4232, Average CER: 0.402288 Average WER: 0.8485\n\nTrain Epoch: 15 [0/2800 (0%)]\tLoss: 0.681726\nTrain Epoch: 15 [400/2800 (14%)]\tLoss: 0.857132\nTrain Epoch: 15 [800/2800 (29%)]\tLoss: 1.454946\nTrain Epoch: 15 [1200/2800 (43%)]\tLoss: 1.425026\nTrain Epoch: 15 [1600/2800 (57%)]\tLoss: 0.913960\nTrain Epoch: 15 [2000/2800 (71%)]\tLoss: 0.905513\nTrain Epoch: 15 [2400/2800 (86%)]\tLoss: 0.727496\n\nevaluating...\nTest set:\tAverage loss: 1.3893, Average CER: 0.406247 Average WER: 0.9028\n\nTrain Epoch: 16 [0/2800 (0%)]\tLoss: 1.252384\nTrain Epoch: 16 [400/2800 (14%)]\tLoss: 0.443339\nTrain Epoch: 16 [800/2800 (29%)]\tLoss: 1.075841\nTrain Epoch: 16 [1200/2800 (43%)]\tLoss: 0.853352\nTrain Epoch: 16 [1600/2800 (57%)]\tLoss: 0.963159\nTrain Epoch: 16 [2000/2800 (71%)]\tLoss: 1.206611\nTrain Epoch: 16 [2400/2800 (86%)]\tLoss: 0.666534\n\nevaluating...\nTest set:\tAverage loss: 1.3652, Average CER: 0.389703 Average WER: 0.8573\n\nTrain Epoch: 17 [0/2800 (0%)]\tLoss: 1.408716\nTrain Epoch: 17 [400/2800 (14%)]\tLoss: 0.796909\nTrain Epoch: 17 [800/2800 (29%)]\tLoss: 1.119956\nTrain Epoch: 17 [1200/2800 (43%)]\tLoss: 0.930686\nTrain Epoch: 17 [1600/2800 (57%)]\tLoss: 1.058136\nTrain Epoch: 17 [2000/2800 (71%)]\tLoss: 0.453038\nTrain Epoch: 17 [2400/2800 (86%)]\tLoss: 0.618722\n\nevaluating...\nTest set:\tAverage loss: 1.3419, Average CER: 0.376421 Average WER: 0.8134\n\nTrain Epoch: 18 [0/2800 (0%)]\tLoss: 0.734850\nTrain Epoch: 18 [400/2800 (14%)]\tLoss: 1.405655\nTrain Epoch: 18 [800/2800 (29%)]\tLoss: 1.141618\nTrain Epoch: 18 [1200/2800 (43%)]\tLoss: 1.267381\nTrain Epoch: 18 [1600/2800 (57%)]\tLoss: 0.871180\nTrain Epoch: 18 [2000/2800 (71%)]\tLoss: 0.457998\nTrain Epoch: 18 [2400/2800 (86%)]\tLoss: 0.411722\n\nevaluating...\nTest set:\tAverage loss: 1.3456, Average CER: 0.374297 Average WER: 0.8162\n\nTrain Epoch: 19 [0/2800 (0%)]\tLoss: 0.496918\nTrain Epoch: 19 [400/2800 (14%)]\tLoss: 0.691446\nTrain Epoch: 19 [800/2800 (29%)]\tLoss: 0.940345\nTrain Epoch: 19 [1200/2800 (43%)]\tLoss: 1.030467\nTrain Epoch: 19 [1600/2800 (57%)]\tLoss: 0.784668\nTrain Epoch: 19 [2000/2800 (71%)]\tLoss: 1.069888\nTrain Epoch: 19 [2400/2800 (86%)]\tLoss: 0.740920\n\nevaluating...\nTest set:\tAverage loss: 1.3693, Average CER: 0.370864 Average WER: 0.8213\n\nTrain Epoch: 20 [0/2800 (0%)]\tLoss: 0.928408\nTrain Epoch: 20 [400/2800 (14%)]\tLoss: 0.921286\nTrain Epoch: 20 [800/2800 (29%)]\tLoss: 1.047835\nTrain Epoch: 20 [1200/2800 (43%)]\tLoss: 1.476903\nTrain Epoch: 20 [1600/2800 (57%)]\tLoss: 0.697037\nTrain Epoch: 20 [2000/2800 (71%)]\tLoss: 0.475452\nTrain Epoch: 20 [2400/2800 (86%)]\tLoss: 0.709837\n\nevaluating...\nTest set:\tAverage loss: 1.3606, Average CER: 0.364473 Average WER: 0.8477\n\nTrain Epoch: 21 [0/2800 (0%)]\tLoss: 0.427700\nTrain Epoch: 21 [400/2800 (14%)]\tLoss: 0.395856\nTrain Epoch: 21 [800/2800 (29%)]\tLoss: 0.579559\nTrain Epoch: 21 [1200/2800 (43%)]\tLoss: 0.925235\nTrain Epoch: 21 [1600/2800 (57%)]\tLoss: 0.475641\nTrain Epoch: 21 [2000/2800 (71%)]\tLoss: 0.434566\nTrain Epoch: 21 [2400/2800 (86%)]\tLoss: 0.934212\n\nevaluating...\nTest set:\tAverage loss: 1.4345, Average CER: 0.366699 Average WER: 0.8244\n\nTrain Epoch: 22 [0/2800 (0%)]\tLoss: 0.426096\nTrain Epoch: 22 [400/2800 (14%)]\tLoss: 0.613922\nTrain Epoch: 22 [800/2800 (29%)]\tLoss: 0.815996\nTrain Epoch: 22 [1200/2800 (43%)]\tLoss: 0.506712\nTrain Epoch: 22 [1600/2800 (57%)]\tLoss: 0.436561\nTrain Epoch: 22 [2000/2800 (71%)]\tLoss: 1.226777\nTrain Epoch: 22 [2400/2800 (86%)]\tLoss: 0.786680\n\nevaluating...\nTest set:\tAverage loss: 1.4237, Average CER: 0.368047 Average WER: 0.8385\n\nTrain Epoch: 23 [0/2800 (0%)]\tLoss: 0.886333\nTrain Epoch: 23 [400/2800 (14%)]\tLoss: 0.348590\nTrain Epoch: 23 [800/2800 (29%)]\tLoss: 0.681487\nTrain Epoch: 23 [1200/2800 (43%)]\tLoss: 0.853395\nTrain Epoch: 23 [1600/2800 (57%)]\tLoss: 0.843926\nTrain Epoch: 23 [2000/2800 (71%)]\tLoss: 0.261400\nTrain Epoch: 23 [2400/2800 (86%)]\tLoss: 0.892703\n\nevaluating...\nTest set:\tAverage loss: 1.4215, Average CER: 0.359597 Average WER: 0.8130\n\nTrain Epoch: 24 [0/2800 (0%)]\tLoss: 0.261255\nTrain Epoch: 24 [400/2800 (14%)]\tLoss: 0.739599\nTrain Epoch: 24 [800/2800 (29%)]\tLoss: 0.639676\nTrain Epoch: 24 [1200/2800 (43%)]\tLoss: 0.370157\nTrain Epoch: 24 [1600/2800 (57%)]\tLoss: 0.645930\nTrain Epoch: 24 [2000/2800 (71%)]\tLoss: 0.513732\nTrain Epoch: 24 [2400/2800 (86%)]\tLoss: 0.157741\n\nevaluating...\nTest set:\tAverage loss: 1.4849, Average CER: 0.352864 Average WER: 0.7909\n\nTrain Epoch: 25 [0/2800 (0%)]\tLoss: 0.748332\nTrain Epoch: 25 [400/2800 (14%)]\tLoss: 0.902986\nTrain Epoch: 25 [800/2800 (29%)]\tLoss: 0.468882\nTrain Epoch: 25 [1200/2800 (43%)]\tLoss: 0.859162\nTrain Epoch: 25 [1600/2800 (57%)]\tLoss: 0.367399\nTrain Epoch: 25 [2000/2800 (71%)]\tLoss: 0.217816\nTrain Epoch: 25 [2400/2800 (86%)]\tLoss: 0.546130\n\nevaluating...\nTest set:\tAverage loss: 1.4803, Average CER: 0.344311 Average WER: 0.8312\n\nTrain Epoch: 26 [0/2800 (0%)]\tLoss: 0.182708\nTrain Epoch: 26 [400/2800 (14%)]\tLoss: 0.621111\nTrain Epoch: 26 [800/2800 (29%)]\tLoss: 0.324369\nTrain Epoch: 26 [1200/2800 (43%)]\tLoss: 0.791975\nTrain Epoch: 26 [1600/2800 (57%)]\tLoss: 0.429768\nTrain Epoch: 26 [2000/2800 (71%)]\tLoss: 0.218896\nTrain Epoch: 26 [2400/2800 (86%)]\tLoss: 0.811700\n\nevaluating...\nTest set:\tAverage loss: 1.5332, Average CER: 0.348805 Average WER: 0.8217\n\nTrain Epoch: 27 [0/2800 (0%)]\tLoss: 0.605862\nTrain Epoch: 27 [400/2800 (14%)]\tLoss: 0.641894\nTrain Epoch: 27 [800/2800 (29%)]\tLoss: 0.387439\nTrain Epoch: 27 [1200/2800 (43%)]\tLoss: 0.329077\nTrain Epoch: 27 [1600/2800 (57%)]\tLoss: 0.448046\nTrain Epoch: 27 [2000/2800 (71%)]\tLoss: 0.378240\nTrain Epoch: 27 [2400/2800 (86%)]\tLoss: 0.433902\n\nevaluating...\nTest set:\tAverage loss: 1.5499, Average CER: 0.351626 Average WER: 0.8280\n\nTrain Epoch: 28 [0/2800 (0%)]\tLoss: 0.669468\nTrain Epoch: 28 [400/2800 (14%)]\tLoss: 0.660015\nTrain Epoch: 28 [800/2800 (29%)]\tLoss: 0.564417\nTrain Epoch: 28 [1200/2800 (43%)]\tLoss: 0.762930\nTrain Epoch: 28 [1600/2800 (57%)]\tLoss: 0.721129\nTrain Epoch: 28 [2000/2800 (71%)]\tLoss: 0.537061\nTrain Epoch: 28 [2400/2800 (86%)]\tLoss: 0.967195\n\nevaluating...\nTest set:\tAverage loss: 1.5727, Average CER: 0.350532 Average WER: 0.8447\n\nTrain Epoch: 29 [0/2800 (0%)]\tLoss: 0.578930\nTrain Epoch: 29 [400/2800 (14%)]\tLoss: 0.534527\nTrain Epoch: 29 [800/2800 (29%)]\tLoss: 0.426198\nTrain Epoch: 29 [1200/2800 (43%)]\tLoss: 0.434465\nTrain Epoch: 29 [1600/2800 (57%)]\tLoss: 0.259322\nTrain Epoch: 29 [2000/2800 (71%)]\tLoss: 0.317482\nTrain Epoch: 29 [2400/2800 (86%)]\tLoss: 0.424992\n\nevaluating...\nTest set:\tAverage loss: 1.5880, Average CER: 0.342776 Average WER: 0.8313\n\nTrain Epoch: 30 [0/2800 (0%)]\tLoss: 0.782806\nTrain Epoch: 30 [400/2800 (14%)]\tLoss: 0.668902\nTrain Epoch: 30 [800/2800 (29%)]\tLoss: 0.374765\nTrain Epoch: 30 [1200/2800 (43%)]\tLoss: 0.273024\nTrain Epoch: 30 [1600/2800 (57%)]\tLoss: 0.570798\nTrain Epoch: 30 [2000/2800 (71%)]\tLoss: 0.479601\nTrain Epoch: 30 [2400/2800 (86%)]\tLoss: 0.528664\n\nevaluating...\nTest set:\tAverage loss: 1.6476, Average CER: 0.346753 Average WER: 0.8399\n\nTrain Epoch: 31 [0/2800 (0%)]\tLoss: 0.389070\nTrain Epoch: 31 [400/2800 (14%)]\tLoss: 0.461477\nTrain Epoch: 31 [800/2800 (29%)]\tLoss: 0.638411\nTrain Epoch: 31 [1200/2800 (43%)]\tLoss: 0.034404\nTrain Epoch: 31 [1600/2800 (57%)]\tLoss: 0.508587\nTrain Epoch: 31 [2000/2800 (71%)]\tLoss: 0.455017\nTrain Epoch: 31 [2400/2800 (86%)]\tLoss: 0.582457\n\nevaluating...\nTest set:\tAverage loss: 1.6781, Average CER: 0.339162 Average WER: 0.8407\n\nTrain Epoch: 32 [0/2800 (0%)]\tLoss: 0.435336\nTrain Epoch: 32 [400/2800 (14%)]\tLoss: 0.035684\nTrain Epoch: 32 [800/2800 (29%)]\tLoss: 0.617799\nTrain Epoch: 32 [1200/2800 (43%)]\tLoss: 0.321126\nTrain Epoch: 32 [1600/2800 (57%)]\tLoss: 0.298676\nTrain Epoch: 32 [2000/2800 (71%)]\tLoss: 0.564767\nTrain Epoch: 32 [2400/2800 (86%)]\tLoss: 0.111739\n\nevaluating...\nTest set:\tAverage loss: 1.7485, Average CER: 0.347966 Average WER: 0.8180\n\nTrain Epoch: 33 [0/2800 (0%)]\tLoss: 0.610401\nTrain Epoch: 33 [400/2800 (14%)]\tLoss: 0.329714\nTrain Epoch: 33 [800/2800 (29%)]\tLoss: 0.632248\nTrain Epoch: 33 [1200/2800 (43%)]\tLoss: 0.329176\nTrain Epoch: 33 [1600/2800 (57%)]\tLoss: 0.018137\nTrain Epoch: 33 [2000/2800 (71%)]\tLoss: 0.337386\nTrain Epoch: 33 [2400/2800 (86%)]\tLoss: 0.254471\n\nevaluating...\nTest set:\tAverage loss: 1.7771, Average CER: 0.331850 Average WER: 0.8371\n\nTrain Epoch: 34 [0/2800 (0%)]\tLoss: 0.479899\nTrain Epoch: 34 [400/2800 (14%)]\tLoss: 0.686454\nTrain Epoch: 34 [800/2800 (29%)]\tLoss: 0.236137\nTrain Epoch: 34 [1200/2800 (43%)]\tLoss: 0.342214\nTrain Epoch: 34 [1600/2800 (57%)]\tLoss: 0.784735\nTrain Epoch: 34 [2000/2800 (71%)]\tLoss: 0.467426\nTrain Epoch: 34 [2400/2800 (86%)]\tLoss: 0.138775\n\nevaluating...\nTest set:\tAverage loss: 1.7909, Average CER: 0.336511 Average WER: 0.8094\n\nTrain Epoch: 35 [0/2800 (0%)]\tLoss: 0.333275\nTrain Epoch: 35 [400/2800 (14%)]\tLoss: 0.237749\nTrain Epoch: 35 [800/2800 (29%)]\tLoss: 0.189277\nTrain Epoch: 35 [1200/2800 (43%)]\tLoss: 0.437836\nTrain Epoch: 35 [1600/2800 (57%)]\tLoss: 0.336338\nTrain Epoch: 35 [2000/2800 (71%)]\tLoss: 0.356653\nTrain Epoch: 35 [2400/2800 (86%)]\tLoss: 0.255534\n\nevaluating...\nTest set:\tAverage loss: 1.8732, Average CER: 0.334558 Average WER: 0.8059\n\nTrain Epoch: 36 [0/2800 (0%)]\tLoss: 0.233784\nTrain Epoch: 36 [400/2800 (14%)]\tLoss: 0.014195\nTrain Epoch: 36 [800/2800 (29%)]\tLoss: 0.213146\nTrain Epoch: 36 [1200/2800 (43%)]\tLoss: 0.326828\nTrain Epoch: 36 [1600/2800 (57%)]\tLoss: 0.505674\nTrain Epoch: 36 [2000/2800 (71%)]\tLoss: 0.445830\nTrain Epoch: 36 [2400/2800 (86%)]\tLoss: 0.307606\n\nevaluating...\nTest set:\tAverage loss: 1.9669, Average CER: 0.339814 Average WER: 0.8473\n\nTrain Epoch: 37 [0/2800 (0%)]\tLoss: 0.126152\nTrain Epoch: 37 [400/2800 (14%)]\tLoss: 0.132512\nTrain Epoch: 37 [800/2800 (29%)]\tLoss: 0.289301\nTrain Epoch: 37 [1200/2800 (43%)]\tLoss: 0.166692\nTrain Epoch: 37 [1600/2800 (57%)]\tLoss: 0.357702\nTrain Epoch: 37 [2000/2800 (71%)]\tLoss: 0.483312\nTrain Epoch: 37 [2400/2800 (86%)]\tLoss: 0.123064\n\nevaluating...\nTest set:\tAverage loss: 1.9746, Average CER: 0.333513 Average WER: 0.8310\n\nTrain Epoch: 38 [0/2800 (0%)]\tLoss: 0.540135\nTrain Epoch: 38 [400/2800 (14%)]\tLoss: 0.180466\nTrain Epoch: 38 [800/2800 (29%)]\tLoss: 0.221327\nTrain Epoch: 38 [1200/2800 (43%)]\tLoss: 0.059567\nTrain Epoch: 38 [1600/2800 (57%)]\tLoss: 0.068922\nTrain Epoch: 38 [2000/2800 (71%)]\tLoss: 0.126524\nTrain Epoch: 38 [2400/2800 (86%)]\tLoss: 0.352811\n\nevaluating...\nTest set:\tAverage loss: 2.0827, Average CER: 0.351064 Average WER: 0.8607\n\nTrain Epoch: 39 [0/2800 (0%)]\tLoss: 0.211165\nTrain Epoch: 39 [400/2800 (14%)]\tLoss: 0.234358\nTrain Epoch: 39 [800/2800 (29%)]\tLoss: 0.149755\nTrain Epoch: 39 [1200/2800 (43%)]\tLoss: 0.208736\nTrain Epoch: 39 [1600/2800 (57%)]\tLoss: 0.274149\nTrain Epoch: 39 [2000/2800 (71%)]\tLoss: 0.243742\nTrain Epoch: 39 [2400/2800 (86%)]\tLoss: 0.279341\n\nevaluating...\nTest set:\tAverage loss: 2.1469, Average CER: 0.346940 Average WER: 0.8504\n\nTrain Epoch: 40 [0/2800 (0%)]\tLoss: 0.153129\nTrain Epoch: 40 [400/2800 (14%)]\tLoss: 0.239334\nTrain Epoch: 40 [800/2800 (29%)]\tLoss: 0.132830\nTrain Epoch: 40 [1200/2800 (43%)]\tLoss: 0.178994\nTrain Epoch: 40 [1600/2800 (57%)]\tLoss: 0.130846\nTrain Epoch: 40 [2000/2800 (71%)]\tLoss: 0.177099\nTrain Epoch: 40 [2400/2800 (86%)]\tLoss: 0.232268\n\nevaluating...\nTest set:\tAverage loss: 2.1829, Average CER: 0.339918 Average WER: 0.8283\n\nTrain Epoch: 41 [0/2800 (0%)]\tLoss: 0.090958\nTrain Epoch: 41 [400/2800 (14%)]\tLoss: 0.025485\nTrain Epoch: 41 [800/2800 (29%)]\tLoss: 0.124193\nTrain Epoch: 41 [1200/2800 (43%)]\tLoss: 0.238251\nTrain Epoch: 41 [1600/2800 (57%)]\tLoss: 0.007427\nTrain Epoch: 41 [2000/2800 (71%)]\tLoss: 0.224075\nTrain Epoch: 41 [2400/2800 (86%)]\tLoss: 0.305309\n\nevaluating...\nTest set:\tAverage loss: 2.2541, Average CER: 0.355917 Average WER: 0.8526\n\nTrain Epoch: 42 [0/2800 (0%)]\tLoss: 0.110490\nTrain Epoch: 42 [400/2800 (14%)]\tLoss: 0.071227\nTrain Epoch: 42 [800/2800 (29%)]\tLoss: 0.152076\nTrain Epoch: 42 [1200/2800 (43%)]\tLoss: 0.086874\nTrain Epoch: 42 [1600/2800 (57%)]\tLoss: 0.278787\nTrain Epoch: 42 [2000/2800 (71%)]\tLoss: 0.155023\nTrain Epoch: 42 [2400/2800 (86%)]\tLoss: 0.328194\n\nevaluating...\nTest set:\tAverage loss: 2.2878, Average CER: 0.358404 Average WER: 0.8493\n\nTrain Epoch: 43 [0/2800 (0%)]\tLoss: 0.137616\nTrain Epoch: 43 [400/2800 (14%)]\tLoss: 0.148073\nTrain Epoch: 43 [800/2800 (29%)]\tLoss: 0.010550\nTrain Epoch: 43 [1200/2800 (43%)]\tLoss: 0.079463\nTrain Epoch: 43 [1600/2800 (57%)]\tLoss: 0.071254\nTrain Epoch: 43 [2000/2800 (71%)]\tLoss: 0.252694\nTrain Epoch: 43 [2400/2800 (86%)]\tLoss: 0.142171\n\nevaluating...\nTest set:\tAverage loss: 2.3448, Average CER: 0.344063 Average WER: 0.8302\n\nTrain Epoch: 44 [0/2800 (0%)]\tLoss: 0.177809\nTrain Epoch: 44 [400/2800 (14%)]\tLoss: 0.030885\nTrain Epoch: 44 [800/2800 (29%)]\tLoss: 0.048809\nTrain Epoch: 44 [1200/2800 (43%)]\tLoss: 0.186209\nTrain Epoch: 44 [1600/2800 (57%)]\tLoss: 0.020009\nTrain Epoch: 44 [2000/2800 (71%)]\tLoss: 0.122912\nTrain Epoch: 44 [2400/2800 (86%)]\tLoss: 0.054802\n\nevaluating...\nTest set:\tAverage loss: 2.3812, Average CER: 0.347207 Average WER: 0.8363\n\nTrain Epoch: 45 [0/2800 (0%)]\tLoss: 0.054760\nTrain Epoch: 45 [400/2800 (14%)]\tLoss: 0.000742\nTrain Epoch: 45 [800/2800 (29%)]\tLoss: 0.090505\nTrain Epoch: 45 [1200/2800 (43%)]\tLoss: 0.038369\nTrain Epoch: 45 [1600/2800 (57%)]\tLoss: 0.023405\nTrain Epoch: 45 [2000/2800 (71%)]\tLoss: 0.054656\nTrain Epoch: 45 [2400/2800 (86%)]\tLoss: 0.063151\n\nevaluating...\nTest set:\tAverage loss: 2.4475, Average CER: 0.345847 Average WER: 0.8318\n\nTrain Epoch: 46 [0/2800 (0%)]\tLoss: 0.048650\nTrain Epoch: 46 [400/2800 (14%)]\tLoss: 0.157854\nTrain Epoch: 46 [800/2800 (29%)]\tLoss: 0.096044\nTrain Epoch: 46 [1200/2800 (43%)]\tLoss: 0.018704\nTrain Epoch: 46 [1600/2800 (57%)]\tLoss: 0.039197\nTrain Epoch: 46 [2000/2800 (71%)]\tLoss: 0.044231\nTrain Epoch: 46 [2400/2800 (86%)]\tLoss: 0.039700\n\nevaluating...\nTest set:\tAverage loss: 2.4664, Average CER: 0.341886 Average WER: 0.8284\n\nTrain Epoch: 47 [0/2800 (0%)]\tLoss: 0.036218\nTrain Epoch: 47 [400/2800 (14%)]\tLoss: 0.019657\nTrain Epoch: 47 [800/2800 (29%)]\tLoss: 0.071758\nTrain Epoch: 47 [1200/2800 (43%)]\tLoss: 0.086663\nTrain Epoch: 47 [1600/2800 (57%)]\tLoss: 0.096939\nTrain Epoch: 47 [2000/2800 (71%)]\tLoss: 0.044151\nTrain Epoch: 47 [2400/2800 (86%)]\tLoss: 0.079955\n\nevaluating...\nTest set:\tAverage loss: 2.4871, Average CER: 0.349384 Average WER: 0.8267\n\nTrain Epoch: 48 [0/2800 (0%)]\tLoss: 0.101709\nTrain Epoch: 48 [400/2800 (14%)]\tLoss: 0.081553\nTrain Epoch: 48 [800/2800 (29%)]\tLoss: 0.105557\nTrain Epoch: 48 [1200/2800 (43%)]\tLoss: 0.147630\nTrain Epoch: 48 [1600/2800 (57%)]\tLoss: 0.088399\nTrain Epoch: 48 [2000/2800 (71%)]\tLoss: 0.066118\nTrain Epoch: 48 [2400/2800 (86%)]\tLoss: 0.077190\n\nevaluating...\nTest set:\tAverage loss: 2.5279, Average CER: 0.349107 Average WER: 0.8252\n\nTrain Epoch: 49 [0/2800 (0%)]\tLoss: 0.162295\nTrain Epoch: 49 [400/2800 (14%)]\tLoss: 0.210280\nTrain Epoch: 49 [800/2800 (29%)]\tLoss: 0.007251\nTrain Epoch: 49 [1200/2800 (43%)]\tLoss: 0.156413\nTrain Epoch: 49 [1600/2800 (57%)]\tLoss: 0.031658\nTrain Epoch: 49 [2000/2800 (71%)]\tLoss: 0.026776\nTrain Epoch: 49 [2400/2800 (86%)]\tLoss: 0.005095\n\nevaluating...\nTest set:\tAverage loss: 2.5480, Average CER: 0.347034 Average WER: 0.8268\n\nTrain Epoch: 50 [0/2800 (0%)]\tLoss: 0.019597\nTrain Epoch: 50 [400/2800 (14%)]\tLoss: 0.065915\nTrain Epoch: 50 [800/2800 (29%)]\tLoss: 0.002083\nTrain Epoch: 50 [1200/2800 (43%)]\tLoss: 0.089030\nTrain Epoch: 50 [1600/2800 (57%)]\tLoss: 0.052762\nTrain Epoch: 50 [2000/2800 (71%)]\tLoss: 0.041630\nTrain Epoch: 50 [2400/2800 (86%)]\tLoss: 0.066413\n\nevaluating...\nTest set:\tAverage loss: 2.5558, Average CER: 0.346562 Average WER: 0.8264\n\nCPU times: user 29min 48s, sys: 1min 17s, total: 31min 5s\nWall time: 31min 4s\n","output_type":"stream"}]},{"cell_type":"code","source":"#use_cuda = torch.cuda.is_available()\n#device = torch.device(\"cpu\")\nneeded_device = torch.device(\"cpu\")\nmodel = torch.load('/kaggle/input/dop-test-files/model_for_making_dataset_v6(20024).pt', map_location=torch.device('cpu'))\n\n#1543 1882 1372\n\nmodel.to(needed_device)\nprint(needed_device)\n#predict(model, '/kaggle/input/upd-speech/mono_voice/1964.wav', device)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:17:41.742766Z","iopub.execute_input":"2024-05-26T04:17:41.743182Z","iopub.status.idle":"2024-05-26T04:17:41.996611Z","shell.execute_reply.started":"2024-05-26T04:17:41.743142Z","shell.execute_reply":"2024-05-26T04:17:41.994992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = {'X_test': X_test, 'label': y_test}\ndf_test = pd.DataFrame(data=d)\ndf_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:17:41.998094Z","iopub.execute_input":"2024-05-26T04:17:41.998687Z","iopub.status.idle":"2024-05-26T04:17:51.083467Z","shell.execute_reply.started":"2024-05-26T04:17:41.998647Z","shell.execute_reply":"2024-05-26T04:17:51.082064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[:5]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:17:51.085635Z","iopub.execute_input":"2024-05-26T04:17:51.086168Z","iopub.status.idle":"2024-05-26T04:17:51.094498Z","shell.execute_reply.started":"2024-05-26T04:17:51.086116Z","shell.execute_reply":"2024-05-26T04:17:51.093194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_test_cer(row, model):\n    prediction = predict_with_tensor_v3(model, row['X_test'])\n    return cer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:17:51.099695Z","iopub.execute_input":"2024-05-26T04:17:51.100268Z","iopub.status.idle":"2024-05-26T04:17:51.106727Z","shell.execute_reply.started":"2024-05-26T04:17:51.100225Z","shell.execute_reply":"2024-05-26T04:17:51.105456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_test_wer(row, model):\n    prediction = predict_with_tensor_v3(model, row['X_test'])\n    return wer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:17:51.108189Z","iopub.execute_input":"2024-05-26T04:17:51.108568Z","iopub.status.idle":"2024-05-26T04:17:51.120213Z","shell.execute_reply.started":"2024-05-26T04:17:51.108530Z","shell.execute_reply":"2024-05-26T04:17:51.118791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_preds(row, model):\n    return predict_with_tensor_v3(model, row['X_test'])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:17:51.121480Z","iopub.execute_input":"2024-05-26T04:17:51.121846Z","iopub.status.idle":"2024-05-26T04:17:51.133747Z","shell.execute_reply.started":"2024-05-26T04:17:51.121800Z","shell.execute_reply":"2024-05-26T04:17:51.132140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['CER'] = df_test.apply(count_test_cer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:17:51.135252Z","iopub.execute_input":"2024-05-26T04:17:51.135630Z","iopub.status.idle":"2024-05-26T04:20:18.237700Z","shell.execute_reply.started":"2024-05-26T04:17:51.135592Z","shell.execute_reply":"2024-05-26T04:20:18.235582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['WER'] = df_test.apply(count_test_wer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:20:18.240879Z","iopub.execute_input":"2024-05-26T04:20:18.241567Z","iopub.status.idle":"2024-05-26T04:22:37.380675Z","shell.execute_reply.started":"2024-05-26T04:20:18.241502Z","shell.execute_reply":"2024-05-26T04:22:37.379257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['preds'] = df_test.apply(write_preds, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:22:37.382399Z","iopub.execute_input":"2024-05-26T04:22:37.382791Z","iopub.status.idle":"2024-05-26T04:24:56.562000Z","shell.execute_reply.started":"2024-05-26T04:22:37.382752Z","shell.execute_reply":"2024-05-26T04:24:56.560548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.loc[df_test['CER'] > 0]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:24:56.564034Z","iopub.execute_input":"2024-05-26T04:24:56.564688Z","iopub.status.idle":"2024-05-26T04:25:07.553845Z","shell.execute_reply.started":"2024-05-26T04:24:56.564626Z","shell.execute_reply":"2024-05-26T04:25:07.552232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Проверяем затюненый корректор t5 (на 10 эпохах)","metadata":{}},{"cell_type":"code","source":"print('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ИСПОЛЬЗОВАЛ МАЛЫЙ СЛОВАРЬ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model(3024_seed_data).pt with hunspell. WITHOUT HUNSPELL: CER = Average CER: 0.166791 Average WER: 0.6451","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['CER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:04:49.056490Z","iopub.execute_input":"2024-05-25T16:04:49.057082Z","iopub.status.idle":"2024-05-25T16:04:49.065840Z","shell.execute_reply.started":"2024-05-25T16:04:49.057036Z","shell.execute_reply":"2024-05-25T16:04:49.064572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['WER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:04:51.749217Z","iopub.execute_input":"2024-05-25T16:04:51.750349Z","iopub.status.idle":"2024-05-25T16:04:51.757655Z","shell.execute_reply.started":"2024-05-25T16:04:51.750300Z","shell.execute_reply":"2024-05-25T16:04:51.756495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v4(1242).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:19:41.442613Z","iopub.execute_input":"2024-05-25T16:19:41.443296Z","iopub.status.idle":"2024-05-25T16:19:41.455403Z","shell.execute_reply.started":"2024-05-25T16:19:41.443235Z","shell.execute_reply":"2024-05-25T16:19:41.452786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v5(2204).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:37:13.375118Z","iopub.execute_input":"2024-05-25T16:37:13.375680Z","iopub.status.idle":"2024-05-25T16:37:13.386340Z","shell.execute_reply.started":"2024-05-25T16:37:13.375629Z","shell.execute_reply":"2024-05-25T16:37:13.384681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v6(20024).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:49:05.441566Z","iopub.execute_input":"2024-05-25T16:49:05.442097Z","iopub.status.idle":"2024-05-25T16:49:05.451177Z","shell.execute_reply.started":"2024-05-25T16:49:05.442048Z","shell.execute_reply":"2024-05-25T16:49:05.449638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v7(3016).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:59:36.002854Z","iopub.execute_input":"2024-05-25T16:59:36.004626Z","iopub.status.idle":"2024-05-25T16:59:36.014691Z","shell.execute_reply.started":"2024-05-25T16:59:36.004549Z","shell.execute_reply":"2024-05-25T16:59:36.012949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ДАЛЕЕ ИСПОЛЬЗУЕТСЯ БОЛЬШИЙ СЛОВАРЬ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v4(1242).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:22:32.461589Z","iopub.execute_input":"2024-05-25T17:22:32.462082Z","iopub.status.idle":"2024-05-25T17:22:32.471480Z","shell.execute_reply.started":"2024-05-25T17:22:32.462038Z","shell.execute_reply":"2024-05-25T17:22:32.469970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v5(2204).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:03:18.899437Z","iopub.execute_input":"2024-05-26T04:03:18.900728Z","iopub.status.idle":"2024-05-26T04:03:18.909759Z","shell.execute_reply.started":"2024-05-26T04:03:18.900673Z","shell.execute_reply":"2024-05-26T04:03:18.908456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v6(20024).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:14:20.150015Z","iopub.execute_input":"2024-05-26T04:14:20.150512Z","iopub.status.idle":"2024-05-26T04:14:20.160144Z","shell.execute_reply.started":"2024-05-26T04:14:20.150466Z","shell.execute_reply":"2024-05-26T04:14:20.158071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v7(3016).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-26T04:25:27.601745Z","iopub.execute_input":"2024-05-26T04:25:27.602257Z","iopub.status.idle":"2024-05-26T04:25:27.611942Z","shell.execute_reply.started":"2024-05-26T04:25:27.602207Z","shell.execute_reply":"2024-05-26T04:25:27.610181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:26:24.339488Z","iopub.execute_input":"2023-05-24T10:26:24.340405Z","iopub.status.idle":"2023-05-24T10:26:24.350954Z","shell.execute_reply.started":"2023-05-24T10:26:24.340364Z","shell.execute_reply":"2023-05-24T10:26:24.349831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:16:11.401175Z","iopub.execute_input":"2023-05-13T16:16:11.401879Z","iopub.status.idle":"2023-05-13T16:16:11.430020Z","shell.execute_reply.started":"2023-05-13T16:16:11.401838Z","shell.execute_reply":"2023-05-13T16:16:11.428960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(directory):\n    total_duration = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            filepath = os.path.join(directory, filename)\n            with wave.open(filepath, 'r') as wav_file:\n                frames = wav_file.getnframes()\n                rate = wav_file.getframerate()\n                duration = frames / float(rate)\n                total_duration += duration\n    return total_duration\n\ndirectory = '/kaggle/input/upd-speech/mono_voice'\ntotal_duration = get_wav_duration(directory)\nprint('Total duration of WAV files:', total_duration, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:15.415086Z","iopub.execute_input":"2023-07-05T10:09:15.415876Z","iopub.status.idle":"2023-07-05T10:09:18.755936Z","shell.execute_reply.started":"2023-07-05T10:09:15.415836Z","shell.execute_reply":"2023-07-05T10:09:18.754693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return '{:02d}:{:02d}:{:02d}'.format(int(hours), int(minutes), int(seconds))\nseconds = 3661\nformatted_time = format_time(total_duration)\nprint(formatted_time)  # Output: '01:01:01'","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:23.353548Z","iopub.execute_input":"2023-07-05T10:09:23.354296Z","iopub.status.idle":"2023-07-05T10:09:23.361628Z","shell.execute_reply.started":"2023-07-05T10:09:23.354254Z","shell.execute_reply":"2023-07-05T10:09:23.360431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}