{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5321255,"sourceType":"datasetVersion","datasetId":3091651},{"sourceId":5618710,"sourceType":"datasetVersion","datasetId":3230790},{"sourceId":5677279,"sourceType":"datasetVersion","datasetId":2989949},{"sourceId":5677449,"sourceType":"datasetVersion","datasetId":3071831},{"sourceId":5760288,"sourceType":"datasetVersion","datasetId":3311237},{"sourceId":8515857,"sourceType":"datasetVersion","datasetId":3213578},{"sourceId":8537485,"sourceType":"datasetVersion","datasetId":5099750},{"sourceId":8537499,"sourceType":"datasetVersion","datasetId":5099761},{"sourceId":8537514,"sourceType":"datasetVersion","datasetId":5099772},{"sourceId":8537530,"sourceType":"datasetVersion","datasetId":5099776},{"sourceId":8560616,"sourceType":"datasetVersion","datasetId":4230886},{"sourceId":8562283,"sourceType":"datasetVersion","datasetId":5118180}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np \nimport matplotlib\nfrom transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:52:30.949834Z","iopub.execute_input":"2024-06-09T11:52:30.950784Z","iopub.status.idle":"2024-06-09T11:52:34.845136Z","shell.execute_reply.started":"2024-06-09T11:52:30.950739Z","shell.execute_reply":"2024-06-09T11:52:34.843974Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        int_sequence = []\n        for c in text:\n            ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MFCC(n_mfcc=20)\n)\n\n\nvalid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//3)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=34, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:52:34.847772Z","iopub.execute_input":"2024-06-09T11:52:34.848467Z","iopub.status.idle":"2024-06-09T11:52:34.979014Z","shell.execute_reply.started":"2024-06-09T11:52:34.848430Z","shell.execute_reply":"2024-06-09T11:52:34.977847Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"class BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:52:34.981475Z","iopub.execute_input":"2024-06-09T11:52:34.982432Z","iopub.status.idle":"2024-06-09T11:52:34.990990Z","shell.execute_reply.started":"2024-06-09T11:52:34.982396Z","shell.execute_reply":"2024-06-09T11:52:34.989966Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Поменял там, где происходит загрузка, сохраняется id звукового файла, а потом в excel файле по колонке old_id ищется текст\n#И того звук и текст к нему\n\nimport pandas as pd\nimport librosa\n\n# file = pd.read_excel('/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches v1.xlsx')\n# #y = [sentence for sentence in file['text']]\n# y = []\n# dir_name = \"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"\n# files_in_dir = os.listdir(dir_name)\n\n# X = []\n# i = 1\n\n# for e in os.listdir(\"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"):\n#     file_name = e\n#     for old_id in range(0, 2073):\n#         if file_name.startswith(str(file['old_id'][old_id]) + '.'):\n#             y.extend([''.join(file['text'][old_id])])\n#             sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n#             sampl = sampl[np.newaxis, :]\n#             X.append(torch.Tensor(sampl))\n#             break\n\n# file = pd.read_excel('/kaggle/input/dataset-with-3-speakers/Speeches v1.xlsx')\n# #y = [sentence for sentence in file['text']]\n# y = []\n# dir_name = \"/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/\"\n# files_in_dir = os.listdir(dir_name)\n\n# X = []\n# i = 1\n\n# for e in os.listdir(\"/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/\"):\n#     file_name = e\n#     for old_id in range(0, 2073):\n#         if file_name.startswith(str(file['old_id'][old_id])[:-1]):\n#             y.extend([''.join(file['text'][old_id])])\n#             sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n#             sampl = sampl[np.newaxis, :]\n#             X.append(torch.Tensor(sampl))\n#             break\n\nfile = pd.read_excel('/kaggle/input/dataset-with-3-speakers/Speeches v1.xlsx')\n#y = [sentence for sentence in file['text']]\ny = []\ndir_name = \"/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in os.listdir(\"/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/\"):\n    file_name = e\n    for old_id in range(0, 2073):\n        if file_name.startswith(str(file['old_id'][old_id])[:-1]):\n            if file_name.startswith(str(file['old_id'][old_id])[:-1] + '3'):\n                label = [''.join(file['text'][file['old_id'][old_id]])]\n                y.extend(label)\n                sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n                sampl = sampl[np.newaxis, :]\n                X.append(torch.Tensor(sampl))\n            else: \n                label = [''.join(file['text'][old_id])]\n                y.extend(label)\n                sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n                sampl = sampl[np.newaxis, :]\n                X.append(torch.Tensor(sampl))\n            break","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:52:34.993072Z","iopub.execute_input":"2024-06-09T11:52:34.993478Z","iopub.status.idle":"2024-06-09T11:54:54.554163Z","shell.execute_reply.started":"2024-06-09T11:52:34.993449Z","shell.execute_reply":"2024-06-09T11:54:54.553088Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"y[24]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:29:02.274606Z","iopub.execute_input":"2024-06-09T10:29:02.275035Z","iopub.status.idle":"2024-06-09T10:29:02.282343Z","shell.execute_reply.started":"2024-06-09T10:29:02.274996Z","shell.execute_reply":"2024-06-09T10:29:02.281191Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Сладкий шоколад'"},"metadata":{}}]},{"cell_type":"code","source":"#waveform, sample_rate = torchaudio.load('/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/3000.3.wav')  # Загрузка аудиофайла\ntorchaudio.save('/kaggle/working/audio.wav', X[24], 16000)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:29:05.054557Z","iopub.execute_input":"2024-06-09T10:29:05.055018Z","iopub.status.idle":"2024-06-09T10:29:05.061783Z","shell.execute_reply.started":"2024-06-09T10:29:05.054973Z","shell.execute_reply":"2024-06-09T10:29:05.060577Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import random\npairs = list(zip(X, y))\nrandom.Random(20024).shuffle(pairs)\nX, y = zip(*pairs)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:54:54.556209Z","iopub.execute_input":"2024-06-09T11:54:54.556828Z","iopub.status.idle":"2024-06-09T11:54:54.568694Z","shell.execute_reply.started":"2024-06-09T11:54:54.556795Z","shell.execute_reply":"2024-06-09T11:54:54.567722Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"y[:3]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:54:54.569967Z","iopub.execute_input":"2024-06-09T11:54:54.570333Z","iopub.status.idle":"2024-06-09T11:54:54.581251Z","shell.execute_reply.started":"2024-06-09T11:54:54.570296Z","shell.execute_reply":"2024-06-09T11:54:54.580167Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('Деньги это власть',\n 'Твоя поддержка была для меня невероятно важна',\n 'Купить лекарства от насморка')"},"metadata":{}}]},{"cell_type":"code","source":"X[:3]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:54:54.583624Z","iopub.execute_input":"2024-06-09T11:54:54.584559Z","iopub.status.idle":"2024-06-09T11:54:54.607454Z","shell.execute_reply.started":"2024-06-09T11:54:54.584527Z","shell.execute_reply":"2024-06-09T11:54:54.606485Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -4.3147e-05,\n          -4.8934e-05, -5.3347e-05]]),\n tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0008,  0.0014,  0.0000]]),\n tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0025, 0.0000]]))"},"metadata":{}}]},{"cell_type":"code","source":"torchaudio.save('/kaggle/working/audio.wav', X[540], 16000)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.960331Z","iopub.status.idle":"2024-06-09T08:55:22.961016Z","shell.execute_reply.started":"2024-06-09T08:55:22.960649Z","shell.execute_reply":"2024-06-09T08:55:22.960687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"waveform, sample_rate = torchaudio.load('/kaggle/working/audio.wav')  # Загрузка аудиофайла\ntorchaudio.play(waveform, sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.962680Z","iopub.status.idle":"2024-06-09T08:55:22.963346Z","shell.execute_reply.started":"2024-06-09T08:55:22.963012Z","shell.execute_reply":"2024-06-09T08:55:22.963047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('4', 'четыре').replace('Р-220', 'р двести двадцать').replace('6', 'шесть').replace(\"-\", \" \")\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    sentence = \" \".join(sentence.split())\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:54:54.608672Z","iopub.execute_input":"2024-06-09T11:54:54.609032Z","iopub.status.idle":"2024-06-09T11:54:54.643988Z","shell.execute_reply.started":"2024-06-09T11:54:54.609004Z","shell.execute_reply":"2024-06-09T11:54:54.642887Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train = X[:2800]\nX_test = X[2800:]\ny_train = y[:2800]\ny_test = y[2800:]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:54:54.645272Z","iopub.execute_input":"2024-06-09T11:54:54.645878Z","iopub.status.idle":"2024-06-09T11:54:54.651146Z","shell.execute_reply.started":"2024-06-09T11:54:54.645839Z","shell.execute_reply":"2024-06-09T11:54:54.650113Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:54:54.652401Z","iopub.execute_input":"2024-06-09T11:54:54.652812Z","iopub.status.idle":"2024-06-09T11:54:54.660305Z","shell.execute_reply.started":"2024-06-09T11:54:54.652783Z","shell.execute_reply":"2024-06-09T11:54:54.659084Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 32, kernel_size=(4,4), stride=(3,3), padding=(2,2)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.Conv2d(128, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n        )\n        \n        self.fc_1 = nn.Sequential(\n            nn.Linear(896, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n        )\n        \n        self.BiGRU_1 = BidirectionalGRU(270, 270, 0, True)\n        self.BiGRU_2 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_3 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_4 = BidirectionalGRU(540, 270, 0.5, True)\n        \n        self.fc_2 = nn.Sequential(\n            nn.Linear(540, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.fc_1(x)\n        x = self.BiGRU_1(x)\n        x = self.BiGRU_2(x)\n        x = self.BiGRU_3(x)\n        x = self.BiGRU_4(x)\n        x = self.fc_2(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:54:54.661896Z","iopub.execute_input":"2024-06-09T11:54:54.662269Z","iopub.status.idle":"2024-06-09T11:54:54.678453Z","shell.execute_reply.started":"2024-06-09T11:54:54.662240Z","shell.execute_reply":"2024-06-09T11:54:54.677419Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Зададим название выбронной модели из хаба\nMODEL_NAME = 'UrukHan/t5-russian-spell'\nMAX_INPUT = 256\n\n# Загрузка модели и токенизатора\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\ncorrector = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.975356Z","iopub.status.idle":"2024-06-09T08:55:22.976001Z","shell.execute_reply.started":"2024-06-09T08:55:22.975647Z","shell.execute_reply":"2024-06-09T08:55:22.975681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        \"\"\"decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        for j in range(len(decoded_preds)):\n            train_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n            train_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n    avg_cer = sum(train_cer)/len(train_cer)\n    avg_wer = sum(train_wer)/len(train_wer)\n            \n    print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(train_loss, avg_cer, avg_wer))\"\"\"\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n            \n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n            \n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n   \n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model_for_correction_test.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:54:54.679945Z","iopub.execute_input":"2024-06-09T11:54:54.680292Z","iopub.status.idle":"2024-06-09T11:54:54.709482Z","shell.execute_reply.started":"2024-06-09T11:54:54.680256Z","shell.execute_reply":"2024-06-09T11:54:54.708401Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict(model, file_name, device):\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    sampl = librosa.load(file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    print(spectrogram_tensor.size())\n\n    with torch.no_grad():\n        spectrogram_tensor.to(device)\n        output = model(spectrogram_tensor)\n        print(output.size())\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T11:54:54.712899Z","iopub.execute_input":"2024-06-09T11:54:54.713155Z","iopub.status.idle":"2024-06-09T11:54:54.724034Z","shell.execute_reply.started":"2024-06-09T11:54:54.713130Z","shell.execute_reply":"2024-06-09T11:54:54.723057Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict_with_tensor(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    #sampl = librosa.load(file_name, sr=16000)[0]\n    #sampl = sampl[np.newaxis, :]\n    #sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    input_sequences = decodes[0]\n                \n    task_prefix = \"Spell correct: \"\n\n    if type(input_sequences) != list: input_sequences = [input_sequences]\n    encoded = tokenizer(\n      [task_prefix + sequence for sequence in input_sequences],\n      padding=\"longest\",\n      max_length=MAX_INPUT,\n      truncation=True,\n      return_tensors=\"pt\",\n    )\n\n    predicts = corrector.generate(**encoded.to(needed_device))   # # Прогнозирование\n\n    input_sequences = tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]\n    input_sequences = remove_characters(input_sequences)\n\n    return input_sequences\n\n    #return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.984075Z","iopub.status.idle":"2024-06-09T08:55:22.984737Z","shell.execute_reply.started":"2024-06-09T08:55:22.984397Z","shell.execute_reply":"2024-06-09T08:55:22.984432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import hunspell\nimport os\n\n#пробуем hunspell для исправления ошибок\ndef load_hunspell_russian_dict():\n    dict_path = \"/kaggle/input/dop-test-files\"  \n    \n    ru_dic = os.path.join(dict_path, \"ru_RU_big.dic\")\n    ru_aff = os.path.join(dict_path, \"ru_RU_big.aff\")\n    \n    # Create Hunspell instance for Russian\n    hunspell_instance = hunspell.HunSpell(ru_dic, ru_aff)\n    return hunspell_instance\n\ndef correct_mistakes(text, hunspell_instance):\n    corrected_text = []\n    words = text.split()\n    \n    for word in words:\n        if hunspell_instance.spell(word):\n            corrected_text.append(word)\n        else:\n            suggestions = hunspell_instance.suggest(word)\n            if suggestions:\n                corrected_text.append(suggestions[0])  # Choose the first suggestion\n            else:\n                corrected_text.append(word)  # No suggestion, keep the original word\n    \n    return \" \".join(corrected_text)\n\n# Example usage\nhunspell_instance = load_hunspell_russian_dict()\n#text = \"Привет, как дила?\"\n#corrected_text = correct_mistakes(text, hunspell_instance)\n#print(corrected_text)\n\ndef predict_with_tensor_v2(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    corrected_output = correct_mistakes(decodes[0], hunspell_instance)\n\n    #return decodes[0]\n    return corrected_output","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.986898Z","iopub.status.idle":"2024-06-09T08:55:22.987697Z","shell.execute_reply.started":"2024-06-09T08:55:22.987243Z","shell.execute_reply":"2024-06-09T08:55:22.987282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install safetensors","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.989390Z","iopub.status.idle":"2024-06-09T08:55:22.990124Z","shell.execute_reply.started":"2024-06-09T08:55:22.989719Z","shell.execute_reply":"2024-06-09T08:55:22.989755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.991616Z","iopub.status.idle":"2024-06-09T08:55:22.992276Z","shell.execute_reply.started":"2024-06-09T08:55:22.991947Z","shell.execute_reply":"2024-06-09T08:55:22.991983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.994234Z","iopub.status.idle":"2024-06-09T08:55:22.994933Z","shell.execute_reply.started":"2024-06-09T08:55:22.994550Z","shell.execute_reply":"2024-06-09T08:55:22.994601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntoken = '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'\n\n# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# # Load the model and tokenizer\n# model = AutoModelForSequenceClassification.from_pretrained(\"<your-username>/<your-model-name>\")\n# tokenizer = AutoTokenizer.from_pretrained(\"<your-username>/<your-model-name>\")\n\n# Use the model for inference\n#os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n\nmodel = T5ForConditionalGeneration.from_pretrained('NickChudo/t5_small_fine_tuned_10_epochs_model', \n                                                   use_auth_token=token, \n                                                   from_tf=False)\ntokenizer = T5Tokenizer.from_pretrained('NickChudo/t5_small_fine_tuned_10_epochs_tokenizer', \n                                                   use_auth_token=token,\n                                                   from_tf=False)\n\ndef correct_mistakes(text, model, tokenizer):\n    input_text = \"correct: \" + text\n    inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n    \n    outputs = model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return corrected_text\n\n\n#corrected_text = correct_mistakes(text, model, tokenizer)\n\ndef predict_with_tensor_v3(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    corrected_output = correct_mistakes(decodes[0], model, tokenizer)\n\n    #return decodes[0]\n    return corrected_output","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.997975Z","iopub.status.idle":"2024-06-09T08:55:22.998663Z","shell.execute_reply.started":"2024-06-09T08:55:22.998308Z","shell.execute_reply":"2024-06-09T08:55:22.998347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlearning_rate = 0.003\nbatch_size = 20\nepochs = 200\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-09T11:54:54.725031Z","iopub.execute_input":"2024-06-09T11:54:54.725306Z","iopub.status.idle":"2024-06-09T14:01:56.134913Z","shell.execute_reply.started":"2024-06-09T11:54:54.725280Z","shell.execute_reply":"2024-06-09T14:01:56.133664Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel1(\n  (conv): Sequential(\n    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Conv2d(1, 32, kernel_size=(4, 4), stride=(3, 3), padding=(2, 2))\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): GELU(approximate='none')\n    (4): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): GELU(approximate='none')\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): GELU(approximate='none')\n  )\n  (fc_1): Sequential(\n    (0): Linear(in_features=896, out_features=270, bias=True)\n    (1): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=270, out_features=270, bias=True)\n    (4): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (5): GELU(approximate='none')\n    (6): Linear(in_features=270, out_features=270, bias=True)\n    (7): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (8): GELU(approximate='none')\n  )\n  (BiGRU_1): BidirectionalGRU(\n    (BiGRU): GRU(270, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_2): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_3): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_4): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (fc_2): Sequential(\n    (0): Linear(in_features=540, out_features=35, bias=True)\n  )\n  (softmax): LogSoftmax(dim=2)\n)\nNum Model Parameters 5422923\nTrain Epoch: 1 [0/2800 (0%)]\tLoss: 27.910843\nTrain Epoch: 1 [400/2800 (14%)]\tLoss: 4.121183\nTrain Epoch: 1 [800/2800 (29%)]\tLoss: 3.511927\nTrain Epoch: 1 [1200/2800 (43%)]\tLoss: 3.418364\nTrain Epoch: 1 [1600/2800 (57%)]\tLoss: 3.344627\nTrain Epoch: 1 [2000/2800 (71%)]\tLoss: 3.434308\nTrain Epoch: 1 [2400/2800 (86%)]\tLoss: 3.300121\n\nevaluating...\nTest set:\tAverage loss: 3.2967, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 2 [0/2800 (0%)]\tLoss: 3.307545\nTrain Epoch: 2 [400/2800 (14%)]\tLoss: 3.319965\nTrain Epoch: 2 [800/2800 (29%)]\tLoss: 3.350420\nTrain Epoch: 2 [1200/2800 (43%)]\tLoss: 3.368297\nTrain Epoch: 2 [1600/2800 (57%)]\tLoss: 3.274558\nTrain Epoch: 2 [2000/2800 (71%)]\tLoss: 3.285496\nTrain Epoch: 2 [2400/2800 (86%)]\tLoss: 3.309556\n\nevaluating...\nTest set:\tAverage loss: 3.2863, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 3 [0/2800 (0%)]\tLoss: 3.256809\nTrain Epoch: 3 [400/2800 (14%)]\tLoss: 3.243348\nTrain Epoch: 3 [800/2800 (29%)]\tLoss: 3.201972\nTrain Epoch: 3 [1200/2800 (43%)]\tLoss: 3.259642\nTrain Epoch: 3 [1600/2800 (57%)]\tLoss: 3.193727\nTrain Epoch: 3 [2000/2800 (71%)]\tLoss: 3.094163\nTrain Epoch: 3 [2400/2800 (86%)]\tLoss: 2.970524\n\nevaluating...\nTest set:\tAverage loss: 2.9265, Average CER: 0.999856 Average WER: 1.0014\n\nTrain Epoch: 4 [0/2800 (0%)]\tLoss: 2.958387\nTrain Epoch: 4 [400/2800 (14%)]\tLoss: 2.702619\nTrain Epoch: 4 [800/2800 (29%)]\tLoss: 2.681760\nTrain Epoch: 4 [1200/2800 (43%)]\tLoss: 2.693986\nTrain Epoch: 4 [1600/2800 (57%)]\tLoss: 2.619524\nTrain Epoch: 4 [2000/2800 (71%)]\tLoss: 2.472588\nTrain Epoch: 4 [2400/2800 (86%)]\tLoss: 2.290076\n\nevaluating...\nTest set:\tAverage loss: 2.2405, Average CER: 0.849613 Average WER: 1.0289\n\nTrain Epoch: 5 [0/2800 (0%)]\tLoss: 2.242229\nTrain Epoch: 5 [400/2800 (14%)]\tLoss: 2.027115\nTrain Epoch: 5 [800/2800 (29%)]\tLoss: 2.084886\nTrain Epoch: 5 [1200/2800 (43%)]\tLoss: 2.101840\nTrain Epoch: 5 [1600/2800 (57%)]\tLoss: 2.161685\nTrain Epoch: 5 [2000/2800 (71%)]\tLoss: 1.617534\nTrain Epoch: 5 [2400/2800 (86%)]\tLoss: 1.733261\n\nevaluating...\nTest set:\tAverage loss: 1.6264, Average CER: 0.506767 Average WER: 0.9890\n\nTrain Epoch: 6 [0/2800 (0%)]\tLoss: 1.587919\nTrain Epoch: 6 [400/2800 (14%)]\tLoss: 1.749314\nTrain Epoch: 6 [800/2800 (29%)]\tLoss: 1.580745\nTrain Epoch: 6 [1200/2800 (43%)]\tLoss: 1.427251\nTrain Epoch: 6 [1600/2800 (57%)]\tLoss: 1.449893\nTrain Epoch: 6 [2000/2800 (71%)]\tLoss: 1.532128\nTrain Epoch: 6 [2400/2800 (86%)]\tLoss: 1.593677\n\nevaluating...\nTest set:\tAverage loss: 1.3809, Average CER: 0.412439 Average WER: 0.9727\n\nTrain Epoch: 7 [0/2800 (0%)]\tLoss: 1.411031\nTrain Epoch: 7 [400/2800 (14%)]\tLoss: 1.199146\nTrain Epoch: 7 [800/2800 (29%)]\tLoss: 1.361123\nTrain Epoch: 7 [1200/2800 (43%)]\tLoss: 1.000848\nTrain Epoch: 7 [1600/2800 (57%)]\tLoss: 1.076598\nTrain Epoch: 7 [2000/2800 (71%)]\tLoss: 1.234227\nTrain Epoch: 7 [2400/2800 (86%)]\tLoss: 1.142360\n\nevaluating...\nTest set:\tAverage loss: 1.2428, Average CER: 0.372059 Average WER: 0.9496\n\nTrain Epoch: 8 [0/2800 (0%)]\tLoss: 1.154770\nTrain Epoch: 8 [400/2800 (14%)]\tLoss: 1.238473\nTrain Epoch: 8 [800/2800 (29%)]\tLoss: 0.939736\nTrain Epoch: 8 [1200/2800 (43%)]\tLoss: 0.989664\nTrain Epoch: 8 [1600/2800 (57%)]\tLoss: 0.990988\nTrain Epoch: 8 [2000/2800 (71%)]\tLoss: 0.962381\nTrain Epoch: 8 [2400/2800 (86%)]\tLoss: 1.076973\n\nevaluating...\nTest set:\tAverage loss: 1.1565, Average CER: 0.349592 Average WER: 0.9267\n\nTrain Epoch: 9 [0/2800 (0%)]\tLoss: 0.896408\nTrain Epoch: 9 [400/2800 (14%)]\tLoss: 1.021278\nTrain Epoch: 9 [800/2800 (29%)]\tLoss: 1.201550\nTrain Epoch: 9 [1200/2800 (43%)]\tLoss: 0.941824\nTrain Epoch: 9 [1600/2800 (57%)]\tLoss: 0.855069\nTrain Epoch: 9 [2000/2800 (71%)]\tLoss: 0.891299\nTrain Epoch: 9 [2400/2800 (86%)]\tLoss: 0.775455\n\nevaluating...\nTest set:\tAverage loss: 1.1353, Average CER: 0.334722 Average WER: 0.9604\n\nTrain Epoch: 10 [0/2800 (0%)]\tLoss: 0.970085\nTrain Epoch: 10 [400/2800 (14%)]\tLoss: 0.638175\nTrain Epoch: 10 [800/2800 (29%)]\tLoss: 0.781367\nTrain Epoch: 10 [1200/2800 (43%)]\tLoss: 0.896426\nTrain Epoch: 10 [1600/2800 (57%)]\tLoss: 0.607704\nTrain Epoch: 10 [2000/2800 (71%)]\tLoss: 0.786255\nTrain Epoch: 10 [2400/2800 (86%)]\tLoss: 0.952784\n\nevaluating...\nTest set:\tAverage loss: 1.0753, Average CER: 0.314313 Average WER: 0.9457\n\nTrain Epoch: 11 [0/2800 (0%)]\tLoss: 0.723373\nTrain Epoch: 11 [400/2800 (14%)]\tLoss: 0.516484\nTrain Epoch: 11 [800/2800 (29%)]\tLoss: 0.606568\nTrain Epoch: 11 [1200/2800 (43%)]\tLoss: 0.805311\nTrain Epoch: 11 [1600/2800 (57%)]\tLoss: 0.718042\nTrain Epoch: 11 [2000/2800 (71%)]\tLoss: 0.528872\nTrain Epoch: 11 [2400/2800 (86%)]\tLoss: 0.636518\n\nevaluating...\nTest set:\tAverage loss: 1.0156, Average CER: 0.308100 Average WER: 0.8919\n\nTrain Epoch: 12 [0/2800 (0%)]\tLoss: 0.586224\nTrain Epoch: 12 [400/2800 (14%)]\tLoss: 0.613934\nTrain Epoch: 12 [800/2800 (29%)]\tLoss: 0.674928\nTrain Epoch: 12 [1200/2800 (43%)]\tLoss: 0.572566\nTrain Epoch: 12 [1600/2800 (57%)]\tLoss: 0.638698\nTrain Epoch: 12 [2000/2800 (71%)]\tLoss: 0.678764\nTrain Epoch: 12 [2400/2800 (86%)]\tLoss: 0.915978\n\nevaluating...\nTest set:\tAverage loss: 0.9716, Average CER: 0.285929 Average WER: 0.8568\n\nTrain Epoch: 13 [0/2800 (0%)]\tLoss: 0.423287\nTrain Epoch: 13 [400/2800 (14%)]\tLoss: 0.468022\nTrain Epoch: 13 [800/2800 (29%)]\tLoss: 0.469222\nTrain Epoch: 13 [1200/2800 (43%)]\tLoss: 0.611487\nTrain Epoch: 13 [1600/2800 (57%)]\tLoss: 0.651585\nTrain Epoch: 13 [2000/2800 (71%)]\tLoss: 0.670693\nTrain Epoch: 13 [2400/2800 (86%)]\tLoss: 0.671215\n\nevaluating...\nTest set:\tAverage loss: 1.0364, Average CER: 0.293283 Average WER: 0.8650\n\nTrain Epoch: 14 [0/2800 (0%)]\tLoss: 0.480275\nTrain Epoch: 14 [400/2800 (14%)]\tLoss: 0.461198\nTrain Epoch: 14 [800/2800 (29%)]\tLoss: 0.452056\nTrain Epoch: 14 [1200/2800 (43%)]\tLoss: 0.576328\nTrain Epoch: 14 [1600/2800 (57%)]\tLoss: 0.558373\nTrain Epoch: 14 [2000/2800 (71%)]\tLoss: 0.645211\nTrain Epoch: 14 [2400/2800 (86%)]\tLoss: 0.554480\n\nevaluating...\nTest set:\tAverage loss: 0.9920, Average CER: 0.273613 Average WER: 0.8441\n\nTrain Epoch: 15 [0/2800 (0%)]\tLoss: 0.412230\nTrain Epoch: 15 [400/2800 (14%)]\tLoss: 0.498316\nTrain Epoch: 15 [800/2800 (29%)]\tLoss: 0.443605\nTrain Epoch: 15 [1200/2800 (43%)]\tLoss: 0.528146\nTrain Epoch: 15 [1600/2800 (57%)]\tLoss: 0.593969\nTrain Epoch: 15 [2000/2800 (71%)]\tLoss: 0.432388\nTrain Epoch: 15 [2400/2800 (86%)]\tLoss: 0.495009\n\nevaluating...\nTest set:\tAverage loss: 0.9993, Average CER: 0.283444 Average WER: 0.8697\n\nTrain Epoch: 16 [0/2800 (0%)]\tLoss: 0.390544\nTrain Epoch: 16 [400/2800 (14%)]\tLoss: 0.342431\nTrain Epoch: 16 [800/2800 (29%)]\tLoss: 0.343995\nTrain Epoch: 16 [1200/2800 (43%)]\tLoss: 0.396526\nTrain Epoch: 16 [1600/2800 (57%)]\tLoss: 0.530309\nTrain Epoch: 16 [2000/2800 (71%)]\tLoss: 0.471101\nTrain Epoch: 16 [2400/2800 (86%)]\tLoss: 0.415944\n\nevaluating...\nTest set:\tAverage loss: 1.0505, Average CER: 0.285075 Average WER: 0.8516\n\nTrain Epoch: 17 [0/2800 (0%)]\tLoss: 0.393678\nTrain Epoch: 17 [400/2800 (14%)]\tLoss: 0.397787\nTrain Epoch: 17 [800/2800 (29%)]\tLoss: 0.351495\nTrain Epoch: 17 [1200/2800 (43%)]\tLoss: 0.337985\nTrain Epoch: 17 [1600/2800 (57%)]\tLoss: 0.392681\nTrain Epoch: 17 [2000/2800 (71%)]\tLoss: 0.384322\nTrain Epoch: 17 [2400/2800 (86%)]\tLoss: 0.342283\n\nevaluating...\nTest set:\tAverage loss: 1.0179, Average CER: 0.269862 Average WER: 0.8358\n\nTrain Epoch: 18 [0/2800 (0%)]\tLoss: 0.384488\nTrain Epoch: 18 [400/2800 (14%)]\tLoss: 0.392900\nTrain Epoch: 18 [800/2800 (29%)]\tLoss: 0.489611\nTrain Epoch: 18 [1200/2800 (43%)]\tLoss: 0.285862\nTrain Epoch: 18 [1600/2800 (57%)]\tLoss: 0.511806\nTrain Epoch: 18 [2000/2800 (71%)]\tLoss: 0.331233\nTrain Epoch: 18 [2400/2800 (86%)]\tLoss: 0.423588\n\nevaluating...\nTest set:\tAverage loss: 1.0225, Average CER: 0.274531 Average WER: 0.8375\n\nTrain Epoch: 19 [0/2800 (0%)]\tLoss: 0.254073\nTrain Epoch: 19 [400/2800 (14%)]\tLoss: 0.303267\nTrain Epoch: 19 [800/2800 (29%)]\tLoss: 0.283702\nTrain Epoch: 19 [1200/2800 (43%)]\tLoss: 0.383376\nTrain Epoch: 19 [1600/2800 (57%)]\tLoss: 0.392188\nTrain Epoch: 19 [2000/2800 (71%)]\tLoss: 0.384343\nTrain Epoch: 19 [2400/2800 (86%)]\tLoss: 0.519363\n\nevaluating...\nTest set:\tAverage loss: 1.0324, Average CER: 0.271763 Average WER: 0.8523\n\nTrain Epoch: 20 [0/2800 (0%)]\tLoss: 0.288084\nTrain Epoch: 20 [400/2800 (14%)]\tLoss: 0.358225\nTrain Epoch: 20 [800/2800 (29%)]\tLoss: 0.371667\nTrain Epoch: 20 [1200/2800 (43%)]\tLoss: 0.393712\nTrain Epoch: 20 [1600/2800 (57%)]\tLoss: 0.365678\nTrain Epoch: 20 [2000/2800 (71%)]\tLoss: 0.371267\nTrain Epoch: 20 [2400/2800 (86%)]\tLoss: 0.536139\n\nevaluating...\nTest set:\tAverage loss: 1.0540, Average CER: 0.282685 Average WER: 0.8478\n\nTrain Epoch: 21 [0/2800 (0%)]\tLoss: 0.331657\nTrain Epoch: 21 [400/2800 (14%)]\tLoss: 0.212441\nTrain Epoch: 21 [800/2800 (29%)]\tLoss: 0.360431\nTrain Epoch: 21 [1200/2800 (43%)]\tLoss: 0.261218\nTrain Epoch: 21 [1600/2800 (57%)]\tLoss: 0.237339\nTrain Epoch: 21 [2000/2800 (71%)]\tLoss: 0.356035\nTrain Epoch: 21 [2400/2800 (86%)]\tLoss: 0.431388\n\nevaluating...\nTest set:\tAverage loss: 1.0967, Average CER: 0.272556 Average WER: 0.8394\n\nTrain Epoch: 22 [0/2800 (0%)]\tLoss: 0.296097\nTrain Epoch: 22 [400/2800 (14%)]\tLoss: 0.379868\nTrain Epoch: 22 [800/2800 (29%)]\tLoss: 0.409200\nTrain Epoch: 22 [1200/2800 (43%)]\tLoss: 0.276078\nTrain Epoch: 22 [1600/2800 (57%)]\tLoss: 0.219433\nTrain Epoch: 22 [2000/2800 (71%)]\tLoss: 0.313056\nTrain Epoch: 22 [2400/2800 (86%)]\tLoss: 0.441262\n\nevaluating...\nTest set:\tAverage loss: 1.1265, Average CER: 0.276945 Average WER: 0.8488\n\nTrain Epoch: 23 [0/2800 (0%)]\tLoss: 0.309094\nTrain Epoch: 23 [400/2800 (14%)]\tLoss: 0.234395\nTrain Epoch: 23 [800/2800 (29%)]\tLoss: 0.253440\nTrain Epoch: 23 [1200/2800 (43%)]\tLoss: 0.560498\nTrain Epoch: 23 [1600/2800 (57%)]\tLoss: 0.319822\nTrain Epoch: 23 [2000/2800 (71%)]\tLoss: 0.209280\nTrain Epoch: 23 [2400/2800 (86%)]\tLoss: 0.456954\n\nevaluating...\nTest set:\tAverage loss: 1.1325, Average CER: 0.285351 Average WER: 0.8375\n\nTrain Epoch: 24 [0/2800 (0%)]\tLoss: 0.246802\nTrain Epoch: 24 [400/2800 (14%)]\tLoss: 0.170776\nTrain Epoch: 24 [800/2800 (29%)]\tLoss: 0.177553\nTrain Epoch: 24 [1200/2800 (43%)]\tLoss: 0.233810\nTrain Epoch: 24 [1600/2800 (57%)]\tLoss: 0.252071\nTrain Epoch: 24 [2000/2800 (71%)]\tLoss: 0.250540\nTrain Epoch: 24 [2400/2800 (86%)]\tLoss: 0.320939\n\nevaluating...\nTest set:\tAverage loss: 1.1248, Average CER: 0.268461 Average WER: 0.8670\n\nTrain Epoch: 25 [0/2800 (0%)]\tLoss: 0.224975\nTrain Epoch: 25 [400/2800 (14%)]\tLoss: 0.283352\nTrain Epoch: 25 [800/2800 (29%)]\tLoss: 0.204341\nTrain Epoch: 25 [1200/2800 (43%)]\tLoss: 0.234690\nTrain Epoch: 25 [1600/2800 (57%)]\tLoss: 0.237822\nTrain Epoch: 25 [2000/2800 (71%)]\tLoss: 0.240974\nTrain Epoch: 25 [2400/2800 (86%)]\tLoss: 0.220728\n\nevaluating...\nTest set:\tAverage loss: 1.1512, Average CER: 0.269710 Average WER: 0.8127\n\nTrain Epoch: 26 [0/2800 (0%)]\tLoss: 0.126889\nTrain Epoch: 26 [400/2800 (14%)]\tLoss: 0.325916\nTrain Epoch: 26 [800/2800 (29%)]\tLoss: 0.241989\nTrain Epoch: 26 [1200/2800 (43%)]\tLoss: 0.186339\nTrain Epoch: 26 [1600/2800 (57%)]\tLoss: 0.295666\nTrain Epoch: 26 [2000/2800 (71%)]\tLoss: 0.262172\nTrain Epoch: 26 [2400/2800 (86%)]\tLoss: 0.350328\n\nevaluating...\nTest set:\tAverage loss: 1.1766, Average CER: 0.271739 Average WER: 0.8316\n\nTrain Epoch: 27 [0/2800 (0%)]\tLoss: 0.222785\nTrain Epoch: 27 [400/2800 (14%)]\tLoss: 0.231845\nTrain Epoch: 27 [800/2800 (29%)]\tLoss: 0.247358\nTrain Epoch: 27 [1200/2800 (43%)]\tLoss: 0.174357\nTrain Epoch: 27 [1600/2800 (57%)]\tLoss: 0.211054\nTrain Epoch: 27 [2000/2800 (71%)]\tLoss: 0.226846\nTrain Epoch: 27 [2400/2800 (86%)]\tLoss: 0.356406\n\nevaluating...\nTest set:\tAverage loss: 1.1351, Average CER: 0.267386 Average WER: 0.8190\n\nTrain Epoch: 28 [0/2800 (0%)]\tLoss: 0.329889\nTrain Epoch: 28 [400/2800 (14%)]\tLoss: 0.306514\nTrain Epoch: 28 [800/2800 (29%)]\tLoss: 0.273558\nTrain Epoch: 28 [1200/2800 (43%)]\tLoss: 0.263076\nTrain Epoch: 28 [1600/2800 (57%)]\tLoss: 0.171766\nTrain Epoch: 28 [2000/2800 (71%)]\tLoss: 0.260560\nTrain Epoch: 28 [2400/2800 (86%)]\tLoss: 0.389344\n\nevaluating...\nTest set:\tAverage loss: 1.1289, Average CER: 0.266509 Average WER: 0.8017\n\nTrain Epoch: 29 [0/2800 (0%)]\tLoss: 0.232017\nTrain Epoch: 29 [400/2800 (14%)]\tLoss: 0.280057\nTrain Epoch: 29 [800/2800 (29%)]\tLoss: 0.380173\nTrain Epoch: 29 [1200/2800 (43%)]\tLoss: 0.228902\nTrain Epoch: 29 [1600/2800 (57%)]\tLoss: 0.308176\nTrain Epoch: 29 [2000/2800 (71%)]\tLoss: 0.315374\nTrain Epoch: 29 [2400/2800 (86%)]\tLoss: 0.430262\n\nevaluating...\nTest set:\tAverage loss: 1.1941, Average CER: 0.280593 Average WER: 0.8320\n\nTrain Epoch: 30 [0/2800 (0%)]\tLoss: 0.323471\nTrain Epoch: 30 [400/2800 (14%)]\tLoss: 0.214524\nTrain Epoch: 30 [800/2800 (29%)]\tLoss: 0.236361\nTrain Epoch: 30 [1200/2800 (43%)]\tLoss: 0.281276\nTrain Epoch: 30 [1600/2800 (57%)]\tLoss: 0.266582\nTrain Epoch: 30 [2000/2800 (71%)]\tLoss: 0.238326\nTrain Epoch: 30 [2400/2800 (86%)]\tLoss: 0.254262\n\nevaluating...\nTest set:\tAverage loss: 1.1437, Average CER: 0.265151 Average WER: 0.8111\n\nTrain Epoch: 31 [0/2800 (0%)]\tLoss: 0.224558\nTrain Epoch: 31 [400/2800 (14%)]\tLoss: 0.236210\nTrain Epoch: 31 [800/2800 (29%)]\tLoss: 0.146986\nTrain Epoch: 31 [1200/2800 (43%)]\tLoss: 0.159818\nTrain Epoch: 31 [1600/2800 (57%)]\tLoss: 0.230275\nTrain Epoch: 31 [2000/2800 (71%)]\tLoss: 0.421094\nTrain Epoch: 31 [2400/2800 (86%)]\tLoss: 0.262956\n\nevaluating...\nTest set:\tAverage loss: 1.1226, Average CER: 0.261987 Average WER: 0.8074\n\nTrain Epoch: 32 [0/2800 (0%)]\tLoss: 0.270560\nTrain Epoch: 32 [400/2800 (14%)]\tLoss: 0.140799\nTrain Epoch: 32 [800/2800 (29%)]\tLoss: 0.193405\nTrain Epoch: 32 [1200/2800 (43%)]\tLoss: 0.211880\nTrain Epoch: 32 [1600/2800 (57%)]\tLoss: 0.206002\nTrain Epoch: 32 [2000/2800 (71%)]\tLoss: 0.177794\nTrain Epoch: 32 [2400/2800 (86%)]\tLoss: 0.198301\n\nevaluating...\nTest set:\tAverage loss: 1.2104, Average CER: 0.273342 Average WER: 0.8242\n\nTrain Epoch: 33 [0/2800 (0%)]\tLoss: 0.313525\nTrain Epoch: 33 [400/2800 (14%)]\tLoss: 0.151993\nTrain Epoch: 33 [800/2800 (29%)]\tLoss: 0.258950\nTrain Epoch: 33 [1200/2800 (43%)]\tLoss: 0.196784\nTrain Epoch: 33 [1600/2800 (57%)]\tLoss: 0.273305\nTrain Epoch: 33 [2000/2800 (71%)]\tLoss: 0.380173\nTrain Epoch: 33 [2400/2800 (86%)]\tLoss: 0.231192\n\nevaluating...\nTest set:\tAverage loss: 1.1639, Average CER: 0.258303 Average WER: 0.8182\n\nTrain Epoch: 34 [0/2800 (0%)]\tLoss: 0.304895\nTrain Epoch: 34 [400/2800 (14%)]\tLoss: 0.356104\nTrain Epoch: 34 [800/2800 (29%)]\tLoss: 0.238066\nTrain Epoch: 34 [1200/2800 (43%)]\tLoss: 0.428585\nTrain Epoch: 34 [1600/2800 (57%)]\tLoss: 0.240241\nTrain Epoch: 34 [2000/2800 (71%)]\tLoss: 0.341821\nTrain Epoch: 34 [2400/2800 (86%)]\tLoss: 0.234944\n\nevaluating...\nTest set:\tAverage loss: 1.1690, Average CER: 0.259090 Average WER: 0.7928\n\nTrain Epoch: 35 [0/2800 (0%)]\tLoss: 0.295017\nTrain Epoch: 35 [400/2800 (14%)]\tLoss: 0.283526\nTrain Epoch: 35 [800/2800 (29%)]\tLoss: 0.223203\nTrain Epoch: 35 [1200/2800 (43%)]\tLoss: 0.240271\nTrain Epoch: 35 [1600/2800 (57%)]\tLoss: 0.394577\nTrain Epoch: 35 [2000/2800 (71%)]\tLoss: 0.305308\nTrain Epoch: 35 [2400/2800 (86%)]\tLoss: 0.423718\n\nevaluating...\nTest set:\tAverage loss: 1.1804, Average CER: 0.269495 Average WER: 0.8395\n\nTrain Epoch: 36 [0/2800 (0%)]\tLoss: 0.356575\nTrain Epoch: 36 [400/2800 (14%)]\tLoss: 0.123056\nTrain Epoch: 36 [800/2800 (29%)]\tLoss: 0.304658\nTrain Epoch: 36 [1200/2800 (43%)]\tLoss: 0.242821\nTrain Epoch: 36 [1600/2800 (57%)]\tLoss: 0.226919\nTrain Epoch: 36 [2000/2800 (71%)]\tLoss: 0.283631\nTrain Epoch: 36 [2400/2800 (86%)]\tLoss: 0.377949\n\nevaluating...\nTest set:\tAverage loss: 1.2181, Average CER: 0.262096 Average WER: 0.8022\n\nTrain Epoch: 37 [0/2800 (0%)]\tLoss: 0.287048\nTrain Epoch: 37 [400/2800 (14%)]\tLoss: 0.193537\nTrain Epoch: 37 [800/2800 (29%)]\tLoss: 0.324222\nTrain Epoch: 37 [1200/2800 (43%)]\tLoss: 0.232280\nTrain Epoch: 37 [1600/2800 (57%)]\tLoss: 0.338450\nTrain Epoch: 37 [2000/2800 (71%)]\tLoss: 0.391161\nTrain Epoch: 37 [2400/2800 (86%)]\tLoss: 0.140328\n\nevaluating...\nTest set:\tAverage loss: 1.0989, Average CER: 0.238416 Average WER: 0.7521\n\nTrain Epoch: 38 [0/2800 (0%)]\tLoss: 0.168965\nTrain Epoch: 38 [400/2800 (14%)]\tLoss: 0.168662\nTrain Epoch: 38 [800/2800 (29%)]\tLoss: 0.110482\nTrain Epoch: 38 [1200/2800 (43%)]\tLoss: 0.173412\nTrain Epoch: 38 [1600/2800 (57%)]\tLoss: 0.093465\nTrain Epoch: 38 [2000/2800 (71%)]\tLoss: 0.183706\nTrain Epoch: 38 [2400/2800 (86%)]\tLoss: 0.175478\n\nevaluating...\nTest set:\tAverage loss: 1.2013, Average CER: 0.259647 Average WER: 0.8030\n\nTrain Epoch: 39 [0/2800 (0%)]\tLoss: 0.318979\nTrain Epoch: 39 [400/2800 (14%)]\tLoss: 0.223314\nTrain Epoch: 39 [800/2800 (29%)]\tLoss: 0.311405\nTrain Epoch: 39 [1200/2800 (43%)]\tLoss: 0.174403\nTrain Epoch: 39 [1600/2800 (57%)]\tLoss: 0.272084\nTrain Epoch: 39 [2000/2800 (71%)]\tLoss: 0.262076\nTrain Epoch: 39 [2400/2800 (86%)]\tLoss: 0.380398\n\nevaluating...\nTest set:\tAverage loss: 1.1033, Average CER: 0.253440 Average WER: 0.7884\n\nTrain Epoch: 40 [0/2800 (0%)]\tLoss: 0.222107\nTrain Epoch: 40 [400/2800 (14%)]\tLoss: 0.234012\nTrain Epoch: 40 [800/2800 (29%)]\tLoss: 0.331973\nTrain Epoch: 40 [1200/2800 (43%)]\tLoss: 0.294042\nTrain Epoch: 40 [1600/2800 (57%)]\tLoss: 0.245060\nTrain Epoch: 40 [2000/2800 (71%)]\tLoss: 0.343873\nTrain Epoch: 40 [2400/2800 (86%)]\tLoss: 0.273168\n\nevaluating...\nTest set:\tAverage loss: 1.1979, Average CER: 0.268001 Average WER: 0.8350\n\nTrain Epoch: 41 [0/2800 (0%)]\tLoss: 0.169957\nTrain Epoch: 41 [400/2800 (14%)]\tLoss: 0.180591\nTrain Epoch: 41 [800/2800 (29%)]\tLoss: 0.295697\nTrain Epoch: 41 [1200/2800 (43%)]\tLoss: 0.286102\nTrain Epoch: 41 [1600/2800 (57%)]\tLoss: 0.211692\nTrain Epoch: 41 [2000/2800 (71%)]\tLoss: 0.261537\nTrain Epoch: 41 [2400/2800 (86%)]\tLoss: 0.431743\n\nevaluating...\nTest set:\tAverage loss: 1.2189, Average CER: 0.269366 Average WER: 0.8117\n\nTrain Epoch: 42 [0/2800 (0%)]\tLoss: 0.213812\nTrain Epoch: 42 [400/2800 (14%)]\tLoss: 0.266600\nTrain Epoch: 42 [800/2800 (29%)]\tLoss: 0.228501\nTrain Epoch: 42 [1200/2800 (43%)]\tLoss: 0.331767\nTrain Epoch: 42 [1600/2800 (57%)]\tLoss: 0.307992\nTrain Epoch: 42 [2000/2800 (71%)]\tLoss: 0.389069\nTrain Epoch: 42 [2400/2800 (86%)]\tLoss: 0.395115\n\nevaluating...\nTest set:\tAverage loss: 1.1822, Average CER: 0.265786 Average WER: 0.8035\n\nTrain Epoch: 43 [0/2800 (0%)]\tLoss: 0.242815\nTrain Epoch: 43 [400/2800 (14%)]\tLoss: 0.228635\nTrain Epoch: 43 [800/2800 (29%)]\tLoss: 0.252395\nTrain Epoch: 43 [1200/2800 (43%)]\tLoss: 0.319466\nTrain Epoch: 43 [1600/2800 (57%)]\tLoss: 0.336789\nTrain Epoch: 43 [2000/2800 (71%)]\tLoss: 0.408466\nTrain Epoch: 43 [2400/2800 (86%)]\tLoss: 0.316449\n\nevaluating...\nTest set:\tAverage loss: 1.1413, Average CER: 0.260573 Average WER: 0.8003\n\nTrain Epoch: 44 [0/2800 (0%)]\tLoss: 0.293282\nTrain Epoch: 44 [400/2800 (14%)]\tLoss: 0.331848\nTrain Epoch: 44 [800/2800 (29%)]\tLoss: 0.390764\nTrain Epoch: 44 [1200/2800 (43%)]\tLoss: 0.317897\nTrain Epoch: 44 [1600/2800 (57%)]\tLoss: 0.199222\nTrain Epoch: 44 [2000/2800 (71%)]\tLoss: 0.279742\nTrain Epoch: 44 [2400/2800 (86%)]\tLoss: 0.126971\n\nevaluating...\nTest set:\tAverage loss: 1.1722, Average CER: 0.261411 Average WER: 0.8118\n\nTrain Epoch: 45 [0/2800 (0%)]\tLoss: 0.278962\nTrain Epoch: 45 [400/2800 (14%)]\tLoss: 0.250287\nTrain Epoch: 45 [800/2800 (29%)]\tLoss: 0.218305\nTrain Epoch: 45 [1200/2800 (43%)]\tLoss: 0.208105\nTrain Epoch: 45 [1600/2800 (57%)]\tLoss: 0.181753\nTrain Epoch: 45 [2000/2800 (71%)]\tLoss: 0.183642\nTrain Epoch: 45 [2400/2800 (86%)]\tLoss: 0.429652\n\nevaluating...\nTest set:\tAverage loss: 1.1244, Average CER: 0.244132 Average WER: 0.7791\n\nTrain Epoch: 46 [0/2800 (0%)]\tLoss: 0.127181\nTrain Epoch: 46 [400/2800 (14%)]\tLoss: 0.198497\nTrain Epoch: 46 [800/2800 (29%)]\tLoss: 0.174538\nTrain Epoch: 46 [1200/2800 (43%)]\tLoss: 0.158180\nTrain Epoch: 46 [1600/2800 (57%)]\tLoss: 0.229951\nTrain Epoch: 46 [2000/2800 (71%)]\tLoss: 0.280124\nTrain Epoch: 46 [2400/2800 (86%)]\tLoss: 0.223229\n\nevaluating...\nTest set:\tAverage loss: 1.2896, Average CER: 0.272090 Average WER: 0.8324\n\nTrain Epoch: 47 [0/2800 (0%)]\tLoss: 0.139854\nTrain Epoch: 47 [400/2800 (14%)]\tLoss: 0.222959\nTrain Epoch: 47 [800/2800 (29%)]\tLoss: 0.369854\nTrain Epoch: 47 [1200/2800 (43%)]\tLoss: 0.377461\nTrain Epoch: 47 [1600/2800 (57%)]\tLoss: 0.414194\nTrain Epoch: 47 [2000/2800 (71%)]\tLoss: 0.270438\nTrain Epoch: 47 [2400/2800 (86%)]\tLoss: 0.349520\n\nevaluating...\nTest set:\tAverage loss: 1.1750, Average CER: 0.258317 Average WER: 0.8104\n\nTrain Epoch: 48 [0/2800 (0%)]\tLoss: 0.254177\nTrain Epoch: 48 [400/2800 (14%)]\tLoss: 0.380346\nTrain Epoch: 48 [800/2800 (29%)]\tLoss: 0.179827\nTrain Epoch: 48 [1200/2800 (43%)]\tLoss: 0.279035\nTrain Epoch: 48 [1600/2800 (57%)]\tLoss: 0.501258\nTrain Epoch: 48 [2000/2800 (71%)]\tLoss: 0.400352\nTrain Epoch: 48 [2400/2800 (86%)]\tLoss: 0.357056\n\nevaluating...\nTest set:\tAverage loss: 1.1682, Average CER: 0.256079 Average WER: 0.8100\n\nTrain Epoch: 49 [0/2800 (0%)]\tLoss: 0.228174\nTrain Epoch: 49 [400/2800 (14%)]\tLoss: 0.216331\nTrain Epoch: 49 [800/2800 (29%)]\tLoss: 0.224592\nTrain Epoch: 49 [1200/2800 (43%)]\tLoss: 0.211777\nTrain Epoch: 49 [1600/2800 (57%)]\tLoss: 0.428699\nTrain Epoch: 49 [2000/2800 (71%)]\tLoss: 0.466723\nTrain Epoch: 49 [2400/2800 (86%)]\tLoss: 0.330122\n\nevaluating...\nTest set:\tAverage loss: 1.2372, Average CER: 0.276166 Average WER: 0.8259\n\nTrain Epoch: 50 [0/2800 (0%)]\tLoss: 0.286049\nTrain Epoch: 50 [400/2800 (14%)]\tLoss: 0.454238\nTrain Epoch: 50 [800/2800 (29%)]\tLoss: 0.433519\nTrain Epoch: 50 [1200/2800 (43%)]\tLoss: 0.473735\nTrain Epoch: 50 [1600/2800 (57%)]\tLoss: 0.288294\nTrain Epoch: 50 [2000/2800 (71%)]\tLoss: 0.322458\nTrain Epoch: 50 [2400/2800 (86%)]\tLoss: 0.319078\n\nevaluating...\nTest set:\tAverage loss: 1.1881, Average CER: 0.259748 Average WER: 0.8154\n\nTrain Epoch: 51 [0/2800 (0%)]\tLoss: 0.292000\nTrain Epoch: 51 [400/2800 (14%)]\tLoss: 0.211314\nTrain Epoch: 51 [800/2800 (29%)]\tLoss: 0.200470\nTrain Epoch: 51 [1200/2800 (43%)]\tLoss: 0.170024\nTrain Epoch: 51 [1600/2800 (57%)]\tLoss: 0.379989\nTrain Epoch: 51 [2000/2800 (71%)]\tLoss: 0.239171\nTrain Epoch: 51 [2400/2800 (86%)]\tLoss: 0.393801\n\nevaluating...\nTest set:\tAverage loss: 1.2039, Average CER: 0.255471 Average WER: 0.8295\n\nTrain Epoch: 52 [0/2800 (0%)]\tLoss: 0.117312\nTrain Epoch: 52 [400/2800 (14%)]\tLoss: 0.280610\nTrain Epoch: 52 [800/2800 (29%)]\tLoss: 0.245110\nTrain Epoch: 52 [1200/2800 (43%)]\tLoss: 0.154047\nTrain Epoch: 52 [1600/2800 (57%)]\tLoss: 0.269268\nTrain Epoch: 52 [2000/2800 (71%)]\tLoss: 0.234628\nTrain Epoch: 52 [2400/2800 (86%)]\tLoss: 0.437004\n\nevaluating...\nTest set:\tAverage loss: 1.1748, Average CER: 0.262978 Average WER: 0.7978\n\nTrain Epoch: 53 [0/2800 (0%)]\tLoss: 0.239088\nTrain Epoch: 53 [400/2800 (14%)]\tLoss: 0.319387\nTrain Epoch: 53 [800/2800 (29%)]\tLoss: 0.218050\nTrain Epoch: 53 [1200/2800 (43%)]\tLoss: 0.226135\nTrain Epoch: 53 [1600/2800 (57%)]\tLoss: 0.491152\nTrain Epoch: 53 [2000/2800 (71%)]\tLoss: 0.337886\nTrain Epoch: 53 [2400/2800 (86%)]\tLoss: 0.465548\n\nevaluating...\nTest set:\tAverage loss: 1.1809, Average CER: 0.268824 Average WER: 0.8033\n\nTrain Epoch: 54 [0/2800 (0%)]\tLoss: 0.247739\nTrain Epoch: 54 [400/2800 (14%)]\tLoss: 0.395856\nTrain Epoch: 54 [800/2800 (29%)]\tLoss: 0.328908\nTrain Epoch: 54 [1200/2800 (43%)]\tLoss: 0.272178\nTrain Epoch: 54 [1600/2800 (57%)]\tLoss: 0.363490\nTrain Epoch: 54 [2000/2800 (71%)]\tLoss: 0.350919\nTrain Epoch: 54 [2400/2800 (86%)]\tLoss: 0.352816\n\nevaluating...\nTest set:\tAverage loss: 1.1594, Average CER: 0.265244 Average WER: 0.8169\n\nTrain Epoch: 55 [0/2800 (0%)]\tLoss: 0.217153\nTrain Epoch: 55 [400/2800 (14%)]\tLoss: 0.231105\nTrain Epoch: 55 [800/2800 (29%)]\tLoss: 0.313632\nTrain Epoch: 55 [1200/2800 (43%)]\tLoss: 0.335196\nTrain Epoch: 55 [1600/2800 (57%)]\tLoss: 0.534288\nTrain Epoch: 55 [2000/2800 (71%)]\tLoss: 0.418416\nTrain Epoch: 55 [2400/2800 (86%)]\tLoss: 0.634670\n\nevaluating...\nTest set:\tAverage loss: 1.1962, Average CER: 0.295468 Average WER: 0.8564\n\nTrain Epoch: 56 [0/2800 (0%)]\tLoss: 0.322286\nTrain Epoch: 56 [400/2800 (14%)]\tLoss: 0.400935\nTrain Epoch: 56 [800/2800 (29%)]\tLoss: 0.324809\nTrain Epoch: 56 [1200/2800 (43%)]\tLoss: 0.208230\nTrain Epoch: 56 [1600/2800 (57%)]\tLoss: 0.548648\nTrain Epoch: 56 [2000/2800 (71%)]\tLoss: 0.396727\nTrain Epoch: 56 [2400/2800 (86%)]\tLoss: 0.536992\n\nevaluating...\nTest set:\tAverage loss: 1.2501, Average CER: 0.285509 Average WER: 0.8294\n\nTrain Epoch: 57 [0/2800 (0%)]\tLoss: 0.456112\nTrain Epoch: 57 [400/2800 (14%)]\tLoss: 0.435590\nTrain Epoch: 57 [800/2800 (29%)]\tLoss: 0.418290\nTrain Epoch: 57 [1200/2800 (43%)]\tLoss: 0.507339\nTrain Epoch: 57 [1600/2800 (57%)]\tLoss: 0.524723\nTrain Epoch: 57 [2000/2800 (71%)]\tLoss: 0.328511\nTrain Epoch: 57 [2400/2800 (86%)]\tLoss: 0.481564\n\nevaluating...\nTest set:\tAverage loss: 1.0761, Average CER: 0.267471 Average WER: 0.8161\n\nTrain Epoch: 58 [0/2800 (0%)]\tLoss: 0.311422\nTrain Epoch: 58 [400/2800 (14%)]\tLoss: 0.318016\nTrain Epoch: 58 [800/2800 (29%)]\tLoss: 0.296001\nTrain Epoch: 58 [1200/2800 (43%)]\tLoss: 0.326159\nTrain Epoch: 58 [1600/2800 (57%)]\tLoss: 0.328409\nTrain Epoch: 58 [2000/2800 (71%)]\tLoss: 0.483067\nTrain Epoch: 58 [2400/2800 (86%)]\tLoss: 0.361334\n\nevaluating...\nTest set:\tAverage loss: 1.1500, Average CER: 0.266819 Average WER: 0.8121\n\nTrain Epoch: 59 [0/2800 (0%)]\tLoss: 0.367062\nTrain Epoch: 59 [400/2800 (14%)]\tLoss: 0.406608\nTrain Epoch: 59 [800/2800 (29%)]\tLoss: 0.368130\nTrain Epoch: 59 [1200/2800 (43%)]\tLoss: 0.339148\nTrain Epoch: 59 [1600/2800 (57%)]\tLoss: 0.536750\nTrain Epoch: 59 [2000/2800 (71%)]\tLoss: 0.442448\nTrain Epoch: 59 [2400/2800 (86%)]\tLoss: 0.403757\n\nevaluating...\nTest set:\tAverage loss: 1.1642, Average CER: 0.281419 Average WER: 0.8288\n\nTrain Epoch: 60 [0/2800 (0%)]\tLoss: 0.384554\nTrain Epoch: 60 [400/2800 (14%)]\tLoss: 0.553575\nTrain Epoch: 60 [800/2800 (29%)]\tLoss: 0.373293\nTrain Epoch: 60 [1200/2800 (43%)]\tLoss: 0.356690\nTrain Epoch: 60 [1600/2800 (57%)]\tLoss: 0.457114\nTrain Epoch: 60 [2000/2800 (71%)]\tLoss: 0.482642\nTrain Epoch: 60 [2400/2800 (86%)]\tLoss: 0.677134\n\nevaluating...\nTest set:\tAverage loss: 1.1254, Average CER: 0.279718 Average WER: 0.8353\n\nTrain Epoch: 61 [0/2800 (0%)]\tLoss: 0.314371\nTrain Epoch: 61 [400/2800 (14%)]\tLoss: 0.715452\nTrain Epoch: 61 [800/2800 (29%)]\tLoss: 0.437703\nTrain Epoch: 61 [1200/2800 (43%)]\tLoss: 0.397879\nTrain Epoch: 61 [1600/2800 (57%)]\tLoss: 0.370816\nTrain Epoch: 61 [2000/2800 (71%)]\tLoss: 0.415704\nTrain Epoch: 61 [2400/2800 (86%)]\tLoss: 0.600832\n\nevaluating...\nTest set:\tAverage loss: 1.1183, Average CER: 0.277075 Average WER: 0.8292\n\nTrain Epoch: 62 [0/2800 (0%)]\tLoss: 0.357743\nTrain Epoch: 62 [400/2800 (14%)]\tLoss: 0.315886\nTrain Epoch: 62 [800/2800 (29%)]\tLoss: 0.455266\nTrain Epoch: 62 [1200/2800 (43%)]\tLoss: 0.603361\nTrain Epoch: 62 [1600/2800 (57%)]\tLoss: 0.445539\nTrain Epoch: 62 [2000/2800 (71%)]\tLoss: 0.488356\nTrain Epoch: 62 [2400/2800 (86%)]\tLoss: 0.377018\n\nevaluating...\nTest set:\tAverage loss: 1.2152, Average CER: 0.296099 Average WER: 0.8496\n\nTrain Epoch: 63 [0/2800 (0%)]\tLoss: 0.499172\nTrain Epoch: 63 [400/2800 (14%)]\tLoss: 0.422348\nTrain Epoch: 63 [800/2800 (29%)]\tLoss: 0.289238\nTrain Epoch: 63 [1200/2800 (43%)]\tLoss: 0.262008\nTrain Epoch: 63 [1600/2800 (57%)]\tLoss: 0.486193\nTrain Epoch: 63 [2000/2800 (71%)]\tLoss: 0.648312\nTrain Epoch: 63 [2400/2800 (86%)]\tLoss: 0.515878\n\nevaluating...\nTest set:\tAverage loss: 1.1204, Average CER: 0.281645 Average WER: 0.8458\n\nTrain Epoch: 64 [0/2800 (0%)]\tLoss: 0.449069\nTrain Epoch: 64 [400/2800 (14%)]\tLoss: 0.326435\nTrain Epoch: 64 [800/2800 (29%)]\tLoss: 0.425936\nTrain Epoch: 64 [1200/2800 (43%)]\tLoss: 0.464773\nTrain Epoch: 64 [1600/2800 (57%)]\tLoss: 0.591625\nTrain Epoch: 64 [2000/2800 (71%)]\tLoss: 0.662768\nTrain Epoch: 64 [2400/2800 (86%)]\tLoss: 0.566925\n\nevaluating...\nTest set:\tAverage loss: 1.1780, Average CER: 0.290514 Average WER: 0.8540\n\nTrain Epoch: 65 [0/2800 (0%)]\tLoss: 0.608907\nTrain Epoch: 65 [400/2800 (14%)]\tLoss: 0.453881\nTrain Epoch: 65 [800/2800 (29%)]\tLoss: 0.700079\nTrain Epoch: 65 [1200/2800 (43%)]\tLoss: 0.523334\nTrain Epoch: 65 [1600/2800 (57%)]\tLoss: 0.653336\nTrain Epoch: 65 [2000/2800 (71%)]\tLoss: 0.609505\nTrain Epoch: 65 [2400/2800 (86%)]\tLoss: 0.634777\n\nevaluating...\nTest set:\tAverage loss: 1.0866, Average CER: 0.279886 Average WER: 0.8268\n\nTrain Epoch: 66 [0/2800 (0%)]\tLoss: 0.324230\nTrain Epoch: 66 [400/2800 (14%)]\tLoss: 0.493182\nTrain Epoch: 66 [800/2800 (29%)]\tLoss: 0.444120\nTrain Epoch: 66 [1200/2800 (43%)]\tLoss: 0.577664\nTrain Epoch: 66 [1600/2800 (57%)]\tLoss: 0.561998\nTrain Epoch: 66 [2000/2800 (71%)]\tLoss: 0.682894\nTrain Epoch: 66 [2400/2800 (86%)]\tLoss: 0.599730\n\nevaluating...\nTest set:\tAverage loss: 1.1084, Average CER: 0.284235 Average WER: 0.8274\n\nTrain Epoch: 67 [0/2800 (0%)]\tLoss: 0.361428\nTrain Epoch: 67 [400/2800 (14%)]\tLoss: 0.541449\nTrain Epoch: 67 [800/2800 (29%)]\tLoss: 0.415406\nTrain Epoch: 67 [1200/2800 (43%)]\tLoss: 0.410968\nTrain Epoch: 67 [1600/2800 (57%)]\tLoss: 0.445870\nTrain Epoch: 67 [2000/2800 (71%)]\tLoss: 0.401062\nTrain Epoch: 67 [2400/2800 (86%)]\tLoss: 0.433272\n\nevaluating...\nTest set:\tAverage loss: 1.0802, Average CER: 0.273397 Average WER: 0.8290\n\nTrain Epoch: 68 [0/2800 (0%)]\tLoss: 0.520402\nTrain Epoch: 68 [400/2800 (14%)]\tLoss: 0.530141\nTrain Epoch: 68 [800/2800 (29%)]\tLoss: 0.412591\nTrain Epoch: 68 [1200/2800 (43%)]\tLoss: 0.424249\nTrain Epoch: 68 [1600/2800 (57%)]\tLoss: 0.470197\nTrain Epoch: 68 [2000/2800 (71%)]\tLoss: 0.364410\nTrain Epoch: 68 [2400/2800 (86%)]\tLoss: 0.596910\n\nevaluating...\nTest set:\tAverage loss: 1.1331, Average CER: 0.282556 Average WER: 0.8175\n\nTrain Epoch: 69 [0/2800 (0%)]\tLoss: 0.419227\nTrain Epoch: 69 [400/2800 (14%)]\tLoss: 0.552287\nTrain Epoch: 69 [800/2800 (29%)]\tLoss: 0.465401\nTrain Epoch: 69 [1200/2800 (43%)]\tLoss: 0.528181\nTrain Epoch: 69 [1600/2800 (57%)]\tLoss: 0.623450\nTrain Epoch: 69 [2000/2800 (71%)]\tLoss: 0.527080\nTrain Epoch: 69 [2400/2800 (86%)]\tLoss: 0.563332\n\nevaluating...\nTest set:\tAverage loss: 1.1016, Average CER: 0.282231 Average WER: 0.8395\n\nTrain Epoch: 70 [0/2800 (0%)]\tLoss: 0.623893\nTrain Epoch: 70 [400/2800 (14%)]\tLoss: 0.524311\nTrain Epoch: 70 [800/2800 (29%)]\tLoss: 0.406915\nTrain Epoch: 70 [1200/2800 (43%)]\tLoss: 0.395918\nTrain Epoch: 70 [1600/2800 (57%)]\tLoss: 0.537337\nTrain Epoch: 70 [2000/2800 (71%)]\tLoss: 0.505224\nTrain Epoch: 70 [2400/2800 (86%)]\tLoss: 0.578824\n\nevaluating...\nTest set:\tAverage loss: 1.1329, Average CER: 0.284085 Average WER: 0.8350\n\nTrain Epoch: 71 [0/2800 (0%)]\tLoss: 0.464518\nTrain Epoch: 71 [400/2800 (14%)]\tLoss: 0.555373\nTrain Epoch: 71 [800/2800 (29%)]\tLoss: 0.594796\nTrain Epoch: 71 [1200/2800 (43%)]\tLoss: 0.440765\nTrain Epoch: 71 [1600/2800 (57%)]\tLoss: 0.465304\nTrain Epoch: 71 [2000/2800 (71%)]\tLoss: 0.395843\nTrain Epoch: 71 [2400/2800 (86%)]\tLoss: 0.545179\n\nevaluating...\nTest set:\tAverage loss: 1.0478, Average CER: 0.271778 Average WER: 0.8232\n\nTrain Epoch: 72 [0/2800 (0%)]\tLoss: 0.374767\nTrain Epoch: 72 [400/2800 (14%)]\tLoss: 0.527770\nTrain Epoch: 72 [800/2800 (29%)]\tLoss: 0.491353\nTrain Epoch: 72 [1200/2800 (43%)]\tLoss: 0.547476\nTrain Epoch: 72 [1600/2800 (57%)]\tLoss: 0.557064\nTrain Epoch: 72 [2000/2800 (71%)]\tLoss: 0.557805\nTrain Epoch: 72 [2400/2800 (86%)]\tLoss: 0.643981\n\nevaluating...\nTest set:\tAverage loss: 1.1652, Average CER: 0.295221 Average WER: 0.8631\n\nTrain Epoch: 73 [0/2800 (0%)]\tLoss: 0.597097\nTrain Epoch: 73 [400/2800 (14%)]\tLoss: 0.580552\nTrain Epoch: 73 [800/2800 (29%)]\tLoss: 0.634830\nTrain Epoch: 73 [1200/2800 (43%)]\tLoss: 0.694773\nTrain Epoch: 73 [1600/2800 (57%)]\tLoss: 0.412534\nTrain Epoch: 73 [2000/2800 (71%)]\tLoss: 0.540405\nTrain Epoch: 73 [2400/2800 (86%)]\tLoss: 0.516386\n\nevaluating...\nTest set:\tAverage loss: 1.0886, Average CER: 0.280974 Average WER: 0.8236\n\nTrain Epoch: 74 [0/2800 (0%)]\tLoss: 0.444418\nTrain Epoch: 74 [400/2800 (14%)]\tLoss: 0.368918\nTrain Epoch: 74 [800/2800 (29%)]\tLoss: 0.644511\nTrain Epoch: 74 [1200/2800 (43%)]\tLoss: 0.601222\nTrain Epoch: 74 [1600/2800 (57%)]\tLoss: 0.660908\nTrain Epoch: 74 [2000/2800 (71%)]\tLoss: 0.459607\nTrain Epoch: 74 [2400/2800 (86%)]\tLoss: 0.604011\n\nevaluating...\nTest set:\tAverage loss: 1.1256, Average CER: 0.289575 Average WER: 0.8453\n\nTrain Epoch: 75 [0/2800 (0%)]\tLoss: 0.623686\nTrain Epoch: 75 [400/2800 (14%)]\tLoss: 0.598639\nTrain Epoch: 75 [800/2800 (29%)]\tLoss: 0.729558\nTrain Epoch: 75 [1200/2800 (43%)]\tLoss: 0.548558\nTrain Epoch: 75 [1600/2800 (57%)]\tLoss: 0.505737\nTrain Epoch: 75 [2000/2800 (71%)]\tLoss: 0.512504\nTrain Epoch: 75 [2400/2800 (86%)]\tLoss: 0.595795\n\nevaluating...\nTest set:\tAverage loss: 1.1165, Average CER: 0.293919 Average WER: 0.8497\n\nTrain Epoch: 76 [0/2800 (0%)]\tLoss: 0.677332\nTrain Epoch: 76 [400/2800 (14%)]\tLoss: 0.569247\nTrain Epoch: 76 [800/2800 (29%)]\tLoss: 0.638034\nTrain Epoch: 76 [1200/2800 (43%)]\tLoss: 0.585519\nTrain Epoch: 76 [1600/2800 (57%)]\tLoss: 0.793275\nTrain Epoch: 76 [2000/2800 (71%)]\tLoss: 0.738911\nTrain Epoch: 76 [2400/2800 (86%)]\tLoss: 0.682076\n\nevaluating...\nTest set:\tAverage loss: 1.2340, Average CER: 0.326976 Average WER: 0.8775\n\nTrain Epoch: 77 [0/2800 (0%)]\tLoss: 0.749074\nTrain Epoch: 77 [400/2800 (14%)]\tLoss: 0.749278\nTrain Epoch: 77 [800/2800 (29%)]\tLoss: 0.841140\nTrain Epoch: 77 [1200/2800 (43%)]\tLoss: 0.740280\nTrain Epoch: 77 [1600/2800 (57%)]\tLoss: 0.802474\nTrain Epoch: 77 [2000/2800 (71%)]\tLoss: 0.612970\nTrain Epoch: 77 [2400/2800 (86%)]\tLoss: 0.526224\n\nevaluating...\nTest set:\tAverage loss: 1.1577, Average CER: 0.310229 Average WER: 0.8476\n\nTrain Epoch: 78 [0/2800 (0%)]\tLoss: 0.538765\nTrain Epoch: 78 [400/2800 (14%)]\tLoss: 0.616201\nTrain Epoch: 78 [800/2800 (29%)]\tLoss: 0.622288\nTrain Epoch: 78 [1200/2800 (43%)]\tLoss: 0.561036\nTrain Epoch: 78 [1600/2800 (57%)]\tLoss: 0.854184\nTrain Epoch: 78 [2000/2800 (71%)]\tLoss: 0.678420\nTrain Epoch: 78 [2400/2800 (86%)]\tLoss: 0.733964\n\nevaluating...\nTest set:\tAverage loss: 1.1255, Average CER: 0.317890 Average WER: 0.8685\n\nTrain Epoch: 79 [0/2800 (0%)]\tLoss: 0.671251\nTrain Epoch: 79 [400/2800 (14%)]\tLoss: 0.777296\nTrain Epoch: 79 [800/2800 (29%)]\tLoss: 0.494805\nTrain Epoch: 79 [1200/2800 (43%)]\tLoss: 0.716394\nTrain Epoch: 79 [1600/2800 (57%)]\tLoss: 0.718360\nTrain Epoch: 79 [2000/2800 (71%)]\tLoss: 0.862479\nTrain Epoch: 79 [2400/2800 (86%)]\tLoss: 0.734330\n\nevaluating...\nTest set:\tAverage loss: 1.1255, Average CER: 0.305388 Average WER: 0.8573\n\nTrain Epoch: 80 [0/2800 (0%)]\tLoss: 0.715801\nTrain Epoch: 80 [400/2800 (14%)]\tLoss: 0.663623\nTrain Epoch: 80 [800/2800 (29%)]\tLoss: 0.609231\nTrain Epoch: 80 [1200/2800 (43%)]\tLoss: 0.436105\nTrain Epoch: 80 [1600/2800 (57%)]\tLoss: 0.461894\nTrain Epoch: 80 [2000/2800 (71%)]\tLoss: 0.794377\nTrain Epoch: 80 [2400/2800 (86%)]\tLoss: 0.745457\n\nevaluating...\nTest set:\tAverage loss: 1.1752, Average CER: 0.325294 Average WER: 0.9267\n\nTrain Epoch: 81 [0/2800 (0%)]\tLoss: 0.755648\nTrain Epoch: 81 [400/2800 (14%)]\tLoss: 0.798068\nTrain Epoch: 81 [800/2800 (29%)]\tLoss: 0.658889\nTrain Epoch: 81 [1200/2800 (43%)]\tLoss: 0.589155\nTrain Epoch: 81 [1600/2800 (57%)]\tLoss: 0.615799\nTrain Epoch: 81 [2000/2800 (71%)]\tLoss: 0.898974\nTrain Epoch: 81 [2400/2800 (86%)]\tLoss: 0.696065\n\nevaluating...\nTest set:\tAverage loss: 1.0716, Average CER: 0.294377 Average WER: 0.8452\n\nTrain Epoch: 82 [0/2800 (0%)]\tLoss: 0.603466\nTrain Epoch: 82 [400/2800 (14%)]\tLoss: 0.574555\nTrain Epoch: 82 [800/2800 (29%)]\tLoss: 0.394671\nTrain Epoch: 82 [1200/2800 (43%)]\tLoss: 0.569485\nTrain Epoch: 82 [1600/2800 (57%)]\tLoss: 0.459639\nTrain Epoch: 82 [2000/2800 (71%)]\tLoss: 0.584866\nTrain Epoch: 82 [2400/2800 (86%)]\tLoss: 0.623572\n\nevaluating...\nTest set:\tAverage loss: 1.0465, Average CER: 0.289895 Average WER: 0.8529\n\nTrain Epoch: 83 [0/2800 (0%)]\tLoss: 0.656788\nTrain Epoch: 83 [400/2800 (14%)]\tLoss: 0.804801\nTrain Epoch: 83 [800/2800 (29%)]\tLoss: 0.599383\nTrain Epoch: 83 [1200/2800 (43%)]\tLoss: 0.650316\nTrain Epoch: 83 [1600/2800 (57%)]\tLoss: 0.600515\nTrain Epoch: 83 [2000/2800 (71%)]\tLoss: 0.680656\nTrain Epoch: 83 [2400/2800 (86%)]\tLoss: 0.752618\n\nevaluating...\nTest set:\tAverage loss: 1.1728, Average CER: 0.310609 Average WER: 0.8758\n\nTrain Epoch: 84 [0/2800 (0%)]\tLoss: 0.620167\nTrain Epoch: 84 [400/2800 (14%)]\tLoss: 0.850473\nTrain Epoch: 84 [800/2800 (29%)]\tLoss: 0.698346\nTrain Epoch: 84 [1200/2800 (43%)]\tLoss: 0.745478\nTrain Epoch: 84 [1600/2800 (57%)]\tLoss: 0.927536\nTrain Epoch: 84 [2000/2800 (71%)]\tLoss: 0.758037\nTrain Epoch: 84 [2400/2800 (86%)]\tLoss: 0.862916\n\nevaluating...\nTest set:\tAverage loss: 1.1004, Average CER: 0.310014 Average WER: 0.8802\n\nTrain Epoch: 85 [0/2800 (0%)]\tLoss: 0.734322\nTrain Epoch: 85 [400/2800 (14%)]\tLoss: 0.854014\nTrain Epoch: 85 [800/2800 (29%)]\tLoss: 0.564812\nTrain Epoch: 85 [1200/2800 (43%)]\tLoss: 0.744864\nTrain Epoch: 85 [1600/2800 (57%)]\tLoss: 0.912531\nTrain Epoch: 85 [2000/2800 (71%)]\tLoss: 0.736695\nTrain Epoch: 85 [2400/2800 (86%)]\tLoss: 0.785384\n\nevaluating...\nTest set:\tAverage loss: 1.2310, Average CER: 0.342555 Average WER: 0.9058\n\nTrain Epoch: 86 [0/2800 (0%)]\tLoss: 0.832337\nTrain Epoch: 86 [400/2800 (14%)]\tLoss: 0.762067\nTrain Epoch: 86 [800/2800 (29%)]\tLoss: 0.700735\nTrain Epoch: 86 [1200/2800 (43%)]\tLoss: 0.825670\nTrain Epoch: 86 [1600/2800 (57%)]\tLoss: 0.837293\nTrain Epoch: 86 [2000/2800 (71%)]\tLoss: 0.962881\nTrain Epoch: 86 [2400/2800 (86%)]\tLoss: 0.877609\n\nevaluating...\nTest set:\tAverage loss: 1.1859, Average CER: 0.340508 Average WER: 0.8989\n\nTrain Epoch: 87 [0/2800 (0%)]\tLoss: 0.845177\nTrain Epoch: 87 [400/2800 (14%)]\tLoss: 0.795978\nTrain Epoch: 87 [800/2800 (29%)]\tLoss: 1.109850\nTrain Epoch: 87 [1200/2800 (43%)]\tLoss: 0.991172\nTrain Epoch: 87 [1600/2800 (57%)]\tLoss: 1.480605\nTrain Epoch: 87 [2000/2800 (71%)]\tLoss: 0.923156\nTrain Epoch: 87 [2400/2800 (86%)]\tLoss: 0.715548\n\nevaluating...\nTest set:\tAverage loss: 1.2619, Average CER: 0.352292 Average WER: 0.9137\n\nTrain Epoch: 88 [0/2800 (0%)]\tLoss: 0.884083\nTrain Epoch: 88 [400/2800 (14%)]\tLoss: 0.880228\nTrain Epoch: 88 [800/2800 (29%)]\tLoss: 1.058639\nTrain Epoch: 88 [1200/2800 (43%)]\tLoss: 0.881039\nTrain Epoch: 88 [1600/2800 (57%)]\tLoss: 1.075134\nTrain Epoch: 88 [2000/2800 (71%)]\tLoss: 1.061668\nTrain Epoch: 88 [2400/2800 (86%)]\tLoss: 0.802156\n\nevaluating...\nTest set:\tAverage loss: 1.1687, Average CER: 0.339268 Average WER: 0.8979\n\nTrain Epoch: 89 [0/2800 (0%)]\tLoss: 0.748937\nTrain Epoch: 89 [400/2800 (14%)]\tLoss: 0.734660\nTrain Epoch: 89 [800/2800 (29%)]\tLoss: 0.631773\nTrain Epoch: 89 [1200/2800 (43%)]\tLoss: 0.791445\nTrain Epoch: 89 [1600/2800 (57%)]\tLoss: 0.985842\nTrain Epoch: 89 [2000/2800 (71%)]\tLoss: 1.108770\nTrain Epoch: 89 [2400/2800 (86%)]\tLoss: 0.958733\n\nevaluating...\nTest set:\tAverage loss: 1.1112, Average CER: 0.330051 Average WER: 0.8933\n\nTrain Epoch: 90 [0/2800 (0%)]\tLoss: 0.804885\nTrain Epoch: 90 [400/2800 (14%)]\tLoss: 0.736135\nTrain Epoch: 90 [800/2800 (29%)]\tLoss: 0.939461\nTrain Epoch: 90 [1200/2800 (43%)]\tLoss: 1.006804\nTrain Epoch: 90 [1600/2800 (57%)]\tLoss: 0.749059\nTrain Epoch: 90 [2000/2800 (71%)]\tLoss: 0.856858\nTrain Epoch: 90 [2400/2800 (86%)]\tLoss: 0.901713\n\nevaluating...\nTest set:\tAverage loss: 1.2277, Average CER: 0.345099 Average WER: 0.9167\n\nTrain Epoch: 91 [0/2800 (0%)]\tLoss: 0.717445\nTrain Epoch: 91 [400/2800 (14%)]\tLoss: 0.811227\nTrain Epoch: 91 [800/2800 (29%)]\tLoss: 1.251284\nTrain Epoch: 91 [1200/2800 (43%)]\tLoss: 1.111829\nTrain Epoch: 91 [1600/2800 (57%)]\tLoss: 0.849132\nTrain Epoch: 91 [2000/2800 (71%)]\tLoss: 1.218874\nTrain Epoch: 91 [2400/2800 (86%)]\tLoss: 1.169538\n\nevaluating...\nTest set:\tAverage loss: 1.2377, Average CER: 0.372588 Average WER: 0.9262\n\nTrain Epoch: 92 [0/2800 (0%)]\tLoss: 1.188752\nTrain Epoch: 92 [400/2800 (14%)]\tLoss: 1.032001\nTrain Epoch: 92 [800/2800 (29%)]\tLoss: 1.105634\nTrain Epoch: 92 [1200/2800 (43%)]\tLoss: 0.940960\nTrain Epoch: 92 [1600/2800 (57%)]\tLoss: 0.753066\nTrain Epoch: 92 [2000/2800 (71%)]\tLoss: 1.186250\nTrain Epoch: 92 [2400/2800 (86%)]\tLoss: 0.931034\n\nevaluating...\nTest set:\tAverage loss: 1.2433, Average CER: 0.375361 Average WER: 0.9308\n\nTrain Epoch: 93 [0/2800 (0%)]\tLoss: 0.905219\nTrain Epoch: 93 [400/2800 (14%)]\tLoss: 0.937054\nTrain Epoch: 93 [800/2800 (29%)]\tLoss: 1.225566\nTrain Epoch: 93 [1200/2800 (43%)]\tLoss: 1.019559\nTrain Epoch: 93 [1600/2800 (57%)]\tLoss: 0.855991\nTrain Epoch: 93 [2000/2800 (71%)]\tLoss: 0.910969\nTrain Epoch: 93 [2400/2800 (86%)]\tLoss: 1.096329\n\nevaluating...\nTest set:\tAverage loss: 1.2428, Average CER: 0.361418 Average WER: 0.9274\n\nTrain Epoch: 94 [0/2800 (0%)]\tLoss: 1.109768\nTrain Epoch: 94 [400/2800 (14%)]\tLoss: 0.927228\nTrain Epoch: 94 [800/2800 (29%)]\tLoss: 0.999853\nTrain Epoch: 94 [1200/2800 (43%)]\tLoss: 1.031710\nTrain Epoch: 94 [1600/2800 (57%)]\tLoss: 1.095309\nTrain Epoch: 94 [2000/2800 (71%)]\tLoss: 1.090803\nTrain Epoch: 94 [2400/2800 (86%)]\tLoss: 1.102400\n\nevaluating...\nTest set:\tAverage loss: 1.1814, Average CER: 0.350717 Average WER: 0.9059\n\nTrain Epoch: 95 [0/2800 (0%)]\tLoss: 1.185939\nTrain Epoch: 95 [400/2800 (14%)]\tLoss: 0.931671\nTrain Epoch: 95 [800/2800 (29%)]\tLoss: 0.691117\nTrain Epoch: 95 [1200/2800 (43%)]\tLoss: 0.912363\nTrain Epoch: 95 [1600/2800 (57%)]\tLoss: 1.060169\nTrain Epoch: 95 [2000/2800 (71%)]\tLoss: 0.874862\nTrain Epoch: 95 [2400/2800 (86%)]\tLoss: 1.003760\n\nevaluating...\nTest set:\tAverage loss: 1.1619, Average CER: 0.336193 Average WER: 0.8809\n\nTrain Epoch: 96 [0/2800 (0%)]\tLoss: 0.808335\nTrain Epoch: 96 [400/2800 (14%)]\tLoss: 0.746465\nTrain Epoch: 96 [800/2800 (29%)]\tLoss: 0.883259\nTrain Epoch: 96 [1200/2800 (43%)]\tLoss: 1.269631\nTrain Epoch: 96 [1600/2800 (57%)]\tLoss: 1.077976\nTrain Epoch: 96 [2000/2800 (71%)]\tLoss: 0.946818\nTrain Epoch: 96 [2400/2800 (86%)]\tLoss: 0.994235\n\nevaluating...\nTest set:\tAverage loss: 1.2057, Average CER: 0.351260 Average WER: 0.9161\n\nTrain Epoch: 97 [0/2800 (0%)]\tLoss: 0.963431\nTrain Epoch: 97 [400/2800 (14%)]\tLoss: 2.582839\nTrain Epoch: 97 [800/2800 (29%)]\tLoss: 1.963019\nTrain Epoch: 97 [1200/2800 (43%)]\tLoss: 2.084960\nTrain Epoch: 97 [1600/2800 (57%)]\tLoss: 1.934946\nTrain Epoch: 97 [2000/2800 (71%)]\tLoss: 1.571317\nTrain Epoch: 97 [2400/2800 (86%)]\tLoss: 1.432866\n\nevaluating...\nTest set:\tAverage loss: 1.5123, Average CER: 0.447774 Average WER: 0.9623\n\nTrain Epoch: 98 [0/2800 (0%)]\tLoss: 1.273110\nTrain Epoch: 98 [400/2800 (14%)]\tLoss: 1.230317\nTrain Epoch: 98 [800/2800 (29%)]\tLoss: 1.300252\nTrain Epoch: 98 [1200/2800 (43%)]\tLoss: 1.689100\nTrain Epoch: 98 [1600/2800 (57%)]\tLoss: 1.297830\nTrain Epoch: 98 [2000/2800 (71%)]\tLoss: 1.622223\nTrain Epoch: 98 [2400/2800 (86%)]\tLoss: 1.653679\n\nevaluating...\nTest set:\tAverage loss: 1.4162, Average CER: 0.425492 Average WER: 0.9573\n\nTrain Epoch: 99 [0/2800 (0%)]\tLoss: 1.397881\nTrain Epoch: 99 [400/2800 (14%)]\tLoss: 1.435328\nTrain Epoch: 99 [800/2800 (29%)]\tLoss: 1.242321\nTrain Epoch: 99 [1200/2800 (43%)]\tLoss: 1.168814\nTrain Epoch: 99 [1600/2800 (57%)]\tLoss: 1.473864\nTrain Epoch: 99 [2000/2800 (71%)]\tLoss: 1.188627\nTrain Epoch: 99 [2400/2800 (86%)]\tLoss: 1.244855\n\nevaluating...\nTest set:\tAverage loss: 1.2721, Average CER: 0.388989 Average WER: 0.9330\n\nTrain Epoch: 100 [0/2800 (0%)]\tLoss: 1.011441\nTrain Epoch: 100 [400/2800 (14%)]\tLoss: 1.084151\nTrain Epoch: 100 [800/2800 (29%)]\tLoss: 1.020558\nTrain Epoch: 100 [1200/2800 (43%)]\tLoss: 1.155003\nTrain Epoch: 100 [1600/2800 (57%)]\tLoss: 1.073197\nTrain Epoch: 100 [2000/2800 (71%)]\tLoss: 1.110000\nTrain Epoch: 100 [2400/2800 (86%)]\tLoss: 1.350447\n\nevaluating...\nTest set:\tAverage loss: 1.2828, Average CER: 0.390560 Average WER: 0.9332\n\nTrain Epoch: 101 [0/2800 (0%)]\tLoss: 0.938547\nTrain Epoch: 101 [400/2800 (14%)]\tLoss: 0.949762\nTrain Epoch: 101 [800/2800 (29%)]\tLoss: 0.984539\nTrain Epoch: 101 [1200/2800 (43%)]\tLoss: 0.968300\nTrain Epoch: 101 [1600/2800 (57%)]\tLoss: 1.274707\nTrain Epoch: 101 [2000/2800 (71%)]\tLoss: 1.140088\nTrain Epoch: 101 [2400/2800 (86%)]\tLoss: 1.088451\n\nevaluating...\nTest set:\tAverage loss: 1.1852, Average CER: 0.354242 Average WER: 0.9159\n\nTrain Epoch: 102 [0/2800 (0%)]\tLoss: 1.009858\nTrain Epoch: 102 [400/2800 (14%)]\tLoss: 1.042035\nTrain Epoch: 102 [800/2800 (29%)]\tLoss: 0.894711\nTrain Epoch: 102 [1200/2800 (43%)]\tLoss: 0.913054\nTrain Epoch: 102 [1600/2800 (57%)]\tLoss: 1.086471\nTrain Epoch: 102 [2000/2800 (71%)]\tLoss: 1.120836\nTrain Epoch: 102 [2400/2800 (86%)]\tLoss: 0.826328\n\nevaluating...\nTest set:\tAverage loss: 1.1805, Average CER: 0.356589 Average WER: 0.9214\n\nTrain Epoch: 103 [0/2800 (0%)]\tLoss: 0.780116\nTrain Epoch: 103 [400/2800 (14%)]\tLoss: 1.022322\nTrain Epoch: 103 [800/2800 (29%)]\tLoss: 1.100525\nTrain Epoch: 103 [1200/2800 (43%)]\tLoss: 1.043556\nTrain Epoch: 103 [1600/2800 (57%)]\tLoss: 1.095996\nTrain Epoch: 103 [2000/2800 (71%)]\tLoss: 0.855546\nTrain Epoch: 103 [2400/2800 (86%)]\tLoss: 0.957784\n\nevaluating...\nTest set:\tAverage loss: 1.1602, Average CER: 0.353554 Average WER: 0.9180\n\nTrain Epoch: 104 [0/2800 (0%)]\tLoss: 1.021181\nTrain Epoch: 104 [400/2800 (14%)]\tLoss: 0.816296\nTrain Epoch: 104 [800/2800 (29%)]\tLoss: 0.861667\nTrain Epoch: 104 [1200/2800 (43%)]\tLoss: 1.029820\nTrain Epoch: 104 [1600/2800 (57%)]\tLoss: 1.090953\nTrain Epoch: 104 [2000/2800 (71%)]\tLoss: 1.014089\nTrain Epoch: 104 [2400/2800 (86%)]\tLoss: 0.900751\n\nevaluating...\nTest set:\tAverage loss: 1.1725, Average CER: 0.349014 Average WER: 0.9222\n\nTrain Epoch: 105 [0/2800 (0%)]\tLoss: 0.810559\nTrain Epoch: 105 [400/2800 (14%)]\tLoss: 0.798286\nTrain Epoch: 105 [800/2800 (29%)]\tLoss: 0.911926\nTrain Epoch: 105 [1200/2800 (43%)]\tLoss: 0.918090\nTrain Epoch: 105 [1600/2800 (57%)]\tLoss: 0.981705\nTrain Epoch: 105 [2000/2800 (71%)]\tLoss: 0.964075\nTrain Epoch: 105 [2400/2800 (86%)]\tLoss: 1.004669\n\nevaluating...\nTest set:\tAverage loss: 1.1515, Average CER: 0.340579 Average WER: 0.9097\n\nTrain Epoch: 106 [0/2800 (0%)]\tLoss: 0.620814\nTrain Epoch: 106 [400/2800 (14%)]\tLoss: 1.152542\nTrain Epoch: 106 [800/2800 (29%)]\tLoss: 1.209378\nTrain Epoch: 106 [1200/2800 (43%)]\tLoss: 0.938748\nTrain Epoch: 106 [1600/2800 (57%)]\tLoss: 0.854880\nTrain Epoch: 106 [2000/2800 (71%)]\tLoss: 1.060681\nTrain Epoch: 106 [2400/2800 (86%)]\tLoss: 0.845496\n\nevaluating...\nTest set:\tAverage loss: 1.1376, Average CER: 0.337952 Average WER: 0.9041\n\nTrain Epoch: 107 [0/2800 (0%)]\tLoss: 0.697918\nTrain Epoch: 107 [400/2800 (14%)]\tLoss: 0.886265\nTrain Epoch: 107 [800/2800 (29%)]\tLoss: 0.879931\nTrain Epoch: 107 [1200/2800 (43%)]\tLoss: 0.822576\nTrain Epoch: 107 [1600/2800 (57%)]\tLoss: 0.793917\nTrain Epoch: 107 [2000/2800 (71%)]\tLoss: 1.004560\nTrain Epoch: 107 [2400/2800 (86%)]\tLoss: 0.871036\n\nevaluating...\nTest set:\tAverage loss: 1.1020, Average CER: 0.325458 Average WER: 0.8932\n\nTrain Epoch: 108 [0/2800 (0%)]\tLoss: 0.611098\nTrain Epoch: 108 [400/2800 (14%)]\tLoss: 0.736898\nTrain Epoch: 108 [800/2800 (29%)]\tLoss: 1.025741\nTrain Epoch: 108 [1200/2800 (43%)]\tLoss: 0.917887\nTrain Epoch: 108 [1600/2800 (57%)]\tLoss: 0.908320\nTrain Epoch: 108 [2000/2800 (71%)]\tLoss: 1.055579\nTrain Epoch: 108 [2400/2800 (86%)]\tLoss: 0.799940\n\nevaluating...\nTest set:\tAverage loss: 1.0940, Average CER: 0.322334 Average WER: 0.8805\n\nTrain Epoch: 109 [0/2800 (0%)]\tLoss: 0.788451\nTrain Epoch: 109 [400/2800 (14%)]\tLoss: 0.745366\nTrain Epoch: 109 [800/2800 (29%)]\tLoss: 0.870400\nTrain Epoch: 109 [1200/2800 (43%)]\tLoss: 0.845827\nTrain Epoch: 109 [1600/2800 (57%)]\tLoss: 0.854193\nTrain Epoch: 109 [2000/2800 (71%)]\tLoss: 0.951474\nTrain Epoch: 109 [2400/2800 (86%)]\tLoss: 0.831109\n\nevaluating...\nTest set:\tAverage loss: 1.0800, Average CER: 0.317411 Average WER: 0.8684\n\nTrain Epoch: 110 [0/2800 (0%)]\tLoss: 0.813152\nTrain Epoch: 110 [400/2800 (14%)]\tLoss: 0.577540\nTrain Epoch: 110 [800/2800 (29%)]\tLoss: 0.809220\nTrain Epoch: 110 [1200/2800 (43%)]\tLoss: 0.740094\nTrain Epoch: 110 [1600/2800 (57%)]\tLoss: 0.726949\nTrain Epoch: 110 [2000/2800 (71%)]\tLoss: 0.826427\nTrain Epoch: 110 [2400/2800 (86%)]\tLoss: 0.815620\n\nevaluating...\nTest set:\tAverage loss: 1.0860, Average CER: 0.317553 Average WER: 0.8793\n\nTrain Epoch: 111 [0/2800 (0%)]\tLoss: 0.731634\nTrain Epoch: 111 [400/2800 (14%)]\tLoss: 0.687169\nTrain Epoch: 111 [800/2800 (29%)]\tLoss: 0.908702\nTrain Epoch: 111 [1200/2800 (43%)]\tLoss: 0.994192\nTrain Epoch: 111 [1600/2800 (57%)]\tLoss: 0.787135\nTrain Epoch: 111 [2000/2800 (71%)]\tLoss: 0.900366\nTrain Epoch: 111 [2400/2800 (86%)]\tLoss: 1.086207\n\nevaluating...\nTest set:\tAverage loss: 1.0726, Average CER: 0.315841 Average WER: 0.8766\n\nTrain Epoch: 112 [0/2800 (0%)]\tLoss: 0.756688\nTrain Epoch: 112 [400/2800 (14%)]\tLoss: 0.732163\nTrain Epoch: 112 [800/2800 (29%)]\tLoss: 0.787793\nTrain Epoch: 112 [1200/2800 (43%)]\tLoss: 0.626285\nTrain Epoch: 112 [1600/2800 (57%)]\tLoss: 0.938749\nTrain Epoch: 112 [2000/2800 (71%)]\tLoss: 0.737626\nTrain Epoch: 112 [2400/2800 (86%)]\tLoss: 0.863215\n\nevaluating...\nTest set:\tAverage loss: 1.0791, Average CER: 0.312585 Average WER: 0.8765\n\nTrain Epoch: 113 [0/2800 (0%)]\tLoss: 0.842279\nTrain Epoch: 113 [400/2800 (14%)]\tLoss: 0.664199\nTrain Epoch: 113 [800/2800 (29%)]\tLoss: 0.628061\nTrain Epoch: 113 [1200/2800 (43%)]\tLoss: 1.073124\nTrain Epoch: 113 [1600/2800 (57%)]\tLoss: 0.874884\nTrain Epoch: 113 [2000/2800 (71%)]\tLoss: 0.840376\nTrain Epoch: 113 [2400/2800 (86%)]\tLoss: 0.847761\n\nevaluating...\nTest set:\tAverage loss: 1.0559, Average CER: 0.312068 Average WER: 0.8645\n\nTrain Epoch: 114 [0/2800 (0%)]\tLoss: 0.616250\nTrain Epoch: 114 [400/2800 (14%)]\tLoss: 0.590972\nTrain Epoch: 114 [800/2800 (29%)]\tLoss: 0.591240\nTrain Epoch: 114 [1200/2800 (43%)]\tLoss: 0.447575\nTrain Epoch: 114 [1600/2800 (57%)]\tLoss: 0.629318\nTrain Epoch: 114 [2000/2800 (71%)]\tLoss: 0.830814\nTrain Epoch: 114 [2400/2800 (86%)]\tLoss: 0.729125\n\nevaluating...\nTest set:\tAverage loss: 1.1067, Average CER: 0.316585 Average WER: 0.8743\n\nTrain Epoch: 115 [0/2800 (0%)]\tLoss: 0.690852\nTrain Epoch: 115 [400/2800 (14%)]\tLoss: 0.714457\nTrain Epoch: 115 [800/2800 (29%)]\tLoss: 1.047138\nTrain Epoch: 115 [1200/2800 (43%)]\tLoss: 0.803493\nTrain Epoch: 115 [1600/2800 (57%)]\tLoss: 0.670631\nTrain Epoch: 115 [2000/2800 (71%)]\tLoss: 0.845653\nTrain Epoch: 115 [2400/2800 (86%)]\tLoss: 0.774711\n\nevaluating...\nTest set:\tAverage loss: 1.0850, Average CER: 0.310954 Average WER: 0.8694\n\nTrain Epoch: 116 [0/2800 (0%)]\tLoss: 0.804823\nTrain Epoch: 116 [400/2800 (14%)]\tLoss: 0.604846\nTrain Epoch: 116 [800/2800 (29%)]\tLoss: 0.534810\nTrain Epoch: 116 [1200/2800 (43%)]\tLoss: 0.751943\nTrain Epoch: 116 [1600/2800 (57%)]\tLoss: 0.702155\nTrain Epoch: 116 [2000/2800 (71%)]\tLoss: 0.738188\nTrain Epoch: 116 [2400/2800 (86%)]\tLoss: 0.938723\n\nevaluating...\nTest set:\tAverage loss: 1.1274, Average CER: 0.329212 Average WER: 0.9076\n\nTrain Epoch: 117 [0/2800 (0%)]\tLoss: 0.697739\nTrain Epoch: 117 [400/2800 (14%)]\tLoss: 0.691184\nTrain Epoch: 117 [800/2800 (29%)]\tLoss: 1.044012\nTrain Epoch: 117 [1200/2800 (43%)]\tLoss: 0.739249\nTrain Epoch: 117 [1600/2800 (57%)]\tLoss: 0.739891\nTrain Epoch: 117 [2000/2800 (71%)]\tLoss: 0.774853\nTrain Epoch: 117 [2400/2800 (86%)]\tLoss: 1.128957\n\nevaluating...\nTest set:\tAverage loss: 1.0944, Average CER: 0.314986 Average WER: 0.8800\n\nTrain Epoch: 118 [0/2800 (0%)]\tLoss: 0.834094\nTrain Epoch: 118 [400/2800 (14%)]\tLoss: 0.844114\nTrain Epoch: 118 [800/2800 (29%)]\tLoss: 0.670732\nTrain Epoch: 118 [1200/2800 (43%)]\tLoss: 0.597734\nTrain Epoch: 118 [1600/2800 (57%)]\tLoss: 0.747965\nTrain Epoch: 118 [2000/2800 (71%)]\tLoss: 0.852008\nTrain Epoch: 118 [2400/2800 (86%)]\tLoss: 0.660480\n\nevaluating...\nTest set:\tAverage loss: 1.0651, Average CER: 0.308645 Average WER: 0.8661\n\nTrain Epoch: 119 [0/2800 (0%)]\tLoss: 0.700770\nTrain Epoch: 119 [400/2800 (14%)]\tLoss: 0.663839\nTrain Epoch: 119 [800/2800 (29%)]\tLoss: 0.626809\nTrain Epoch: 119 [1200/2800 (43%)]\tLoss: 0.740467\nTrain Epoch: 119 [1600/2800 (57%)]\tLoss: 0.828909\nTrain Epoch: 119 [2000/2800 (71%)]\tLoss: 0.777064\nTrain Epoch: 119 [2400/2800 (86%)]\tLoss: 0.759836\n\nevaluating...\nTest set:\tAverage loss: 1.0821, Average CER: 0.311870 Average WER: 0.8767\n\nTrain Epoch: 120 [0/2800 (0%)]\tLoss: 0.745887\nTrain Epoch: 120 [400/2800 (14%)]\tLoss: 0.527950\nTrain Epoch: 120 [800/2800 (29%)]\tLoss: 0.725412\nTrain Epoch: 120 [1200/2800 (43%)]\tLoss: 0.520841\nTrain Epoch: 120 [1600/2800 (57%)]\tLoss: 0.722641\nTrain Epoch: 120 [2000/2800 (71%)]\tLoss: 0.711261\nTrain Epoch: 120 [2400/2800 (86%)]\tLoss: 0.794180\n\nevaluating...\nTest set:\tAverage loss: 1.1040, Average CER: 0.317241 Average WER: 0.8677\n\nTrain Epoch: 121 [0/2800 (0%)]\tLoss: 0.704652\nTrain Epoch: 121 [400/2800 (14%)]\tLoss: 0.660894\nTrain Epoch: 121 [800/2800 (29%)]\tLoss: 0.761471\nTrain Epoch: 121 [1200/2800 (43%)]\tLoss: 0.657374\nTrain Epoch: 121 [1600/2800 (57%)]\tLoss: 0.715373\nTrain Epoch: 121 [2000/2800 (71%)]\tLoss: 0.788991\nTrain Epoch: 121 [2400/2800 (86%)]\tLoss: 0.685866\n\nevaluating...\nTest set:\tAverage loss: 1.0672, Average CER: 0.304517 Average WER: 0.8570\n\nTrain Epoch: 122 [0/2800 (0%)]\tLoss: 0.668647\nTrain Epoch: 122 [400/2800 (14%)]\tLoss: 0.689073\nTrain Epoch: 122 [800/2800 (29%)]\tLoss: 0.818532\nTrain Epoch: 122 [1200/2800 (43%)]\tLoss: 0.706030\nTrain Epoch: 122 [1600/2800 (57%)]\tLoss: 0.773019\nTrain Epoch: 122 [2000/2800 (71%)]\tLoss: 0.722042\nTrain Epoch: 122 [2400/2800 (86%)]\tLoss: 0.733567\n\nevaluating...\nTest set:\tAverage loss: 1.0885, Average CER: 0.311222 Average WER: 0.8712\n\nTrain Epoch: 123 [0/2800 (0%)]\tLoss: 0.568306\nTrain Epoch: 123 [400/2800 (14%)]\tLoss: 0.678930\nTrain Epoch: 123 [800/2800 (29%)]\tLoss: 0.763720\nTrain Epoch: 123 [1200/2800 (43%)]\tLoss: 0.794861\nTrain Epoch: 123 [1600/2800 (57%)]\tLoss: 0.791422\nTrain Epoch: 123 [2000/2800 (71%)]\tLoss: 0.740373\nTrain Epoch: 123 [2400/2800 (86%)]\tLoss: 0.769958\n\nevaluating...\nTest set:\tAverage loss: 1.1989, Average CER: 0.331781 Average WER: 0.8756\n\nTrain Epoch: 124 [0/2800 (0%)]\tLoss: 0.762516\nTrain Epoch: 124 [400/2800 (14%)]\tLoss: 0.905341\nTrain Epoch: 124 [800/2800 (29%)]\tLoss: 0.692736\nTrain Epoch: 124 [1200/2800 (43%)]\tLoss: 0.674354\nTrain Epoch: 124 [1600/2800 (57%)]\tLoss: 0.896826\nTrain Epoch: 124 [2000/2800 (71%)]\tLoss: 0.918205\nTrain Epoch: 124 [2400/2800 (86%)]\tLoss: 0.752831\n\nevaluating...\nTest set:\tAverage loss: 1.1139, Average CER: 0.311921 Average WER: 0.8600\n\nTrain Epoch: 125 [0/2800 (0%)]\tLoss: 0.793494\nTrain Epoch: 125 [400/2800 (14%)]\tLoss: 0.584193\nTrain Epoch: 125 [800/2800 (29%)]\tLoss: 0.792090\nTrain Epoch: 125 [1200/2800 (43%)]\tLoss: 0.731263\nTrain Epoch: 125 [1600/2800 (57%)]\tLoss: 0.630314\nTrain Epoch: 125 [2000/2800 (71%)]\tLoss: 0.593178\nTrain Epoch: 125 [2400/2800 (86%)]\tLoss: 0.643721\n\nevaluating...\nTest set:\tAverage loss: 1.0786, Average CER: 0.305202 Average WER: 0.8547\n\nTrain Epoch: 126 [0/2800 (0%)]\tLoss: 0.547235\nTrain Epoch: 126 [400/2800 (14%)]\tLoss: 0.561101\nTrain Epoch: 126 [800/2800 (29%)]\tLoss: 0.426091\nTrain Epoch: 126 [1200/2800 (43%)]\tLoss: 0.568139\nTrain Epoch: 126 [1600/2800 (57%)]\tLoss: 0.715281\nTrain Epoch: 126 [2000/2800 (71%)]\tLoss: 0.614022\nTrain Epoch: 126 [2400/2800 (86%)]\tLoss: 0.646248\n\nevaluating...\nTest set:\tAverage loss: 1.0418, Average CER: 0.294742 Average WER: 0.8362\n\nTrain Epoch: 127 [0/2800 (0%)]\tLoss: 0.536929\nTrain Epoch: 127 [400/2800 (14%)]\tLoss: 0.536843\nTrain Epoch: 127 [800/2800 (29%)]\tLoss: 0.681421\nTrain Epoch: 127 [1200/2800 (43%)]\tLoss: 0.551384\nTrain Epoch: 127 [1600/2800 (57%)]\tLoss: 0.512843\nTrain Epoch: 127 [2000/2800 (71%)]\tLoss: 0.587843\nTrain Epoch: 127 [2400/2800 (86%)]\tLoss: 0.435173\n\nevaluating...\nTest set:\tAverage loss: 1.0701, Average CER: 0.300193 Average WER: 0.8483\n\nTrain Epoch: 128 [0/2800 (0%)]\tLoss: 0.476418\nTrain Epoch: 128 [400/2800 (14%)]\tLoss: 0.528229\nTrain Epoch: 128 [800/2800 (29%)]\tLoss: 0.682977\nTrain Epoch: 128 [1200/2800 (43%)]\tLoss: 0.587740\nTrain Epoch: 128 [1600/2800 (57%)]\tLoss: 0.749039\nTrain Epoch: 128 [2000/2800 (71%)]\tLoss: 0.534520\nTrain Epoch: 128 [2400/2800 (86%)]\tLoss: 0.684812\n\nevaluating...\nTest set:\tAverage loss: 1.0609, Average CER: 0.299106 Average WER: 0.8472\n\nTrain Epoch: 129 [0/2800 (0%)]\tLoss: 0.680423\nTrain Epoch: 129 [400/2800 (14%)]\tLoss: 0.691495\nTrain Epoch: 129 [800/2800 (29%)]\tLoss: 0.528241\nTrain Epoch: 129 [1200/2800 (43%)]\tLoss: 0.857501\nTrain Epoch: 129 [1600/2800 (57%)]\tLoss: 0.605153\nTrain Epoch: 129 [2000/2800 (71%)]\tLoss: 0.740541\nTrain Epoch: 129 [2400/2800 (86%)]\tLoss: 0.571742\n\nevaluating...\nTest set:\tAverage loss: 1.0724, Average CER: 0.293215 Average WER: 0.8425\n\nTrain Epoch: 130 [0/2800 (0%)]\tLoss: 0.591824\nTrain Epoch: 130 [400/2800 (14%)]\tLoss: 0.534377\nTrain Epoch: 130 [800/2800 (29%)]\tLoss: 0.770095\nTrain Epoch: 130 [1200/2800 (43%)]\tLoss: 0.631176\nTrain Epoch: 130 [1600/2800 (57%)]\tLoss: 0.608459\nTrain Epoch: 130 [2000/2800 (71%)]\tLoss: 0.582988\nTrain Epoch: 130 [2400/2800 (86%)]\tLoss: 0.573774\n\nevaluating...\nTest set:\tAverage loss: 1.0274, Average CER: 0.280700 Average WER: 0.8311\n\nTrain Epoch: 131 [0/2800 (0%)]\tLoss: 0.633990\nTrain Epoch: 131 [400/2800 (14%)]\tLoss: 0.461682\nTrain Epoch: 131 [800/2800 (29%)]\tLoss: 0.542970\nTrain Epoch: 131 [1200/2800 (43%)]\tLoss: 0.576322\nTrain Epoch: 131 [1600/2800 (57%)]\tLoss: 0.497507\nTrain Epoch: 131 [2000/2800 (71%)]\tLoss: 0.681652\nTrain Epoch: 131 [2400/2800 (86%)]\tLoss: 0.515524\n\nevaluating...\nTest set:\tAverage loss: 1.0144, Average CER: 0.281201 Average WER: 0.8175\n\nTrain Epoch: 132 [0/2800 (0%)]\tLoss: 0.497110\nTrain Epoch: 132 [400/2800 (14%)]\tLoss: 0.603085\nTrain Epoch: 132 [800/2800 (29%)]\tLoss: 0.449908\nTrain Epoch: 132 [1200/2800 (43%)]\tLoss: 0.557505\nTrain Epoch: 132 [1600/2800 (57%)]\tLoss: 0.617599\nTrain Epoch: 132 [2000/2800 (71%)]\tLoss: 0.617573\nTrain Epoch: 132 [2400/2800 (86%)]\tLoss: 0.628785\n\nevaluating...\nTest set:\tAverage loss: 1.0383, Average CER: 0.285455 Average WER: 0.8279\n\nTrain Epoch: 133 [0/2800 (0%)]\tLoss: 0.643304\nTrain Epoch: 133 [400/2800 (14%)]\tLoss: 0.451891\nTrain Epoch: 133 [800/2800 (29%)]\tLoss: 0.479591\nTrain Epoch: 133 [1200/2800 (43%)]\tLoss: 0.538450\nTrain Epoch: 133 [1600/2800 (57%)]\tLoss: 0.372566\nTrain Epoch: 133 [2000/2800 (71%)]\tLoss: 0.399862\nTrain Epoch: 133 [2400/2800 (86%)]\tLoss: 0.397078\n\nevaluating...\nTest set:\tAverage loss: 1.0250, Average CER: 0.277245 Average WER: 0.8258\n\nTrain Epoch: 134 [0/2800 (0%)]\tLoss: 0.581273\nTrain Epoch: 134 [400/2800 (14%)]\tLoss: 0.464802\nTrain Epoch: 134 [800/2800 (29%)]\tLoss: 0.511580\nTrain Epoch: 134 [1200/2800 (43%)]\tLoss: 0.476087\nTrain Epoch: 134 [1600/2800 (57%)]\tLoss: 0.611993\nTrain Epoch: 134 [2000/2800 (71%)]\tLoss: 0.334935\nTrain Epoch: 134 [2400/2800 (86%)]\tLoss: 0.572725\n\nevaluating...\nTest set:\tAverage loss: 1.0470, Average CER: 0.282336 Average WER: 0.8054\n\nTrain Epoch: 135 [0/2800 (0%)]\tLoss: 0.434425\nTrain Epoch: 135 [400/2800 (14%)]\tLoss: 0.393766\nTrain Epoch: 135 [800/2800 (29%)]\tLoss: 0.589051\nTrain Epoch: 135 [1200/2800 (43%)]\tLoss: 0.486217\nTrain Epoch: 135 [1600/2800 (57%)]\tLoss: 0.379976\nTrain Epoch: 135 [2000/2800 (71%)]\tLoss: 0.635430\nTrain Epoch: 135 [2400/2800 (86%)]\tLoss: 0.313555\n\nevaluating...\nTest set:\tAverage loss: 1.0164, Average CER: 0.277392 Average WER: 0.8188\n\nTrain Epoch: 136 [0/2800 (0%)]\tLoss: 0.254962\nTrain Epoch: 136 [400/2800 (14%)]\tLoss: 0.391336\nTrain Epoch: 136 [800/2800 (29%)]\tLoss: 0.449554\nTrain Epoch: 136 [1200/2800 (43%)]\tLoss: 0.396561\nTrain Epoch: 136 [1600/2800 (57%)]\tLoss: 0.392092\nTrain Epoch: 136 [2000/2800 (71%)]\tLoss: 0.330468\nTrain Epoch: 136 [2400/2800 (86%)]\tLoss: 0.516559\n\nevaluating...\nTest set:\tAverage loss: 1.0151, Average CER: 0.269729 Average WER: 0.7980\n\nTrain Epoch: 137 [0/2800 (0%)]\tLoss: 0.346061\nTrain Epoch: 137 [400/2800 (14%)]\tLoss: 0.401835\nTrain Epoch: 137 [800/2800 (29%)]\tLoss: 0.297606\nTrain Epoch: 137 [1200/2800 (43%)]\tLoss: 0.413949\nTrain Epoch: 137 [1600/2800 (57%)]\tLoss: 0.420193\nTrain Epoch: 137 [2000/2800 (71%)]\tLoss: 0.531079\nTrain Epoch: 137 [2400/2800 (86%)]\tLoss: 0.351644\n\nevaluating...\nTest set:\tAverage loss: 1.0148, Average CER: 0.272103 Average WER: 0.7933\n\nTrain Epoch: 138 [0/2800 (0%)]\tLoss: 0.375276\nTrain Epoch: 138 [400/2800 (14%)]\tLoss: 0.303676\nTrain Epoch: 138 [800/2800 (29%)]\tLoss: 0.259423\nTrain Epoch: 138 [1200/2800 (43%)]\tLoss: 0.689056\nTrain Epoch: 138 [1600/2800 (57%)]\tLoss: 0.364444\nTrain Epoch: 138 [2000/2800 (71%)]\tLoss: 0.411661\nTrain Epoch: 138 [2400/2800 (86%)]\tLoss: 0.282758\n\nevaluating...\nTest set:\tAverage loss: 1.0047, Average CER: 0.268044 Average WER: 0.7985\n\nTrain Epoch: 139 [0/2800 (0%)]\tLoss: 0.325847\nTrain Epoch: 139 [400/2800 (14%)]\tLoss: 0.341383\nTrain Epoch: 139 [800/2800 (29%)]\tLoss: 0.349878\nTrain Epoch: 139 [1200/2800 (43%)]\tLoss: 0.298457\nTrain Epoch: 139 [1600/2800 (57%)]\tLoss: 0.338247\nTrain Epoch: 139 [2000/2800 (71%)]\tLoss: 0.365123\nTrain Epoch: 139 [2400/2800 (86%)]\tLoss: 0.564060\n\nevaluating...\nTest set:\tAverage loss: 1.0138, Average CER: 0.270769 Average WER: 0.7986\n\nTrain Epoch: 140 [0/2800 (0%)]\tLoss: 0.431965\nTrain Epoch: 140 [400/2800 (14%)]\tLoss: 0.331573\nTrain Epoch: 140 [800/2800 (29%)]\tLoss: 0.328702\nTrain Epoch: 140 [1200/2800 (43%)]\tLoss: 0.359256\nTrain Epoch: 140 [1600/2800 (57%)]\tLoss: 0.268616\nTrain Epoch: 140 [2000/2800 (71%)]\tLoss: 0.291079\nTrain Epoch: 140 [2400/2800 (86%)]\tLoss: 0.336834\n\nevaluating...\nTest set:\tAverage loss: 1.0376, Average CER: 0.264060 Average WER: 0.7977\n\nTrain Epoch: 141 [0/2800 (0%)]\tLoss: 0.400361\nTrain Epoch: 141 [400/2800 (14%)]\tLoss: 0.322511\nTrain Epoch: 141 [800/2800 (29%)]\tLoss: 0.260142\nTrain Epoch: 141 [1200/2800 (43%)]\tLoss: 0.362558\nTrain Epoch: 141 [1600/2800 (57%)]\tLoss: 0.366938\nTrain Epoch: 141 [2000/2800 (71%)]\tLoss: 0.515956\nTrain Epoch: 141 [2400/2800 (86%)]\tLoss: 0.385020\n\nevaluating...\nTest set:\tAverage loss: 1.0419, Average CER: 0.275038 Average WER: 0.8200\n\nTrain Epoch: 142 [0/2800 (0%)]\tLoss: 0.389678\nTrain Epoch: 142 [400/2800 (14%)]\tLoss: 0.297012\nTrain Epoch: 142 [800/2800 (29%)]\tLoss: 0.281299\nTrain Epoch: 142 [1200/2800 (43%)]\tLoss: 0.416600\nTrain Epoch: 142 [1600/2800 (57%)]\tLoss: 0.330253\nTrain Epoch: 142 [2000/2800 (71%)]\tLoss: 0.376734\nTrain Epoch: 142 [2400/2800 (86%)]\tLoss: 0.546971\n\nevaluating...\nTest set:\tAverage loss: 1.0166, Average CER: 0.267208 Average WER: 0.8033\n\nTrain Epoch: 143 [0/2800 (0%)]\tLoss: 0.365214\nTrain Epoch: 143 [400/2800 (14%)]\tLoss: 0.260956\nTrain Epoch: 143 [800/2800 (29%)]\tLoss: 0.317816\nTrain Epoch: 143 [1200/2800 (43%)]\tLoss: 0.481964\nTrain Epoch: 143 [1600/2800 (57%)]\tLoss: 0.315337\nTrain Epoch: 143 [2000/2800 (71%)]\tLoss: 0.310700\nTrain Epoch: 143 [2400/2800 (86%)]\tLoss: 0.566481\n\nevaluating...\nTest set:\tAverage loss: 1.0267, Average CER: 0.268179 Average WER: 0.8001\n\nTrain Epoch: 144 [0/2800 (0%)]\tLoss: 0.400018\nTrain Epoch: 144 [400/2800 (14%)]\tLoss: 0.426263\nTrain Epoch: 144 [800/2800 (29%)]\tLoss: 0.257652\nTrain Epoch: 144 [1200/2800 (43%)]\tLoss: 0.218876\nTrain Epoch: 144 [1600/2800 (57%)]\tLoss: 0.288099\nTrain Epoch: 144 [2000/2800 (71%)]\tLoss: 0.320679\nTrain Epoch: 144 [2400/2800 (86%)]\tLoss: 0.288755\n\nevaluating...\nTest set:\tAverage loss: 1.0238, Average CER: 0.261342 Average WER: 0.7961\n\nTrain Epoch: 145 [0/2800 (0%)]\tLoss: 0.258128\nTrain Epoch: 145 [400/2800 (14%)]\tLoss: 0.365102\nTrain Epoch: 145 [800/2800 (29%)]\tLoss: 0.404803\nTrain Epoch: 145 [1200/2800 (43%)]\tLoss: 0.543747\nTrain Epoch: 145 [1600/2800 (57%)]\tLoss: 0.473396\nTrain Epoch: 145 [2000/2800 (71%)]\tLoss: 0.364182\nTrain Epoch: 145 [2400/2800 (86%)]\tLoss: 0.537067\n\nevaluating...\nTest set:\tAverage loss: 1.0108, Average CER: 0.256769 Average WER: 0.7810\n\nTrain Epoch: 146 [0/2800 (0%)]\tLoss: 0.274938\nTrain Epoch: 146 [400/2800 (14%)]\tLoss: 0.280043\nTrain Epoch: 146 [800/2800 (29%)]\tLoss: 0.305617\nTrain Epoch: 146 [1200/2800 (43%)]\tLoss: 0.365000\nTrain Epoch: 146 [1600/2800 (57%)]\tLoss: 0.271397\nTrain Epoch: 146 [2000/2800 (71%)]\tLoss: 0.264167\nTrain Epoch: 146 [2400/2800 (86%)]\tLoss: 0.283875\n\nevaluating...\nTest set:\tAverage loss: 1.0403, Average CER: 0.256670 Average WER: 0.7770\n\nTrain Epoch: 147 [0/2800 (0%)]\tLoss: 0.404344\nTrain Epoch: 147 [400/2800 (14%)]\tLoss: 0.343962\nTrain Epoch: 147 [800/2800 (29%)]\tLoss: 0.279902\nTrain Epoch: 147 [1200/2800 (43%)]\tLoss: 0.347935\nTrain Epoch: 147 [1600/2800 (57%)]\tLoss: 0.294829\nTrain Epoch: 147 [2000/2800 (71%)]\tLoss: 0.327971\nTrain Epoch: 147 [2400/2800 (86%)]\tLoss: 0.257188\n\nevaluating...\nTest set:\tAverage loss: 1.0229, Average CER: 0.257849 Average WER: 0.7945\n\nTrain Epoch: 148 [0/2800 (0%)]\tLoss: 0.340035\nTrain Epoch: 148 [400/2800 (14%)]\tLoss: 0.275465\nTrain Epoch: 148 [800/2800 (29%)]\tLoss: 0.302029\nTrain Epoch: 148 [1200/2800 (43%)]\tLoss: 0.185075\nTrain Epoch: 148 [1600/2800 (57%)]\tLoss: 0.240808\nTrain Epoch: 148 [2000/2800 (71%)]\tLoss: 0.271589\nTrain Epoch: 148 [2400/2800 (86%)]\tLoss: 0.341080\n\nevaluating...\nTest set:\tAverage loss: 1.0411, Average CER: 0.254278 Average WER: 0.7784\n\nTrain Epoch: 149 [0/2800 (0%)]\tLoss: 0.230008\nTrain Epoch: 149 [400/2800 (14%)]\tLoss: 0.190320\nTrain Epoch: 149 [800/2800 (29%)]\tLoss: 0.205118\nTrain Epoch: 149 [1200/2800 (43%)]\tLoss: 0.396247\nTrain Epoch: 149 [1600/2800 (57%)]\tLoss: 0.284843\nTrain Epoch: 149 [2000/2800 (71%)]\tLoss: 0.251817\nTrain Epoch: 149 [2400/2800 (86%)]\tLoss: 0.309254\n\nevaluating...\nTest set:\tAverage loss: 1.0496, Average CER: 0.258729 Average WER: 0.7832\n\nTrain Epoch: 150 [0/2800 (0%)]\tLoss: 0.275497\nTrain Epoch: 150 [400/2800 (14%)]\tLoss: 0.304277\nTrain Epoch: 150 [800/2800 (29%)]\tLoss: 0.249668\nTrain Epoch: 150 [1200/2800 (43%)]\tLoss: 0.327450\nTrain Epoch: 150 [1600/2800 (57%)]\tLoss: 0.278453\nTrain Epoch: 150 [2000/2800 (71%)]\tLoss: 0.309910\nTrain Epoch: 150 [2400/2800 (86%)]\tLoss: 0.342395\n\nevaluating...\nTest set:\tAverage loss: 1.0712, Average CER: 0.257288 Average WER: 0.7768\n\nTrain Epoch: 151 [0/2800 (0%)]\tLoss: 0.220638\nTrain Epoch: 151 [400/2800 (14%)]\tLoss: 0.237842\nTrain Epoch: 151 [800/2800 (29%)]\tLoss: 0.329885\nTrain Epoch: 151 [1200/2800 (43%)]\tLoss: 0.370276\nTrain Epoch: 151 [1600/2800 (57%)]\tLoss: 0.299909\nTrain Epoch: 151 [2000/2800 (71%)]\tLoss: 0.144947\nTrain Epoch: 151 [2400/2800 (86%)]\tLoss: 0.257979\n\nevaluating...\nTest set:\tAverage loss: 1.0574, Average CER: 0.251979 Average WER: 0.7737\n\nTrain Epoch: 152 [0/2800 (0%)]\tLoss: 0.205611\nTrain Epoch: 152 [400/2800 (14%)]\tLoss: 0.268563\nTrain Epoch: 152 [800/2800 (29%)]\tLoss: 0.329763\nTrain Epoch: 152 [1200/2800 (43%)]\tLoss: 0.328226\nTrain Epoch: 152 [1600/2800 (57%)]\tLoss: 0.290034\nTrain Epoch: 152 [2000/2800 (71%)]\tLoss: 0.238028\nTrain Epoch: 152 [2400/2800 (86%)]\tLoss: 0.274953\n\nevaluating...\nTest set:\tAverage loss: 1.0504, Average CER: 0.252066 Average WER: 0.7705\n\nTrain Epoch: 153 [0/2800 (0%)]\tLoss: 0.400867\nTrain Epoch: 153 [400/2800 (14%)]\tLoss: 0.244350\nTrain Epoch: 153 [800/2800 (29%)]\tLoss: 0.207968\nTrain Epoch: 153 [1200/2800 (43%)]\tLoss: 0.285300\nTrain Epoch: 153 [1600/2800 (57%)]\tLoss: 0.223018\nTrain Epoch: 153 [2000/2800 (71%)]\tLoss: 0.179813\nTrain Epoch: 153 [2400/2800 (86%)]\tLoss: 0.246326\n\nevaluating...\nTest set:\tAverage loss: 1.0709, Average CER: 0.250422 Average WER: 0.7643\n\nTrain Epoch: 154 [0/2800 (0%)]\tLoss: 0.110218\nTrain Epoch: 154 [400/2800 (14%)]\tLoss: 0.243759\nTrain Epoch: 154 [800/2800 (29%)]\tLoss: 0.242726\nTrain Epoch: 154 [1200/2800 (43%)]\tLoss: 0.346413\nTrain Epoch: 154 [1600/2800 (57%)]\tLoss: 0.260494\nTrain Epoch: 154 [2000/2800 (71%)]\tLoss: 0.294691\nTrain Epoch: 154 [2400/2800 (86%)]\tLoss: 0.194124\n\nevaluating...\nTest set:\tAverage loss: 1.0701, Average CER: 0.250701 Average WER: 0.7711\n\nTrain Epoch: 155 [0/2800 (0%)]\tLoss: 0.191972\nTrain Epoch: 155 [400/2800 (14%)]\tLoss: 0.164650\nTrain Epoch: 155 [800/2800 (29%)]\tLoss: 0.118709\nTrain Epoch: 155 [1200/2800 (43%)]\tLoss: 0.185660\nTrain Epoch: 155 [1600/2800 (57%)]\tLoss: 0.149232\nTrain Epoch: 155 [2000/2800 (71%)]\tLoss: 0.250578\nTrain Epoch: 155 [2400/2800 (86%)]\tLoss: 0.275852\n\nevaluating...\nTest set:\tAverage loss: 1.0491, Average CER: 0.243833 Average WER: 0.7632\n\nTrain Epoch: 156 [0/2800 (0%)]\tLoss: 0.217842\nTrain Epoch: 156 [400/2800 (14%)]\tLoss: 0.176820\nTrain Epoch: 156 [800/2800 (29%)]\tLoss: 0.209818\nTrain Epoch: 156 [1200/2800 (43%)]\tLoss: 0.164422\nTrain Epoch: 156 [1600/2800 (57%)]\tLoss: 0.177948\nTrain Epoch: 156 [2000/2800 (71%)]\tLoss: 0.273190\nTrain Epoch: 156 [2400/2800 (86%)]\tLoss: 0.257379\n\nevaluating...\nTest set:\tAverage loss: 1.0708, Average CER: 0.245297 Average WER: 0.7617\n\nTrain Epoch: 157 [0/2800 (0%)]\tLoss: 0.202988\nTrain Epoch: 157 [400/2800 (14%)]\tLoss: 0.198759\nTrain Epoch: 157 [800/2800 (29%)]\tLoss: 0.139727\nTrain Epoch: 157 [1200/2800 (43%)]\tLoss: 0.174250\nTrain Epoch: 157 [1600/2800 (57%)]\tLoss: 0.170661\nTrain Epoch: 157 [2000/2800 (71%)]\tLoss: 0.172154\nTrain Epoch: 157 [2400/2800 (86%)]\tLoss: 0.131781\n\nevaluating...\nTest set:\tAverage loss: 1.0839, Average CER: 0.246802 Average WER: 0.7688\n\nTrain Epoch: 158 [0/2800 (0%)]\tLoss: 0.180961\nTrain Epoch: 158 [400/2800 (14%)]\tLoss: 0.218735\nTrain Epoch: 158 [800/2800 (29%)]\tLoss: 0.217794\nTrain Epoch: 158 [1200/2800 (43%)]\tLoss: 0.179148\nTrain Epoch: 158 [1600/2800 (57%)]\tLoss: 0.278751\nTrain Epoch: 158 [2000/2800 (71%)]\tLoss: 0.149302\nTrain Epoch: 158 [2400/2800 (86%)]\tLoss: 0.286768\n\nevaluating...\nTest set:\tAverage loss: 1.0846, Average CER: 0.248880 Average WER: 0.7662\n\nTrain Epoch: 159 [0/2800 (0%)]\tLoss: 0.213187\nTrain Epoch: 159 [400/2800 (14%)]\tLoss: 0.223264\nTrain Epoch: 159 [800/2800 (29%)]\tLoss: 0.100882\nTrain Epoch: 159 [1200/2800 (43%)]\tLoss: 0.202768\nTrain Epoch: 159 [1600/2800 (57%)]\tLoss: 0.140043\nTrain Epoch: 159 [2000/2800 (71%)]\tLoss: 0.152372\nTrain Epoch: 159 [2400/2800 (86%)]\tLoss: 0.227586\n\nevaluating...\nTest set:\tAverage loss: 1.1028, Average CER: 0.247184 Average WER: 0.7591\n\nTrain Epoch: 160 [0/2800 (0%)]\tLoss: 0.093596\nTrain Epoch: 160 [400/2800 (14%)]\tLoss: 0.208875\nTrain Epoch: 160 [800/2800 (29%)]\tLoss: 0.162052\nTrain Epoch: 160 [1200/2800 (43%)]\tLoss: 0.181354\nTrain Epoch: 160 [1600/2800 (57%)]\tLoss: 0.257380\nTrain Epoch: 160 [2000/2800 (71%)]\tLoss: 0.200041\nTrain Epoch: 160 [2400/2800 (86%)]\tLoss: 0.148738\n\nevaluating...\nTest set:\tAverage loss: 1.1144, Average CER: 0.243940 Average WER: 0.7405\n\nTrain Epoch: 161 [0/2800 (0%)]\tLoss: 0.070781\nTrain Epoch: 161 [400/2800 (14%)]\tLoss: 0.144655\nTrain Epoch: 161 [800/2800 (29%)]\tLoss: 0.165748\nTrain Epoch: 161 [1200/2800 (43%)]\tLoss: 0.131028\nTrain Epoch: 161 [1600/2800 (57%)]\tLoss: 0.161494\nTrain Epoch: 161 [2000/2800 (71%)]\tLoss: 0.130689\nTrain Epoch: 161 [2400/2800 (86%)]\tLoss: 0.133685\n\nevaluating...\nTest set:\tAverage loss: 1.1157, Average CER: 0.242630 Average WER: 0.7520\n\nTrain Epoch: 162 [0/2800 (0%)]\tLoss: 0.125266\nTrain Epoch: 162 [400/2800 (14%)]\tLoss: 0.159926\nTrain Epoch: 162 [800/2800 (29%)]\tLoss: 0.109795\nTrain Epoch: 162 [1200/2800 (43%)]\tLoss: 0.084866\nTrain Epoch: 162 [1600/2800 (57%)]\tLoss: 0.140739\nTrain Epoch: 162 [2000/2800 (71%)]\tLoss: 0.226982\nTrain Epoch: 162 [2400/2800 (86%)]\tLoss: 0.134974\n\nevaluating...\nTest set:\tAverage loss: 1.1258, Average CER: 0.241146 Average WER: 0.7607\n\nTrain Epoch: 163 [0/2800 (0%)]\tLoss: 0.097156\nTrain Epoch: 163 [400/2800 (14%)]\tLoss: 0.161751\nTrain Epoch: 163 [800/2800 (29%)]\tLoss: 0.146077\nTrain Epoch: 163 [1200/2800 (43%)]\tLoss: 0.134441\nTrain Epoch: 163 [1600/2800 (57%)]\tLoss: 0.243642\nTrain Epoch: 163 [2000/2800 (71%)]\tLoss: 0.166765\nTrain Epoch: 163 [2400/2800 (86%)]\tLoss: 0.120260\n\nevaluating...\nTest set:\tAverage loss: 1.1346, Average CER: 0.239871 Average WER: 0.7553\n\nTrain Epoch: 164 [0/2800 (0%)]\tLoss: 0.196214\nTrain Epoch: 164 [400/2800 (14%)]\tLoss: 0.160798\nTrain Epoch: 164 [800/2800 (29%)]\tLoss: 0.078975\nTrain Epoch: 164 [1200/2800 (43%)]\tLoss: 0.118082\nTrain Epoch: 164 [1600/2800 (57%)]\tLoss: 0.101648\nTrain Epoch: 164 [2000/2800 (71%)]\tLoss: 0.146258\nTrain Epoch: 164 [2400/2800 (86%)]\tLoss: 0.226926\n\nevaluating...\nTest set:\tAverage loss: 1.1372, Average CER: 0.239737 Average WER: 0.7475\n\nTrain Epoch: 165 [0/2800 (0%)]\tLoss: 0.117751\nTrain Epoch: 165 [400/2800 (14%)]\tLoss: 0.182584\nTrain Epoch: 165 [800/2800 (29%)]\tLoss: 0.086339\nTrain Epoch: 165 [1200/2800 (43%)]\tLoss: 0.153812\nTrain Epoch: 165 [1600/2800 (57%)]\tLoss: 0.144945\nTrain Epoch: 165 [2000/2800 (71%)]\tLoss: 0.253311\nTrain Epoch: 165 [2400/2800 (86%)]\tLoss: 0.124313\n\nevaluating...\nTest set:\tAverage loss: 1.1353, Average CER: 0.239085 Average WER: 0.7446\n\nTrain Epoch: 166 [0/2800 (0%)]\tLoss: 0.100834\nTrain Epoch: 166 [400/2800 (14%)]\tLoss: 0.093569\nTrain Epoch: 166 [800/2800 (29%)]\tLoss: 0.088145\nTrain Epoch: 166 [1200/2800 (43%)]\tLoss: 0.237877\nTrain Epoch: 166 [1600/2800 (57%)]\tLoss: 0.113473\nTrain Epoch: 166 [2000/2800 (71%)]\tLoss: 0.222363\nTrain Epoch: 166 [2400/2800 (86%)]\tLoss: 0.103575\n\nevaluating...\nTest set:\tAverage loss: 1.1655, Average CER: 0.238546 Average WER: 0.7537\n\nTrain Epoch: 167 [0/2800 (0%)]\tLoss: 0.122258\nTrain Epoch: 167 [400/2800 (14%)]\tLoss: 0.153382\nTrain Epoch: 167 [800/2800 (29%)]\tLoss: 0.116661\nTrain Epoch: 167 [1200/2800 (43%)]\tLoss: 0.186860\nTrain Epoch: 167 [1600/2800 (57%)]\tLoss: 0.102744\nTrain Epoch: 167 [2000/2800 (71%)]\tLoss: 0.107386\nTrain Epoch: 167 [2400/2800 (86%)]\tLoss: 0.098732\n\nevaluating...\nTest set:\tAverage loss: 1.1338, Average CER: 0.237791 Average WER: 0.7460\n\nTrain Epoch: 168 [0/2800 (0%)]\tLoss: 0.115286\nTrain Epoch: 168 [400/2800 (14%)]\tLoss: 0.071486\nTrain Epoch: 168 [800/2800 (29%)]\tLoss: 0.089728\nTrain Epoch: 168 [1200/2800 (43%)]\tLoss: 0.053591\nTrain Epoch: 168 [1600/2800 (57%)]\tLoss: 0.171293\nTrain Epoch: 168 [2000/2800 (71%)]\tLoss: 0.079836\nTrain Epoch: 168 [2400/2800 (86%)]\tLoss: 0.129169\n\nevaluating...\nTest set:\tAverage loss: 1.1423, Average CER: 0.237940 Average WER: 0.7319\n\nTrain Epoch: 169 [0/2800 (0%)]\tLoss: 0.114573\nTrain Epoch: 169 [400/2800 (14%)]\tLoss: 0.085285\nTrain Epoch: 169 [800/2800 (29%)]\tLoss: 0.201400\nTrain Epoch: 169 [1200/2800 (43%)]\tLoss: 0.087653\nTrain Epoch: 169 [1600/2800 (57%)]\tLoss: 0.070867\nTrain Epoch: 169 [2000/2800 (71%)]\tLoss: 0.209755\nTrain Epoch: 169 [2400/2800 (86%)]\tLoss: 0.093555\n\nevaluating...\nTest set:\tAverage loss: 1.1593, Average CER: 0.233406 Average WER: 0.7419\n\nTrain Epoch: 170 [0/2800 (0%)]\tLoss: 0.081000\nTrain Epoch: 170 [400/2800 (14%)]\tLoss: 0.061133\nTrain Epoch: 170 [800/2800 (29%)]\tLoss: 0.123212\nTrain Epoch: 170 [1200/2800 (43%)]\tLoss: 0.061666\nTrain Epoch: 170 [1600/2800 (57%)]\tLoss: 0.065629\nTrain Epoch: 170 [2000/2800 (71%)]\tLoss: 0.099323\nTrain Epoch: 170 [2400/2800 (86%)]\tLoss: 0.129015\n\nevaluating...\nTest set:\tAverage loss: 1.1793, Average CER: 0.232607 Average WER: 0.7414\n\nTrain Epoch: 171 [0/2800 (0%)]\tLoss: 0.071432\nTrain Epoch: 171 [400/2800 (14%)]\tLoss: 0.079697\nTrain Epoch: 171 [800/2800 (29%)]\tLoss: 0.084914\nTrain Epoch: 171 [1200/2800 (43%)]\tLoss: 0.051186\nTrain Epoch: 171 [1600/2800 (57%)]\tLoss: 0.078887\nTrain Epoch: 171 [2000/2800 (71%)]\tLoss: 0.083213\nTrain Epoch: 171 [2400/2800 (86%)]\tLoss: 0.094245\n\nevaluating...\nTest set:\tAverage loss: 1.1759, Average CER: 0.232323 Average WER: 0.7362\n\nTrain Epoch: 172 [0/2800 (0%)]\tLoss: 0.061723\nTrain Epoch: 172 [400/2800 (14%)]\tLoss: 0.088876\nTrain Epoch: 172 [800/2800 (29%)]\tLoss: 0.073050\nTrain Epoch: 172 [1200/2800 (43%)]\tLoss: 0.068426\nTrain Epoch: 172 [1600/2800 (57%)]\tLoss: 0.099693\nTrain Epoch: 172 [2000/2800 (71%)]\tLoss: 0.107942\nTrain Epoch: 172 [2400/2800 (86%)]\tLoss: 0.039003\n\nevaluating...\nTest set:\tAverage loss: 1.1807, Average CER: 0.233377 Average WER: 0.7382\n\nTrain Epoch: 173 [0/2800 (0%)]\tLoss: 0.087233\nTrain Epoch: 173 [400/2800 (14%)]\tLoss: 0.064935\nTrain Epoch: 173 [800/2800 (29%)]\tLoss: 0.078743\nTrain Epoch: 173 [1200/2800 (43%)]\tLoss: 0.069279\nTrain Epoch: 173 [1600/2800 (57%)]\tLoss: 0.100942\nTrain Epoch: 173 [2000/2800 (71%)]\tLoss: 0.080259\nTrain Epoch: 173 [2400/2800 (86%)]\tLoss: 0.079023\n\nevaluating...\nTest set:\tAverage loss: 1.2044, Average CER: 0.231601 Average WER: 0.7363\n\nTrain Epoch: 174 [0/2800 (0%)]\tLoss: 0.068966\nTrain Epoch: 174 [400/2800 (14%)]\tLoss: 0.066692\nTrain Epoch: 174 [800/2800 (29%)]\tLoss: 0.074256\nTrain Epoch: 174 [1200/2800 (43%)]\tLoss: 0.177772\nTrain Epoch: 174 [1600/2800 (57%)]\tLoss: 0.089289\nTrain Epoch: 174 [2000/2800 (71%)]\tLoss: 0.092891\nTrain Epoch: 174 [2400/2800 (86%)]\tLoss: 0.115108\n\nevaluating...\nTest set:\tAverage loss: 1.2132, Average CER: 0.234963 Average WER: 0.7473\n\nTrain Epoch: 175 [0/2800 (0%)]\tLoss: 0.079772\nTrain Epoch: 175 [400/2800 (14%)]\tLoss: 0.060387\nTrain Epoch: 175 [800/2800 (29%)]\tLoss: 0.035570\nTrain Epoch: 175 [1200/2800 (43%)]\tLoss: 0.090983\nTrain Epoch: 175 [1600/2800 (57%)]\tLoss: 0.077662\nTrain Epoch: 175 [2000/2800 (71%)]\tLoss: 0.102522\nTrain Epoch: 175 [2400/2800 (86%)]\tLoss: 0.075319\n\nevaluating...\nTest set:\tAverage loss: 1.2099, Average CER: 0.230464 Average WER: 0.7248\n\nTrain Epoch: 176 [0/2800 (0%)]\tLoss: 0.044281\nTrain Epoch: 176 [400/2800 (14%)]\tLoss: 0.070472\nTrain Epoch: 176 [800/2800 (29%)]\tLoss: 0.055362\nTrain Epoch: 176 [1200/2800 (43%)]\tLoss: 0.042816\nTrain Epoch: 176 [1600/2800 (57%)]\tLoss: 0.063250\nTrain Epoch: 176 [2000/2800 (71%)]\tLoss: 0.081879\nTrain Epoch: 176 [2400/2800 (86%)]\tLoss: 0.080974\n\nevaluating...\nTest set:\tAverage loss: 1.2258, Average CER: 0.232070 Average WER: 0.7300\n\nTrain Epoch: 177 [0/2800 (0%)]\tLoss: 0.068859\nTrain Epoch: 177 [400/2800 (14%)]\tLoss: 0.045137\nTrain Epoch: 177 [800/2800 (29%)]\tLoss: 0.072478\nTrain Epoch: 177 [1200/2800 (43%)]\tLoss: 0.101086\nTrain Epoch: 177 [1600/2800 (57%)]\tLoss: 0.048759\nTrain Epoch: 177 [2000/2800 (71%)]\tLoss: 0.043577\nTrain Epoch: 177 [2400/2800 (86%)]\tLoss: 0.041503\n\nevaluating...\nTest set:\tAverage loss: 1.2412, Average CER: 0.227925 Average WER: 0.7226\n\nTrain Epoch: 178 [0/2800 (0%)]\tLoss: 0.034203\nTrain Epoch: 178 [400/2800 (14%)]\tLoss: 0.069340\nTrain Epoch: 178 [800/2800 (29%)]\tLoss: 0.081733\nTrain Epoch: 178 [1200/2800 (43%)]\tLoss: 0.056441\nTrain Epoch: 178 [1600/2800 (57%)]\tLoss: 0.080053\nTrain Epoch: 178 [2000/2800 (71%)]\tLoss: 0.086994\nTrain Epoch: 178 [2400/2800 (86%)]\tLoss: 0.051746\n\nevaluating...\nTest set:\tAverage loss: 1.2500, Average CER: 0.226646 Average WER: 0.7222\n\nTrain Epoch: 179 [0/2800 (0%)]\tLoss: 0.059561\nTrain Epoch: 179 [400/2800 (14%)]\tLoss: 0.038374\nTrain Epoch: 179 [800/2800 (29%)]\tLoss: 0.046236\nTrain Epoch: 179 [1200/2800 (43%)]\tLoss: 0.064648\nTrain Epoch: 179 [1600/2800 (57%)]\tLoss: 0.083683\nTrain Epoch: 179 [2000/2800 (71%)]\tLoss: 0.039836\nTrain Epoch: 179 [2400/2800 (86%)]\tLoss: 0.048352\n\nevaluating...\nTest set:\tAverage loss: 1.2675, Average CER: 0.229539 Average WER: 0.7360\n\nTrain Epoch: 180 [0/2800 (0%)]\tLoss: 0.070030\nTrain Epoch: 180 [400/2800 (14%)]\tLoss: 0.031520\nTrain Epoch: 180 [800/2800 (29%)]\tLoss: 0.048252\nTrain Epoch: 180 [1200/2800 (43%)]\tLoss: 0.044725\nTrain Epoch: 180 [1600/2800 (57%)]\tLoss: 0.023679\nTrain Epoch: 180 [2000/2800 (71%)]\tLoss: 0.038455\nTrain Epoch: 180 [2400/2800 (86%)]\tLoss: 0.029747\n\nevaluating...\nTest set:\tAverage loss: 1.2766, Average CER: 0.229670 Average WER: 0.7243\n\nTrain Epoch: 181 [0/2800 (0%)]\tLoss: 0.024658\nTrain Epoch: 181 [400/2800 (14%)]\tLoss: 0.035322\nTrain Epoch: 181 [800/2800 (29%)]\tLoss: 0.034977\nTrain Epoch: 181 [1200/2800 (43%)]\tLoss: 0.023509\nTrain Epoch: 181 [1600/2800 (57%)]\tLoss: 0.059036\nTrain Epoch: 181 [2000/2800 (71%)]\tLoss: 0.045298\nTrain Epoch: 181 [2400/2800 (86%)]\tLoss: 0.049003\n\nevaluating...\nTest set:\tAverage loss: 1.2642, Average CER: 0.229666 Average WER: 0.7236\n\nTrain Epoch: 182 [0/2800 (0%)]\tLoss: 0.050630\nTrain Epoch: 182 [400/2800 (14%)]\tLoss: 0.064224\nTrain Epoch: 182 [800/2800 (29%)]\tLoss: 0.045125\nTrain Epoch: 182 [1200/2800 (43%)]\tLoss: 0.041263\nTrain Epoch: 182 [1600/2800 (57%)]\tLoss: 0.056107\nTrain Epoch: 182 [2000/2800 (71%)]\tLoss: 0.019873\nTrain Epoch: 182 [2400/2800 (86%)]\tLoss: 0.055078\n\nevaluating...\nTest set:\tAverage loss: 1.2915, Average CER: 0.230573 Average WER: 0.7227\n\nTrain Epoch: 183 [0/2800 (0%)]\tLoss: 0.041199\nTrain Epoch: 183 [400/2800 (14%)]\tLoss: 0.040796\nTrain Epoch: 183 [800/2800 (29%)]\tLoss: 0.074057\nTrain Epoch: 183 [1200/2800 (43%)]\tLoss: 0.051959\nTrain Epoch: 183 [1600/2800 (57%)]\tLoss: 0.039981\nTrain Epoch: 183 [2000/2800 (71%)]\tLoss: 0.035907\nTrain Epoch: 183 [2400/2800 (86%)]\tLoss: 0.040476\n\nevaluating...\nTest set:\tAverage loss: 1.2934, Average CER: 0.225543 Average WER: 0.7206\n\nTrain Epoch: 184 [0/2800 (0%)]\tLoss: 0.019603\nTrain Epoch: 184 [400/2800 (14%)]\tLoss: 0.026521\nTrain Epoch: 184 [800/2800 (29%)]\tLoss: 0.025256\nTrain Epoch: 184 [1200/2800 (43%)]\tLoss: 0.041317\nTrain Epoch: 184 [1600/2800 (57%)]\tLoss: 0.024863\nTrain Epoch: 184 [2000/2800 (71%)]\tLoss: 0.059039\nTrain Epoch: 184 [2400/2800 (86%)]\tLoss: 0.018368\n\nevaluating...\nTest set:\tAverage loss: 1.3085, Average CER: 0.226620 Average WER: 0.7272\n\nTrain Epoch: 185 [0/2800 (0%)]\tLoss: 0.031988\nTrain Epoch: 185 [400/2800 (14%)]\tLoss: 0.036769\nTrain Epoch: 185 [800/2800 (29%)]\tLoss: 0.022327\nTrain Epoch: 185 [1200/2800 (43%)]\tLoss: 0.029292\nTrain Epoch: 185 [1600/2800 (57%)]\tLoss: 0.057172\nTrain Epoch: 185 [2000/2800 (71%)]\tLoss: 0.025943\nTrain Epoch: 185 [2400/2800 (86%)]\tLoss: 0.015372\n\nevaluating...\nTest set:\tAverage loss: 1.3095, Average CER: 0.230109 Average WER: 0.7266\n\nTrain Epoch: 186 [0/2800 (0%)]\tLoss: 0.055220\nTrain Epoch: 186 [400/2800 (14%)]\tLoss: 0.035896\nTrain Epoch: 186 [800/2800 (29%)]\tLoss: 0.014885\nTrain Epoch: 186 [1200/2800 (43%)]\tLoss: 0.040241\nTrain Epoch: 186 [1600/2800 (57%)]\tLoss: 0.057812\nTrain Epoch: 186 [2000/2800 (71%)]\tLoss: 0.036535\nTrain Epoch: 186 [2400/2800 (86%)]\tLoss: 0.030004\n\nevaluating...\nTest set:\tAverage loss: 1.3171, Average CER: 0.225571 Average WER: 0.7323\n\nTrain Epoch: 187 [0/2800 (0%)]\tLoss: 0.028128\nTrain Epoch: 187 [400/2800 (14%)]\tLoss: 0.032658\nTrain Epoch: 187 [800/2800 (29%)]\tLoss: 0.032142\nTrain Epoch: 187 [1200/2800 (43%)]\tLoss: 0.018115\nTrain Epoch: 187 [1600/2800 (57%)]\tLoss: 0.035665\nTrain Epoch: 187 [2000/2800 (71%)]\tLoss: 0.038560\nTrain Epoch: 187 [2400/2800 (86%)]\tLoss: 0.023932\n\nevaluating...\nTest set:\tAverage loss: 1.3202, Average CER: 0.227349 Average WER: 0.7336\n\nTrain Epoch: 188 [0/2800 (0%)]\tLoss: 0.023683\nTrain Epoch: 188 [400/2800 (14%)]\tLoss: 0.026117\nTrain Epoch: 188 [800/2800 (29%)]\tLoss: 0.022241\nTrain Epoch: 188 [1200/2800 (43%)]\tLoss: 0.031820\nTrain Epoch: 188 [1600/2800 (57%)]\tLoss: 0.019396\nTrain Epoch: 188 [2000/2800 (71%)]\tLoss: 0.011282\nTrain Epoch: 188 [2400/2800 (86%)]\tLoss: 0.052808\n\nevaluating...\nTest set:\tAverage loss: 1.3345, Average CER: 0.227829 Average WER: 0.7277\n\nTrain Epoch: 189 [0/2800 (0%)]\tLoss: 0.021072\nTrain Epoch: 189 [400/2800 (14%)]\tLoss: 0.017941\nTrain Epoch: 189 [800/2800 (29%)]\tLoss: 0.026473\nTrain Epoch: 189 [1200/2800 (43%)]\tLoss: 0.010003\nTrain Epoch: 189 [1600/2800 (57%)]\tLoss: 0.022150\nTrain Epoch: 189 [2000/2800 (71%)]\tLoss: 0.019478\nTrain Epoch: 189 [2400/2800 (86%)]\tLoss: 0.015925\n\nevaluating...\nTest set:\tAverage loss: 1.3261, Average CER: 0.226313 Average WER: 0.7284\n\nTrain Epoch: 190 [0/2800 (0%)]\tLoss: 0.013545\nTrain Epoch: 190 [400/2800 (14%)]\tLoss: 0.017072\nTrain Epoch: 190 [800/2800 (29%)]\tLoss: 0.013878\nTrain Epoch: 190 [1200/2800 (43%)]\tLoss: 0.012080\nTrain Epoch: 190 [1600/2800 (57%)]\tLoss: 0.015171\nTrain Epoch: 190 [2000/2800 (71%)]\tLoss: 0.015532\nTrain Epoch: 190 [2400/2800 (86%)]\tLoss: 0.018486\n\nevaluating...\nTest set:\tAverage loss: 1.3346, Average CER: 0.225872 Average WER: 0.7254\n\nTrain Epoch: 191 [0/2800 (0%)]\tLoss: 0.024642\nTrain Epoch: 191 [400/2800 (14%)]\tLoss: 0.029258\nTrain Epoch: 191 [800/2800 (29%)]\tLoss: 0.019935\nTrain Epoch: 191 [1200/2800 (43%)]\tLoss: 0.015348\nTrain Epoch: 191 [1600/2800 (57%)]\tLoss: 0.020983\nTrain Epoch: 191 [2000/2800 (71%)]\tLoss: 0.020304\nTrain Epoch: 191 [2400/2800 (86%)]\tLoss: 0.006942\n\nevaluating...\nTest set:\tAverage loss: 1.3560, Average CER: 0.225555 Average WER: 0.7189\n\nTrain Epoch: 192 [0/2800 (0%)]\tLoss: 0.008885\nTrain Epoch: 192 [400/2800 (14%)]\tLoss: 0.029404\nTrain Epoch: 192 [800/2800 (29%)]\tLoss: 0.013637\nTrain Epoch: 192 [1200/2800 (43%)]\tLoss: 0.011817\nTrain Epoch: 192 [1600/2800 (57%)]\tLoss: 0.019938\nTrain Epoch: 192 [2000/2800 (71%)]\tLoss: 0.015353\nTrain Epoch: 192 [2400/2800 (86%)]\tLoss: 0.013636\n\nevaluating...\nTest set:\tAverage loss: 1.3561, Average CER: 0.226903 Average WER: 0.7284\n\nTrain Epoch: 193 [0/2800 (0%)]\tLoss: 0.007241\nTrain Epoch: 193 [400/2800 (14%)]\tLoss: 0.016741\nTrain Epoch: 193 [800/2800 (29%)]\tLoss: 0.008418\nTrain Epoch: 193 [1200/2800 (43%)]\tLoss: 0.032583\nTrain Epoch: 193 [1600/2800 (57%)]\tLoss: 0.013554\nTrain Epoch: 193 [2000/2800 (71%)]\tLoss: 0.012497\nTrain Epoch: 193 [2400/2800 (86%)]\tLoss: 0.041215\n\nevaluating...\nTest set:\tAverage loss: 1.3687, Average CER: 0.226510 Average WER: 0.7361\n\nTrain Epoch: 194 [0/2800 (0%)]\tLoss: 0.024979\nTrain Epoch: 194 [400/2800 (14%)]\tLoss: 0.029933\nTrain Epoch: 194 [800/2800 (29%)]\tLoss: 0.007348\nTrain Epoch: 194 [1200/2800 (43%)]\tLoss: 0.014361\nTrain Epoch: 194 [1600/2800 (57%)]\tLoss: 0.010212\nTrain Epoch: 194 [2000/2800 (71%)]\tLoss: 0.008576\nTrain Epoch: 194 [2400/2800 (86%)]\tLoss: 0.031701\n\nevaluating...\nTest set:\tAverage loss: 1.3709, Average CER: 0.225684 Average WER: 0.7325\n\nTrain Epoch: 195 [0/2800 (0%)]\tLoss: 0.007101\nTrain Epoch: 195 [400/2800 (14%)]\tLoss: 0.019784\nTrain Epoch: 195 [800/2800 (29%)]\tLoss: 0.012040\nTrain Epoch: 195 [1200/2800 (43%)]\tLoss: 0.005980\nTrain Epoch: 195 [1600/2800 (57%)]\tLoss: 0.017456\nTrain Epoch: 195 [2000/2800 (71%)]\tLoss: 0.010662\nTrain Epoch: 195 [2400/2800 (86%)]\tLoss: 0.003462\n\nevaluating...\nTest set:\tAverage loss: 1.3755, Average CER: 0.227606 Average WER: 0.7340\n\nTrain Epoch: 196 [0/2800 (0%)]\tLoss: 0.008809\nTrain Epoch: 196 [400/2800 (14%)]\tLoss: 0.021281\nTrain Epoch: 196 [800/2800 (29%)]\tLoss: 0.009527\nTrain Epoch: 196 [1200/2800 (43%)]\tLoss: 0.007537\nTrain Epoch: 196 [1600/2800 (57%)]\tLoss: 0.017747\nTrain Epoch: 196 [2000/2800 (71%)]\tLoss: 0.012733\nTrain Epoch: 196 [2400/2800 (86%)]\tLoss: 0.009381\n\nevaluating...\nTest set:\tAverage loss: 1.3873, Average CER: 0.226889 Average WER: 0.7338\n\nTrain Epoch: 197 [0/2800 (0%)]\tLoss: 0.026199\nTrain Epoch: 197 [400/2800 (14%)]\tLoss: 0.010553\nTrain Epoch: 197 [800/2800 (29%)]\tLoss: 0.020813\nTrain Epoch: 197 [1200/2800 (43%)]\tLoss: 0.013618\nTrain Epoch: 197 [1600/2800 (57%)]\tLoss: 0.010617\nTrain Epoch: 197 [2000/2800 (71%)]\tLoss: 0.018305\nTrain Epoch: 197 [2400/2800 (86%)]\tLoss: 0.012928\n\nevaluating...\nTest set:\tAverage loss: 1.3818, Average CER: 0.225822 Average WER: 0.7301\n\nTrain Epoch: 198 [0/2800 (0%)]\tLoss: 0.013465\nTrain Epoch: 198 [400/2800 (14%)]\tLoss: 0.011977\nTrain Epoch: 198 [800/2800 (29%)]\tLoss: 0.013148\nTrain Epoch: 198 [1200/2800 (43%)]\tLoss: 0.008641\nTrain Epoch: 198 [1600/2800 (57%)]\tLoss: 0.010108\nTrain Epoch: 198 [2000/2800 (71%)]\tLoss: 0.008520\nTrain Epoch: 198 [2400/2800 (86%)]\tLoss: 0.016668\n\nevaluating...\nTest set:\tAverage loss: 1.3878, Average CER: 0.225062 Average WER: 0.7311\n\nTrain Epoch: 199 [0/2800 (0%)]\tLoss: 0.007327\nTrain Epoch: 199 [400/2800 (14%)]\tLoss: 0.008499\nTrain Epoch: 199 [800/2800 (29%)]\tLoss: 0.007192\nTrain Epoch: 199 [1200/2800 (43%)]\tLoss: 0.012987\nTrain Epoch: 199 [1600/2800 (57%)]\tLoss: 0.007826\nTrain Epoch: 199 [2000/2800 (71%)]\tLoss: 0.005456\nTrain Epoch: 199 [2400/2800 (86%)]\tLoss: 0.009765\n\nevaluating...\nTest set:\tAverage loss: 1.3900, Average CER: 0.226064 Average WER: 0.7330\n\nTrain Epoch: 200 [0/2800 (0%)]\tLoss: 0.002888\nTrain Epoch: 200 [400/2800 (14%)]\tLoss: 0.018240\nTrain Epoch: 200 [800/2800 (29%)]\tLoss: 0.011265\nTrain Epoch: 200 [1200/2800 (43%)]\tLoss: 0.008804\nTrain Epoch: 200 [1600/2800 (57%)]\tLoss: 0.032655\nTrain Epoch: 200 [2000/2800 (71%)]\tLoss: 0.004400\nTrain Epoch: 200 [2400/2800 (86%)]\tLoss: 0.014348\n\nevaluating...\nTest set:\tAverage loss: 1.3904, Average CER: 0.227747 Average WER: 0.7330\n\nCPU times: user 2h 2min 28s, sys: 4min 53s, total: 2h 7min 22s\nWall time: 2h 7min 1s\n","output_type":"stream"}]},{"cell_type":"code","source":"#use_cuda = torch.cuda.is_available()\n#device = torch.device(\"cpu\")\nneeded_device = torch.device(\"cpu\")\nmodel = torch.load('/kaggle/input/dop-test-files/model_for_making_dataset_v6(20024).pt', map_location=torch.device('cpu'))\n\n#1543 1882 1372\n\nmodel.to(needed_device)\nprint(needed_device)\n#predict(model, '/kaggle/input/upd-speech/mono_voice/1964.wav', device)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.003288Z","iopub.status.idle":"2024-06-09T08:55:23.004019Z","shell.execute_reply.started":"2024-06-09T08:55:23.003632Z","shell.execute_reply":"2024-06-09T08:55:23.003668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = {'X_test': X_test, 'label': y_test}\ndf_test = pd.DataFrame(data=d)\ndf_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.005854Z","iopub.status.idle":"2024-06-09T08:55:23.006533Z","shell.execute_reply.started":"2024-06-09T08:55:23.006189Z","shell.execute_reply":"2024-06-09T08:55:23.006228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[:5]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.008255Z","iopub.status.idle":"2024-06-09T08:55:23.008943Z","shell.execute_reply.started":"2024-06-09T08:55:23.008573Z","shell.execute_reply":"2024-06-09T08:55:23.008611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_test_cer(row, model):\n    prediction = predict_with_tensor_v3(model, row['X_test'])\n    return cer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.011078Z","iopub.status.idle":"2024-06-09T08:55:23.011750Z","shell.execute_reply.started":"2024-06-09T08:55:23.011406Z","shell.execute_reply":"2024-06-09T08:55:23.011443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_test_wer(row, model):\n    prediction = predict_with_tensor_v3(model, row['X_test'])\n    return wer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.013505Z","iopub.status.idle":"2024-06-09T08:55:23.014209Z","shell.execute_reply.started":"2024-06-09T08:55:23.013856Z","shell.execute_reply":"2024-06-09T08:55:23.013894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_preds(row, model):\n    return predict_with_tensor_v3(model, row['X_test'])","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.016372Z","iopub.status.idle":"2024-06-09T08:55:23.017052Z","shell.execute_reply.started":"2024-06-09T08:55:23.016688Z","shell.execute_reply":"2024-06-09T08:55:23.016727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['CER'] = df_test.apply(count_test_cer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.018723Z","iopub.status.idle":"2024-06-09T08:55:23.019394Z","shell.execute_reply.started":"2024-06-09T08:55:23.019063Z","shell.execute_reply":"2024-06-09T08:55:23.019099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['WER'] = df_test.apply(count_test_wer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.021600Z","iopub.status.idle":"2024-06-09T08:55:23.022289Z","shell.execute_reply.started":"2024-06-09T08:55:23.021957Z","shell.execute_reply":"2024-06-09T08:55:23.021992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['preds'] = df_test.apply(write_preds, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.024111Z","iopub.status.idle":"2024-06-09T08:55:23.024770Z","shell.execute_reply.started":"2024-06-09T08:55:23.024431Z","shell.execute_reply":"2024-06-09T08:55:23.024466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.loc[df_test['CER'] > 0]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.026421Z","iopub.status.idle":"2024-06-09T08:55:23.027097Z","shell.execute_reply.started":"2024-06-09T08:55:23.026721Z","shell.execute_reply":"2024-06-09T08:55:23.026767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Проверяем затюненый корректор t5 (на 10 эпохах)","metadata":{}},{"cell_type":"code","source":"print('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.029040Z","iopub.status.idle":"2024-06-09T08:55:23.029661Z","shell.execute_reply.started":"2024-06-09T08:55:23.029336Z","shell.execute_reply":"2024-06-09T08:55:23.029369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ИСПОЛЬЗОВАЛ МАЛЫЙ СЛОВАРЬ","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.033020Z","iopub.status.idle":"2024-06-09T08:55:23.033700Z","shell.execute_reply.started":"2024-06-09T08:55:23.033365Z","shell.execute_reply":"2024-06-09T08:55:23.033399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model(3024_seed_data).pt with hunspell. WITHOUT HUNSPELL: CER = Average CER: 0.166791 Average WER: 0.6451","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.036199Z","iopub.status.idle":"2024-06-09T08:55:23.036893Z","shell.execute_reply.started":"2024-06-09T08:55:23.036521Z","shell.execute_reply":"2024-06-09T08:55:23.036558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['CER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.038843Z","iopub.status.idle":"2024-06-09T08:55:23.039513Z","shell.execute_reply.started":"2024-06-09T08:55:23.039180Z","shell.execute_reply":"2024-06-09T08:55:23.039216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['WER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.041668Z","iopub.status.idle":"2024-06-09T08:55:23.042349Z","shell.execute_reply.started":"2024-06-09T08:55:23.042006Z","shell.execute_reply":"2024-06-09T08:55:23.042049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v4(1242).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.044398Z","iopub.status.idle":"2024-06-09T08:55:23.045044Z","shell.execute_reply.started":"2024-06-09T08:55:23.044704Z","shell.execute_reply":"2024-06-09T08:55:23.044735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v5(2204).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.046849Z","iopub.status.idle":"2024-06-09T08:55:23.047441Z","shell.execute_reply.started":"2024-06-09T08:55:23.047144Z","shell.execute_reply":"2024-06-09T08:55:23.047174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v6(20024).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.049422Z","iopub.status.idle":"2024-06-09T08:55:23.050101Z","shell.execute_reply.started":"2024-06-09T08:55:23.049761Z","shell.execute_reply":"2024-06-09T08:55:23.049814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v7(3016).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.052464Z","iopub.status.idle":"2024-06-09T08:55:23.053149Z","shell.execute_reply.started":"2024-06-09T08:55:23.052774Z","shell.execute_reply":"2024-06-09T08:55:23.052834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ДАЛЕЕ ИСПОЛЬЗУЕТСЯ БОЛЬШИЙ СЛОВАРЬ","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.054450Z","iopub.status.idle":"2024-06-09T08:55:23.055087Z","shell.execute_reply.started":"2024-06-09T08:55:23.054737Z","shell.execute_reply":"2024-06-09T08:55:23.054771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v4(1242).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.057101Z","iopub.status.idle":"2024-06-09T08:55:23.057793Z","shell.execute_reply.started":"2024-06-09T08:55:23.057436Z","shell.execute_reply":"2024-06-09T08:55:23.057471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v5(2204).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.059786Z","iopub.status.idle":"2024-06-09T08:55:23.060475Z","shell.execute_reply.started":"2024-06-09T08:55:23.060136Z","shell.execute_reply":"2024-06-09T08:55:23.060172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v6(20024).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.063125Z","iopub.status.idle":"2024-06-09T08:55:23.063903Z","shell.execute_reply.started":"2024-06-09T08:55:23.063448Z","shell.execute_reply":"2024-06-09T08:55:23.063483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v7(3016).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.066083Z","iopub.status.idle":"2024-06-09T08:55:23.066936Z","shell.execute_reply.started":"2024-06-09T08:55:23.066416Z","shell.execute_reply":"2024-06-09T08:55:23.066451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.069448Z","iopub.status.idle":"2024-06-09T08:55:23.070231Z","shell.execute_reply.started":"2024-06-09T08:55:23.069882Z","shell.execute_reply":"2024-06-09T08:55:23.069921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.072319Z","iopub.status.idle":"2024-06-09T08:55:23.073046Z","shell.execute_reply.started":"2024-06-09T08:55:23.072670Z","shell.execute_reply":"2024-06-09T08:55:23.072709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(directory):\n    total_duration = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            filepath = os.path.join(directory, filename)\n            with wave.open(filepath, 'r') as wav_file:\n                frames = wav_file.getnframes()\n                rate = wav_file.getframerate()\n                duration = frames / float(rate)\n                total_duration += duration\n    return total_duration\n\ndirectory = '/kaggle/input/upd-speech/mono_voice'\ntotal_duration = get_wav_duration(directory)\nprint('Total duration of WAV files:', total_duration, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.074771Z","iopub.status.idle":"2024-06-09T08:55:23.075468Z","shell.execute_reply.started":"2024-06-09T08:55:23.075122Z","shell.execute_reply":"2024-06-09T08:55:23.075158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return '{:02d}:{:02d}:{:02d}'.format(int(hours), int(minutes), int(seconds))\nseconds = 3661\nformatted_time = format_time(total_duration)\nprint(formatted_time)  # Output: '01:01:01'","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.076971Z","iopub.status.idle":"2024-06-09T08:55:23.077742Z","shell.execute_reply.started":"2024-06-09T08:55:23.077292Z","shell.execute_reply":"2024-06-09T08:55:23.077326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}