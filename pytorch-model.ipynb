{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5321255,"sourceType":"datasetVersion","datasetId":3091651},{"sourceId":5618710,"sourceType":"datasetVersion","datasetId":3230790},{"sourceId":5677279,"sourceType":"datasetVersion","datasetId":2989949},{"sourceId":5677449,"sourceType":"datasetVersion","datasetId":3071831},{"sourceId":5760288,"sourceType":"datasetVersion","datasetId":3311237},{"sourceId":8003942,"sourceType":"datasetVersion","datasetId":4230886},{"sourceId":8456227,"sourceType":"datasetVersion","datasetId":3213578}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np \nimport matplotlib\nfrom transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:07:38.280581Z","iopub.execute_input":"2024-05-25T17:07:38.281048Z","iopub.status.idle":"2024-05-25T17:07:44.161864Z","shell.execute_reply.started":"2024-05-25T17:07:38.281003Z","shell.execute_reply":"2024-05-25T17:07:44.160267Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        int_sequence = []\n        for c in text:\n            ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MFCC(n_mfcc=20)\n)\n\n\nvalid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//3)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=34, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:07:44.165226Z","iopub.execute_input":"2024-05-25T17:07:44.166248Z","iopub.status.idle":"2024-05-25T17:07:44.332220Z","shell.execute_reply.started":"2024-05-25T17:07:44.166190Z","shell.execute_reply":"2024-05-25T17:07:44.330838Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"class BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:07:44.334096Z","iopub.execute_input":"2024-05-25T17:07:44.334603Z","iopub.status.idle":"2024-05-25T17:07:44.345105Z","shell.execute_reply.started":"2024-05-25T17:07:44.334551Z","shell.execute_reply":"2024-05-25T17:07:44.343864Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Поменял там, где происходит загрузка, сохраняется id звукового файла, а потом в excel файле по колонке old_id ищется текст\n#И того звук и текст к нему\n\nimport pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches v1.xlsx')\n#y = [sentence for sentence in file['text']]\ny = []\ndir_name = \"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in os.listdir(\"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"):\n    file_name = e\n    for old_id in range(0, 2073):\n        if file_name.startswith(str(file['old_id'][old_id]) + '.'):\n            y.extend([''.join(file['text'][old_id])])\n            sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n            sampl = sampl[np.newaxis, :]\n            X.append(torch.Tensor(sampl))\n            break","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:07:44.348699Z","iopub.execute_input":"2024-05-25T17:07:44.349225Z","iopub.status.idle":"2024-05-25T17:09:48.626052Z","shell.execute_reply.started":"2024-05-25T17:07:44.349168Z","shell.execute_reply":"2024-05-25T17:09:48.623936Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import random\npairs = list(zip(X, y))\nrandom.Random(1242).shuffle(pairs)\nX, y = zip(*pairs)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:09:48.628354Z","iopub.execute_input":"2024-05-25T17:09:48.629679Z","iopub.status.idle":"2024-05-25T17:09:48.645184Z","shell.execute_reply.started":"2024-05-25T17:09:48.629612Z","shell.execute_reply":"2024-05-25T17:09:48.643332Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"y[:3]","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:09:48.647197Z","iopub.execute_input":"2024-05-25T17:09:48.647790Z","iopub.status.idle":"2024-05-25T17:09:48.665538Z","shell.execute_reply.started":"2024-05-25T17:09:48.647734Z","shell.execute_reply":"2024-05-25T17:09:48.664048Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('Густая трава', 'Вернувшись в дом', 'Посмотрим кто придет первым')"},"metadata":{}}]},{"cell_type":"code","source":"X[:3]","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:09:48.667197Z","iopub.execute_input":"2024-05-25T17:09:48.667668Z","iopub.status.idle":"2024-05-25T17:09:48.701194Z","shell.execute_reply.started":"2024-05-25T17:09:48.667627Z","shell.execute_reply":"2024-05-25T17:09:48.699889Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0019, 0.0012]]),\n tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.3927e-03, 5.6087e-05,\n          0.0000e+00]]),\n tensor([[ 1.3269e-03,  1.4476e-03,  1.7870e-04,  ..., -3.7689e-01,\n           4.7558e-01, -1.9667e-02]]))"},"metadata":{}}]},{"cell_type":"code","source":"torchaudio.save('/kaggle/working/audio.wav', X[540], 16000)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T14:52:46.826831Z","iopub.execute_input":"2024-04-02T14:52:46.827791Z","iopub.status.idle":"2024-04-02T14:52:46.834792Z","shell.execute_reply.started":"2024-04-02T14:52:46.827746Z","shell.execute_reply":"2024-04-02T14:52:46.833811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"waveform, sample_rate = torchaudio.load('/kaggle/working/audio.wav')  # Загрузка аудиофайла\ntorchaudio.play(waveform, sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:22:15.368540Z","iopub.execute_input":"2024-04-02T07:22:15.368969Z","iopub.status.idle":"2024-04-02T07:22:15.395250Z","shell.execute_reply.started":"2024-04-02T07:22:15.368930Z","shell.execute_reply":"2024-04-02T07:22:15.393666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('4', 'четыре').replace('Р-220', 'р двести двадцать').replace('6', 'шесть').replace(\"-\", \" \")\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    sentence = \" \".join(sentence.split())\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:09:48.704904Z","iopub.execute_input":"2024-05-25T17:09:48.705325Z","iopub.status.idle":"2024-05-25T17:09:48.742849Z","shell.execute_reply.started":"2024-05-25T17:09:48.705275Z","shell.execute_reply":"2024-05-25T17:09:48.741508Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train = X[:2200]\nX_test = X[2200:]\ny_train = y[:2200]\ny_test = y[2200:]","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:09:48.744547Z","iopub.execute_input":"2024-05-25T17:09:48.745294Z","iopub.status.idle":"2024-05-25T17:09:48.759529Z","shell.execute_reply.started":"2024-05-25T17:09:48.745250Z","shell.execute_reply":"2024-05-25T17:09:48.758050Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:09:48.761139Z","iopub.execute_input":"2024-05-25T17:09:48.761585Z","iopub.status.idle":"2024-05-25T17:09:48.773771Z","shell.execute_reply.started":"2024-05-25T17:09:48.761526Z","shell.execute_reply":"2024-05-25T17:09:48.772516Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 32, kernel_size=(4,4), stride=(3,3), padding=(2,2)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.Conv2d(128, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n        )\n        \n        self.fc_1 = nn.Sequential(\n            nn.Linear(896, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n        )\n        \n        self.BiGRU_1 = BidirectionalGRU(270, 270, 0, True)\n        self.BiGRU_2 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_3 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_4 = BidirectionalGRU(540, 270, 0.5, True)\n        \n        self.fc_2 = nn.Sequential(\n            nn.Linear(540, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.fc_1(x)\n        x = self.BiGRU_1(x)\n        x = self.BiGRU_2(x)\n        x = self.BiGRU_3(x)\n        x = self.BiGRU_4(x)\n        x = self.fc_2(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:09:48.775851Z","iopub.execute_input":"2024-05-25T17:09:48.776300Z","iopub.status.idle":"2024-05-25T17:09:48.796906Z","shell.execute_reply.started":"2024-05-25T17:09:48.776258Z","shell.execute_reply":"2024-05-25T17:09:48.795612Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Зададим название выбронной модели из хаба\nMODEL_NAME = 'UrukHan/t5-russian-spell'\nMAX_INPUT = 256\n\n# Загрузка модели и токенизатора\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\ncorrector = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T13:52:10.590361Z","iopub.execute_input":"2024-05-25T13:52:10.590760Z","iopub.status.idle":"2024-05-25T13:52:36.615013Z","shell.execute_reply.started":"2024-05-25T13:52:10.590725Z","shell.execute_reply":"2024-05-25T13:52:36.613682Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f1bed63cff043e2987aa65e6ba704ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/1.00M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae44b9476e8546ce90e337ef3e36fceb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/2.63M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187d7a72ffc348dda6f945084f197158"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de0ece6ab32742e19b73f58cc14b1550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e654e762f90f429fb92fa302030f946c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e543b6fd832c4006908334e452f932b8"}},"metadata":{}}]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        \"\"\"decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        for j in range(len(decoded_preds)):\n            train_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n            train_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n    avg_cer = sum(train_cer)/len(train_cer)\n    avg_wer = sum(train_wer)/len(train_wer)\n            \n    print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(train_loss, avg_cer, avg_wer))\"\"\"\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n            \n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n            \n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n   \n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model_for_correction_test.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:09:48.799403Z","iopub.execute_input":"2024-05-25T17:09:48.799950Z","iopub.status.idle":"2024-05-25T17:09:48.839242Z","shell.execute_reply.started":"2024-05-25T17:09:48.799896Z","shell.execute_reply":"2024-05-25T17:09:48.837855Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict(model, file_name, device):\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    sampl = librosa.load(file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    print(spectrogram_tensor.size())\n\n    with torch.no_grad():\n        spectrogram_tensor.to(device)\n        output = model(spectrogram_tensor)\n        print(output.size())\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:09:48.840958Z","iopub.execute_input":"2024-05-25T17:09:48.841356Z","iopub.status.idle":"2024-05-25T17:09:48.856237Z","shell.execute_reply.started":"2024-05-25T17:09:48.841280Z","shell.execute_reply":"2024-05-25T17:09:48.854705Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict_with_tensor(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    #sampl = librosa.load(file_name, sr=16000)[0]\n    #sampl = sampl[np.newaxis, :]\n    #sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    input_sequences = decodes[0]\n                \n    task_prefix = \"Spell correct: \"\n\n    if type(input_sequences) != list: input_sequences = [input_sequences]\n    encoded = tokenizer(\n      [task_prefix + sequence for sequence in input_sequences],\n      padding=\"longest\",\n      max_length=MAX_INPUT,\n      truncation=True,\n      return_tensors=\"pt\",\n    )\n\n    predicts = corrector.generate(**encoded.to(needed_device))   # # Прогнозирование\n\n    input_sequences = tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]\n    input_sequences = remove_characters(input_sequences)\n\n    return input_sequences\n\n    #return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:09:48.858065Z","iopub.execute_input":"2024-05-25T17:09:48.858611Z","iopub.status.idle":"2024-05-25T17:09:48.876056Z","shell.execute_reply.started":"2024-05-25T17:09:48.858551Z","shell.execute_reply":"2024-05-25T17:09:48.874621Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import hunspell\nimport os\n\n#пробуем hunspell для исправления ошибок\ndef load_hunspell_russian_dict():\n    dict_path = \"/kaggle/input/dop-test-files\"  \n    \n    ru_dic = os.path.join(dict_path, \"ru_RU_big.dic\")\n    ru_aff = os.path.join(dict_path, \"ru_RU_big.aff\")\n    \n    # Create Hunspell instance for Russian\n    hunspell_instance = hunspell.HunSpell(ru_dic, ru_aff)\n    return hunspell_instance\n\ndef correct_mistakes(text, hunspell_instance):\n    corrected_text = []\n    words = text.split()\n    \n    for word in words:\n        if hunspell_instance.spell(word):\n            corrected_text.append(word)\n        else:\n            suggestions = hunspell_instance.suggest(word)\n            if suggestions:\n                corrected_text.append(suggestions[0])  # Choose the first suggestion\n            else:\n                corrected_text.append(word)  # No suggestion, keep the original word\n    \n    return \" \".join(corrected_text)\n\n# Example usage\nhunspell_instance = load_hunspell_russian_dict()\n#text = \"Привет, как дила?\"\n#corrected_text = correct_mistakes(text, hunspell_instance)\n#print(corrected_text)\n\ndef predict_with_tensor_v2(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    corrected_output = correct_mistakes(decodes[0], hunspell_instance)\n\n    #return decodes[0]\n    return corrected_output","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:11:37.506182Z","iopub.execute_input":"2024-05-25T17:11:37.506720Z","iopub.status.idle":"2024-05-25T17:11:37.759633Z","shell.execute_reply.started":"2024-05-25T17:11:37.506674Z","shell.execute_reply":"2024-05-25T17:11:37.758061Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%%time \nlearning_rate = 0.002\nbatch_size = 20\nepochs = 160\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-25T13:52:43.227009Z","iopub.execute_input":"2024-05-25T13:52:43.228156Z","iopub.status.idle":"2024-05-25T15:23:54.890018Z","shell.execute_reply.started":"2024-05-25T13:52:43.228083Z","shell.execute_reply":"2024-05-25T15:23:54.888635Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel1(\n  (conv): Sequential(\n    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Conv2d(1, 32, kernel_size=(4, 4), stride=(3, 3), padding=(2, 2))\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): GELU(approximate='none')\n    (4): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): GELU(approximate='none')\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): GELU(approximate='none')\n  )\n  (fc_1): Sequential(\n    (0): Linear(in_features=896, out_features=270, bias=True)\n    (1): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=270, out_features=270, bias=True)\n    (4): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (5): GELU(approximate='none')\n    (6): Linear(in_features=270, out_features=270, bias=True)\n    (7): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (8): GELU(approximate='none')\n  )\n  (BiGRU_1): BidirectionalGRU(\n    (BiGRU): GRU(270, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_2): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_3): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_4): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (fc_2): Sequential(\n    (0): Linear(in_features=540, out_features=35, bias=True)\n  )\n  (softmax): LogSoftmax(dim=2)\n)\nNum Model Parameters 5422923\nTrain Epoch: 1 [0/2000 (0%)]\tLoss: 16.464399\nTrain Epoch: 1 [400/2000 (20%)]\tLoss: 3.792566\nTrain Epoch: 1 [800/2000 (40%)]\tLoss: 3.343343\nTrain Epoch: 1 [1200/2000 (60%)]\tLoss: 3.464820\nTrain Epoch: 1 [1600/2000 (80%)]\tLoss: 3.342339\n\nevaluating...\nTest set:\tAverage loss: 3.3031, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 2 [0/2000 (0%)]\tLoss: 3.303538\nTrain Epoch: 2 [400/2000 (20%)]\tLoss: 3.344149\nTrain Epoch: 2 [800/2000 (40%)]\tLoss: 3.299348\nTrain Epoch: 2 [1200/2000 (60%)]\tLoss: 3.246386\nTrain Epoch: 2 [1600/2000 (80%)]\tLoss: 3.312886\n\nevaluating...\nTest set:\tAverage loss: 3.2578, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 3 [0/2000 (0%)]\tLoss: 3.246605\nTrain Epoch: 3 [400/2000 (20%)]\tLoss: 3.294458\nTrain Epoch: 3 [800/2000 (40%)]\tLoss: 3.273452\nTrain Epoch: 3 [1200/2000 (60%)]\tLoss: 3.245345\nTrain Epoch: 3 [1600/2000 (80%)]\tLoss: 3.141803\n\nevaluating...\nTest set:\tAverage loss: 3.1061, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 4 [0/2000 (0%)]\tLoss: 3.088724\nTrain Epoch: 4 [400/2000 (20%)]\tLoss: 3.106804\nTrain Epoch: 4 [800/2000 (40%)]\tLoss: 2.885869\nTrain Epoch: 4 [1200/2000 (60%)]\tLoss: 2.823389\nTrain Epoch: 4 [1600/2000 (80%)]\tLoss: 2.682381\n\nevaluating...\nTest set:\tAverage loss: 2.5082, Average CER: 0.999082 Average WER: 1.0000\n\nTrain Epoch: 5 [0/2000 (0%)]\tLoss: 2.574755\nTrain Epoch: 5 [400/2000 (20%)]\tLoss: 2.365363\nTrain Epoch: 5 [800/2000 (40%)]\tLoss: 2.100136\nTrain Epoch: 5 [1200/2000 (60%)]\tLoss: 2.142982\nTrain Epoch: 5 [1600/2000 (80%)]\tLoss: 1.998213\n\nevaluating...\nTest set:\tAverage loss: 1.7073, Average CER: 0.553890 Average WER: 0.9968\n\nTrain Epoch: 6 [0/2000 (0%)]\tLoss: 1.689556\nTrain Epoch: 6 [400/2000 (20%)]\tLoss: 1.820112\nTrain Epoch: 6 [800/2000 (40%)]\tLoss: 1.451786\nTrain Epoch: 6 [1200/2000 (60%)]\tLoss: 1.371725\nTrain Epoch: 6 [1600/2000 (80%)]\tLoss: 1.396482\n\nevaluating...\nTest set:\tAverage loss: 1.3635, Average CER: 0.434316 Average WER: 0.9698\n\nTrain Epoch: 7 [0/2000 (0%)]\tLoss: 1.271043\nTrain Epoch: 7 [400/2000 (20%)]\tLoss: 1.235139\nTrain Epoch: 7 [800/2000 (40%)]\tLoss: 1.189264\nTrain Epoch: 7 [1200/2000 (60%)]\tLoss: 1.095700\nTrain Epoch: 7 [1600/2000 (80%)]\tLoss: 1.220720\n\nevaluating...\nTest set:\tAverage loss: 1.1181, Average CER: 0.349286 Average WER: 0.9503\n\nTrain Epoch: 8 [0/2000 (0%)]\tLoss: 1.063993\nTrain Epoch: 8 [400/2000 (20%)]\tLoss: 0.811691\nTrain Epoch: 8 [800/2000 (40%)]\tLoss: 0.986680\nTrain Epoch: 8 [1200/2000 (60%)]\tLoss: 1.015251\nTrain Epoch: 8 [1600/2000 (80%)]\tLoss: 0.882745\n\nevaluating...\nTest set:\tAverage loss: 1.0453, Average CER: 0.330778 Average WER: 0.9307\n\nTrain Epoch: 9 [0/2000 (0%)]\tLoss: 0.945654\nTrain Epoch: 9 [400/2000 (20%)]\tLoss: 0.982079\nTrain Epoch: 9 [800/2000 (40%)]\tLoss: 0.734829\nTrain Epoch: 9 [1200/2000 (60%)]\tLoss: 0.763892\nTrain Epoch: 9 [1600/2000 (80%)]\tLoss: 0.777257\n\nevaluating...\nTest set:\tAverage loss: 0.9404, Average CER: 0.291694 Average WER: 0.8898\n\nTrain Epoch: 10 [0/2000 (0%)]\tLoss: 0.816679\nTrain Epoch: 10 [400/2000 (20%)]\tLoss: 0.697432\nTrain Epoch: 10 [800/2000 (40%)]\tLoss: 0.840074\nTrain Epoch: 10 [1200/2000 (60%)]\tLoss: 0.651790\nTrain Epoch: 10 [1600/2000 (80%)]\tLoss: 0.862975\n\nevaluating...\nTest set:\tAverage loss: 0.9318, Average CER: 0.286054 Average WER: 0.8879\n\nTrain Epoch: 11 [0/2000 (0%)]\tLoss: 0.457087\nTrain Epoch: 11 [400/2000 (20%)]\tLoss: 0.676248\nTrain Epoch: 11 [800/2000 (40%)]\tLoss: 0.503951\nTrain Epoch: 11 [1200/2000 (60%)]\tLoss: 0.537562\nTrain Epoch: 11 [1600/2000 (80%)]\tLoss: 0.602130\n\nevaluating...\nTest set:\tAverage loss: 0.8635, Average CER: 0.265253 Average WER: 0.8546\n\nTrain Epoch: 12 [0/2000 (0%)]\tLoss: 0.411039\nTrain Epoch: 12 [400/2000 (20%)]\tLoss: 0.528054\nTrain Epoch: 12 [800/2000 (40%)]\tLoss: 0.582163\nTrain Epoch: 12 [1200/2000 (60%)]\tLoss: 0.589403\nTrain Epoch: 12 [1600/2000 (80%)]\tLoss: 0.556322\n\nevaluating...\nTest set:\tAverage loss: 0.8963, Average CER: 0.273478 Average WER: 0.8710\n\nTrain Epoch: 13 [0/2000 (0%)]\tLoss: 0.325391\nTrain Epoch: 13 [400/2000 (20%)]\tLoss: 0.375101\nTrain Epoch: 13 [800/2000 (40%)]\tLoss: 0.487591\nTrain Epoch: 13 [1200/2000 (60%)]\tLoss: 0.492758\nTrain Epoch: 13 [1600/2000 (80%)]\tLoss: 0.424554\n\nevaluating...\nTest set:\tAverage loss: 0.8725, Average CER: 0.254492 Average WER: 0.8441\n\nTrain Epoch: 14 [0/2000 (0%)]\tLoss: 0.392214\nTrain Epoch: 14 [400/2000 (20%)]\tLoss: 0.434471\nTrain Epoch: 14 [800/2000 (40%)]\tLoss: 0.423467\nTrain Epoch: 14 [1200/2000 (60%)]\tLoss: 0.502798\nTrain Epoch: 14 [1600/2000 (80%)]\tLoss: 0.498161\n\nevaluating...\nTest set:\tAverage loss: 0.8864, Average CER: 0.255217 Average WER: 0.8432\n\nTrain Epoch: 15 [0/2000 (0%)]\tLoss: 0.334302\nTrain Epoch: 15 [400/2000 (20%)]\tLoss: 0.371147\nTrain Epoch: 15 [800/2000 (40%)]\tLoss: 0.385801\nTrain Epoch: 15 [1200/2000 (60%)]\tLoss: 0.384424\nTrain Epoch: 15 [1600/2000 (80%)]\tLoss: 0.470776\n\nevaluating...\nTest set:\tAverage loss: 0.8051, Average CER: 0.231805 Average WER: 0.8038\n\nTrain Epoch: 16 [0/2000 (0%)]\tLoss: 0.311150\nTrain Epoch: 16 [400/2000 (20%)]\tLoss: 0.352677\nTrain Epoch: 16 [800/2000 (40%)]\tLoss: 0.331646\nTrain Epoch: 16 [1200/2000 (60%)]\tLoss: 0.364294\nTrain Epoch: 16 [1600/2000 (80%)]\tLoss: 0.479882\n\nevaluating...\nTest set:\tAverage loss: 0.9040, Average CER: 0.303829 Average WER: 0.8991\n\nTrain Epoch: 17 [0/2000 (0%)]\tLoss: 0.271835\nTrain Epoch: 17 [400/2000 (20%)]\tLoss: 0.359912\nTrain Epoch: 17 [800/2000 (40%)]\tLoss: 0.312720\nTrain Epoch: 17 [1200/2000 (60%)]\tLoss: 0.300180\nTrain Epoch: 17 [1600/2000 (80%)]\tLoss: 0.431492\n\nevaluating...\nTest set:\tAverage loss: 0.9311, Average CER: 0.252390 Average WER: 0.8390\n\nTrain Epoch: 18 [0/2000 (0%)]\tLoss: 0.238873\nTrain Epoch: 18 [400/2000 (20%)]\tLoss: 0.240785\nTrain Epoch: 18 [800/2000 (40%)]\tLoss: 0.270793\nTrain Epoch: 18 [1200/2000 (60%)]\tLoss: 0.331641\nTrain Epoch: 18 [1600/2000 (80%)]\tLoss: 0.372275\n\nevaluating...\nTest set:\tAverage loss: 0.9351, Average CER: 0.251335 Average WER: 0.8252\n\nTrain Epoch: 19 [0/2000 (0%)]\tLoss: 0.252295\nTrain Epoch: 19 [400/2000 (20%)]\tLoss: 0.317486\nTrain Epoch: 19 [800/2000 (40%)]\tLoss: 0.364888\nTrain Epoch: 19 [1200/2000 (60%)]\tLoss: 0.249872\nTrain Epoch: 19 [1600/2000 (80%)]\tLoss: 0.301893\n\nevaluating...\nTest set:\tAverage loss: 0.9543, Average CER: 0.244848 Average WER: 0.8413\n\nTrain Epoch: 20 [0/2000 (0%)]\tLoss: 0.256120\nTrain Epoch: 20 [400/2000 (20%)]\tLoss: 0.268992\nTrain Epoch: 20 [800/2000 (40%)]\tLoss: 0.242818\nTrain Epoch: 20 [1200/2000 (60%)]\tLoss: 0.249636\nTrain Epoch: 20 [1600/2000 (80%)]\tLoss: 0.252493\n\nevaluating...\nTest set:\tAverage loss: 0.9207, Average CER: 0.236076 Average WER: 0.8164\n\nTrain Epoch: 21 [0/2000 (0%)]\tLoss: 0.187632\nTrain Epoch: 21 [400/2000 (20%)]\tLoss: 0.198273\nTrain Epoch: 21 [800/2000 (40%)]\tLoss: 0.250760\nTrain Epoch: 21 [1200/2000 (60%)]\tLoss: 0.169983\nTrain Epoch: 21 [1600/2000 (80%)]\tLoss: 0.218833\n\nevaluating...\nTest set:\tAverage loss: 0.9638, Average CER: 0.243599 Average WER: 0.8176\n\nTrain Epoch: 22 [0/2000 (0%)]\tLoss: 0.252073\nTrain Epoch: 22 [400/2000 (20%)]\tLoss: 0.257766\nTrain Epoch: 22 [800/2000 (40%)]\tLoss: 0.165815\nTrain Epoch: 22 [1200/2000 (60%)]\tLoss: 0.267879\nTrain Epoch: 22 [1600/2000 (80%)]\tLoss: 0.210727\n\nevaluating...\nTest set:\tAverage loss: 0.9009, Average CER: 0.223471 Average WER: 0.8277\n\nTrain Epoch: 23 [0/2000 (0%)]\tLoss: 0.124612\nTrain Epoch: 23 [400/2000 (20%)]\tLoss: 0.174916\nTrain Epoch: 23 [800/2000 (40%)]\tLoss: 0.261883\nTrain Epoch: 23 [1200/2000 (60%)]\tLoss: 0.197555\nTrain Epoch: 23 [1600/2000 (80%)]\tLoss: 0.178786\n\nevaluating...\nTest set:\tAverage loss: 0.9173, Average CER: 0.221786 Average WER: 0.7621\n\nTrain Epoch: 24 [0/2000 (0%)]\tLoss: 0.128756\nTrain Epoch: 24 [400/2000 (20%)]\tLoss: 0.183117\nTrain Epoch: 24 [800/2000 (40%)]\tLoss: 0.232835\nTrain Epoch: 24 [1200/2000 (60%)]\tLoss: 0.230939\nTrain Epoch: 24 [1600/2000 (80%)]\tLoss: 0.361751\n\nevaluating...\nTest set:\tAverage loss: 0.9606, Average CER: 0.232729 Average WER: 0.8054\n\nTrain Epoch: 25 [0/2000 (0%)]\tLoss: 0.139391\nTrain Epoch: 25 [400/2000 (20%)]\tLoss: 0.223429\nTrain Epoch: 25 [800/2000 (40%)]\tLoss: 0.150329\nTrain Epoch: 25 [1200/2000 (60%)]\tLoss: 0.161318\nTrain Epoch: 25 [1600/2000 (80%)]\tLoss: 0.282553\n\nevaluating...\nTest set:\tAverage loss: 1.0014, Average CER: 0.253877 Average WER: 0.8469\n\nTrain Epoch: 26 [0/2000 (0%)]\tLoss: 0.241313\nTrain Epoch: 26 [400/2000 (20%)]\tLoss: 0.264203\nTrain Epoch: 26 [800/2000 (40%)]\tLoss: 0.184472\nTrain Epoch: 26 [1200/2000 (60%)]\tLoss: 0.201904\nTrain Epoch: 26 [1600/2000 (80%)]\tLoss: 0.257264\n\nevaluating...\nTest set:\tAverage loss: 0.9693, Average CER: 0.232046 Average WER: 0.8292\n\nTrain Epoch: 27 [0/2000 (0%)]\tLoss: 0.143751\nTrain Epoch: 27 [400/2000 (20%)]\tLoss: 0.172326\nTrain Epoch: 27 [800/2000 (40%)]\tLoss: 0.128490\nTrain Epoch: 27 [1200/2000 (60%)]\tLoss: 0.154920\nTrain Epoch: 27 [1600/2000 (80%)]\tLoss: 0.349308\n\nevaluating...\nTest set:\tAverage loss: 1.0796, Average CER: 0.249872 Average WER: 0.8508\n\nTrain Epoch: 28 [0/2000 (0%)]\tLoss: 0.258413\nTrain Epoch: 28 [400/2000 (20%)]\tLoss: 0.312934\nTrain Epoch: 28 [800/2000 (40%)]\tLoss: 0.145205\nTrain Epoch: 28 [1200/2000 (60%)]\tLoss: 0.230952\nTrain Epoch: 28 [1600/2000 (80%)]\tLoss: 0.194118\n\nevaluating...\nTest set:\tAverage loss: 0.9436, Average CER: 0.218765 Average WER: 0.7723\n\nTrain Epoch: 29 [0/2000 (0%)]\tLoss: 0.144920\nTrain Epoch: 29 [400/2000 (20%)]\tLoss: 0.088512\nTrain Epoch: 29 [800/2000 (40%)]\tLoss: 0.167642\nTrain Epoch: 29 [1200/2000 (60%)]\tLoss: 0.252271\nTrain Epoch: 29 [1600/2000 (80%)]\tLoss: 0.262602\n\nevaluating...\nTest set:\tAverage loss: 1.0452, Average CER: 0.240208 Average WER: 0.8897\n\nTrain Epoch: 30 [0/2000 (0%)]\tLoss: 0.338028\nTrain Epoch: 30 [400/2000 (20%)]\tLoss: 0.210678\nTrain Epoch: 30 [800/2000 (40%)]\tLoss: 0.307881\nTrain Epoch: 30 [1200/2000 (60%)]\tLoss: 0.251947\nTrain Epoch: 30 [1600/2000 (80%)]\tLoss: 0.275237\n\nevaluating...\nTest set:\tAverage loss: 0.9752, Average CER: 0.233544 Average WER: 0.7930\n\nTrain Epoch: 31 [0/2000 (0%)]\tLoss: 0.159590\nTrain Epoch: 31 [400/2000 (20%)]\tLoss: 0.130691\nTrain Epoch: 31 [800/2000 (40%)]\tLoss: 0.158278\nTrain Epoch: 31 [1200/2000 (60%)]\tLoss: 0.125594\nTrain Epoch: 31 [1600/2000 (80%)]\tLoss: 0.195256\n\nevaluating...\nTest set:\tAverage loss: 1.0267, Average CER: 0.227119 Average WER: 0.7817\n\nTrain Epoch: 32 [0/2000 (0%)]\tLoss: 0.168812\nTrain Epoch: 32 [400/2000 (20%)]\tLoss: 0.260473\nTrain Epoch: 32 [800/2000 (40%)]\tLoss: 0.147087\nTrain Epoch: 32 [1200/2000 (60%)]\tLoss: 0.141109\nTrain Epoch: 32 [1600/2000 (80%)]\tLoss: 0.191825\n\nevaluating...\nTest set:\tAverage loss: 1.0256, Average CER: 0.227385 Average WER: 0.7802\n\nTrain Epoch: 33 [0/2000 (0%)]\tLoss: 0.138029\nTrain Epoch: 33 [400/2000 (20%)]\tLoss: 0.140916\nTrain Epoch: 33 [800/2000 (40%)]\tLoss: 0.189075\nTrain Epoch: 33 [1200/2000 (60%)]\tLoss: 0.276880\nTrain Epoch: 33 [1600/2000 (80%)]\tLoss: 0.163073\n\nevaluating...\nTest set:\tAverage loss: 1.0359, Average CER: 0.233005 Average WER: 0.8140\n\nTrain Epoch: 34 [0/2000 (0%)]\tLoss: 0.184328\nTrain Epoch: 34 [400/2000 (20%)]\tLoss: 0.148223\nTrain Epoch: 34 [800/2000 (40%)]\tLoss: 0.174882\nTrain Epoch: 34 [1200/2000 (60%)]\tLoss: 0.149640\nTrain Epoch: 34 [1600/2000 (80%)]\tLoss: 0.190900\n\nevaluating...\nTest set:\tAverage loss: 0.9733, Average CER: 0.230944 Average WER: 0.7869\n\nTrain Epoch: 35 [0/2000 (0%)]\tLoss: 0.281007\nTrain Epoch: 35 [400/2000 (20%)]\tLoss: 0.216177\nTrain Epoch: 35 [800/2000 (40%)]\tLoss: 0.164492\nTrain Epoch: 35 [1200/2000 (60%)]\tLoss: 0.150702\nTrain Epoch: 35 [1600/2000 (80%)]\tLoss: 0.242368\n\nevaluating...\nTest set:\tAverage loss: 1.0528, Average CER: 0.231245 Average WER: 0.7930\n\nTrain Epoch: 36 [0/2000 (0%)]\tLoss: 0.248217\nTrain Epoch: 36 [400/2000 (20%)]\tLoss: 0.207010\nTrain Epoch: 36 [800/2000 (40%)]\tLoss: 0.136911\nTrain Epoch: 36 [1200/2000 (60%)]\tLoss: 0.180876\nTrain Epoch: 36 [1600/2000 (80%)]\tLoss: 0.257254\n\nevaluating...\nTest set:\tAverage loss: 1.0067, Average CER: 0.223015 Average WER: 0.7967\n\nTrain Epoch: 37 [0/2000 (0%)]\tLoss: 0.121500\nTrain Epoch: 37 [400/2000 (20%)]\tLoss: 0.165801\nTrain Epoch: 37 [800/2000 (40%)]\tLoss: 0.205980\nTrain Epoch: 37 [1200/2000 (60%)]\tLoss: 0.206476\nTrain Epoch: 37 [1600/2000 (80%)]\tLoss: 0.224619\n\nevaluating...\nTest set:\tAverage loss: 1.0576, Average CER: 0.233306 Average WER: 0.7697\n\nTrain Epoch: 38 [0/2000 (0%)]\tLoss: 0.179469\nTrain Epoch: 38 [400/2000 (20%)]\tLoss: 0.161801\nTrain Epoch: 38 [800/2000 (40%)]\tLoss: 0.104188\nTrain Epoch: 38 [1200/2000 (60%)]\tLoss: 0.142761\nTrain Epoch: 38 [1600/2000 (80%)]\tLoss: 0.190709\n\nevaluating...\nTest set:\tAverage loss: 1.0023, Average CER: 0.214146 Average WER: 0.7539\n\nTrain Epoch: 39 [0/2000 (0%)]\tLoss: 0.099175\nTrain Epoch: 39 [400/2000 (20%)]\tLoss: 0.080987\nTrain Epoch: 39 [800/2000 (40%)]\tLoss: 0.078612\nTrain Epoch: 39 [1200/2000 (60%)]\tLoss: 0.128395\nTrain Epoch: 39 [1600/2000 (80%)]\tLoss: 0.202755\n\nevaluating...\nTest set:\tAverage loss: 1.0857, Average CER: 0.234546 Average WER: 0.7968\n\nTrain Epoch: 40 [0/2000 (0%)]\tLoss: 0.176938\nTrain Epoch: 40 [400/2000 (20%)]\tLoss: 0.209937\nTrain Epoch: 40 [800/2000 (40%)]\tLoss: 0.119719\nTrain Epoch: 40 [1200/2000 (60%)]\tLoss: 0.080218\nTrain Epoch: 40 [1600/2000 (80%)]\tLoss: 0.113446\n\nevaluating...\nTest set:\tAverage loss: 1.0008, Average CER: 0.215418 Average WER: 0.7652\n\nTrain Epoch: 41 [0/2000 (0%)]\tLoss: 0.122973\nTrain Epoch: 41 [400/2000 (20%)]\tLoss: 0.151546\nTrain Epoch: 41 [800/2000 (40%)]\tLoss: 0.207947\nTrain Epoch: 41 [1200/2000 (60%)]\tLoss: 0.361303\nTrain Epoch: 41 [1600/2000 (80%)]\tLoss: 0.276718\n\nevaluating...\nTest set:\tAverage loss: 1.0387, Average CER: 0.228669 Average WER: 0.8062\n\nTrain Epoch: 42 [0/2000 (0%)]\tLoss: 0.139083\nTrain Epoch: 42 [400/2000 (20%)]\tLoss: 0.231584\nTrain Epoch: 42 [800/2000 (40%)]\tLoss: 0.202032\nTrain Epoch: 42 [1200/2000 (60%)]\tLoss: 0.216336\nTrain Epoch: 42 [1600/2000 (80%)]\tLoss: 0.263466\n\nevaluating...\nTest set:\tAverage loss: 1.0718, Average CER: 0.231480 Average WER: 0.7969\n\nTrain Epoch: 43 [0/2000 (0%)]\tLoss: 0.204690\nTrain Epoch: 43 [400/2000 (20%)]\tLoss: 0.258446\nTrain Epoch: 43 [800/2000 (40%)]\tLoss: 0.224045\nTrain Epoch: 43 [1200/2000 (60%)]\tLoss: 0.256400\nTrain Epoch: 43 [1600/2000 (80%)]\tLoss: 0.243033\n\nevaluating...\nTest set:\tAverage loss: 1.0835, Average CER: 0.257263 Average WER: 0.8135\n\nTrain Epoch: 44 [0/2000 (0%)]\tLoss: 0.253669\nTrain Epoch: 44 [400/2000 (20%)]\tLoss: 0.145483\nTrain Epoch: 44 [800/2000 (40%)]\tLoss: 0.129418\nTrain Epoch: 44 [1200/2000 (60%)]\tLoss: 0.253941\nTrain Epoch: 44 [1600/2000 (80%)]\tLoss: 0.300564\n\nevaluating...\nTest set:\tAverage loss: 0.9632, Average CER: 0.219650 Average WER: 0.7695\n\nTrain Epoch: 45 [0/2000 (0%)]\tLoss: 0.155424\nTrain Epoch: 45 [400/2000 (20%)]\tLoss: 0.161838\nTrain Epoch: 45 [800/2000 (40%)]\tLoss: 0.137208\nTrain Epoch: 45 [1200/2000 (60%)]\tLoss: 0.181824\nTrain Epoch: 45 [1600/2000 (80%)]\tLoss: 0.154239\n\nevaluating...\nTest set:\tAverage loss: 1.0060, Average CER: 0.221072 Average WER: 0.7660\n\nTrain Epoch: 46 [0/2000 (0%)]\tLoss: 0.152075\nTrain Epoch: 46 [400/2000 (20%)]\tLoss: 0.169161\nTrain Epoch: 46 [800/2000 (40%)]\tLoss: 0.214786\nTrain Epoch: 46 [1200/2000 (60%)]\tLoss: 0.099004\nTrain Epoch: 46 [1600/2000 (80%)]\tLoss: 0.222235\n\nevaluating...\nTest set:\tAverage loss: 0.9617, Average CER: 0.206019 Average WER: 0.7434\n\nTrain Epoch: 47 [0/2000 (0%)]\tLoss: 0.073093\nTrain Epoch: 47 [400/2000 (20%)]\tLoss: 0.094573\nTrain Epoch: 47 [800/2000 (40%)]\tLoss: 0.109187\nTrain Epoch: 47 [1200/2000 (60%)]\tLoss: 0.090843\nTrain Epoch: 47 [1600/2000 (80%)]\tLoss: 0.169994\n\nevaluating...\nTest set:\tAverage loss: 1.1225, Average CER: 0.228636 Average WER: 0.7816\n\nTrain Epoch: 48 [0/2000 (0%)]\tLoss: 0.230732\nTrain Epoch: 48 [400/2000 (20%)]\tLoss: 0.165551\nTrain Epoch: 48 [800/2000 (40%)]\tLoss: 0.143577\nTrain Epoch: 48 [1200/2000 (60%)]\tLoss: 0.153923\nTrain Epoch: 48 [1600/2000 (80%)]\tLoss: 0.150368\n\nevaluating...\nTest set:\tAverage loss: 1.0339, Average CER: 0.219961 Average WER: 0.7601\n\nTrain Epoch: 49 [0/2000 (0%)]\tLoss: 0.291381\nTrain Epoch: 49 [400/2000 (20%)]\tLoss: 0.327558\nTrain Epoch: 49 [800/2000 (40%)]\tLoss: 0.261854\nTrain Epoch: 49 [1200/2000 (60%)]\tLoss: 0.255294\nTrain Epoch: 49 [1600/2000 (80%)]\tLoss: 0.198274\n\nevaluating...\nTest set:\tAverage loss: 0.9551, Average CER: 0.214577 Average WER: 0.7372\n\nTrain Epoch: 50 [0/2000 (0%)]\tLoss: 0.114646\nTrain Epoch: 50 [400/2000 (20%)]\tLoss: 0.089216\nTrain Epoch: 50 [800/2000 (40%)]\tLoss: 0.167458\nTrain Epoch: 50 [1200/2000 (60%)]\tLoss: 0.099758\nTrain Epoch: 50 [1600/2000 (80%)]\tLoss: 0.141507\n\nevaluating...\nTest set:\tAverage loss: 1.0011, Average CER: 0.208419 Average WER: 0.7553\n\nTrain Epoch: 51 [0/2000 (0%)]\tLoss: 0.050726\nTrain Epoch: 51 [400/2000 (20%)]\tLoss: 0.192346\nTrain Epoch: 51 [800/2000 (40%)]\tLoss: 0.086796\nTrain Epoch: 51 [1200/2000 (60%)]\tLoss: 0.203298\nTrain Epoch: 51 [1600/2000 (80%)]\tLoss: 0.102680\n\nevaluating...\nTest set:\tAverage loss: 1.0392, Average CER: 0.220532 Average WER: 0.7823\n\nTrain Epoch: 52 [0/2000 (0%)]\tLoss: 0.130361\nTrain Epoch: 52 [400/2000 (20%)]\tLoss: 0.109534\nTrain Epoch: 52 [800/2000 (40%)]\tLoss: 0.134561\nTrain Epoch: 52 [1200/2000 (60%)]\tLoss: 0.099486\nTrain Epoch: 52 [1600/2000 (80%)]\tLoss: 0.132797\n\nevaluating...\nTest set:\tAverage loss: 1.0638, Average CER: 0.206442 Average WER: 0.7439\n\nTrain Epoch: 53 [0/2000 (0%)]\tLoss: 0.074626\nTrain Epoch: 53 [400/2000 (20%)]\tLoss: 0.123281\nTrain Epoch: 53 [800/2000 (40%)]\tLoss: 0.075929\nTrain Epoch: 53 [1200/2000 (60%)]\tLoss: 0.044019\nTrain Epoch: 53 [1600/2000 (80%)]\tLoss: 0.088123\n\nevaluating...\nTest set:\tAverage loss: 1.0831, Average CER: 0.201897 Average WER: 0.7392\n\nTrain Epoch: 54 [0/2000 (0%)]\tLoss: 0.064762\nTrain Epoch: 54 [400/2000 (20%)]\tLoss: 0.054282\nTrain Epoch: 54 [800/2000 (40%)]\tLoss: 0.097918\nTrain Epoch: 54 [1200/2000 (60%)]\tLoss: 0.116149\nTrain Epoch: 54 [1600/2000 (80%)]\tLoss: 0.081478\n\nevaluating...\nTest set:\tAverage loss: 1.1333, Average CER: 0.205570 Average WER: 0.7372\n\nTrain Epoch: 55 [0/2000 (0%)]\tLoss: 0.093065\nTrain Epoch: 55 [400/2000 (20%)]\tLoss: 0.124849\nTrain Epoch: 55 [800/2000 (40%)]\tLoss: 0.136167\nTrain Epoch: 55 [1200/2000 (60%)]\tLoss: 0.100244\nTrain Epoch: 55 [1600/2000 (80%)]\tLoss: 0.125675\n\nevaluating...\nTest set:\tAverage loss: 1.0501, Average CER: 0.214908 Average WER: 0.7598\n\nTrain Epoch: 56 [0/2000 (0%)]\tLoss: 0.111486\nTrain Epoch: 56 [400/2000 (20%)]\tLoss: 0.121327\nTrain Epoch: 56 [800/2000 (40%)]\tLoss: 0.335894\nTrain Epoch: 56 [1200/2000 (60%)]\tLoss: 0.239321\nTrain Epoch: 56 [1600/2000 (80%)]\tLoss: 0.318980\n\nevaluating...\nTest set:\tAverage loss: 1.0559, Average CER: 0.227221 Average WER: 0.7685\n\nTrain Epoch: 57 [0/2000 (0%)]\tLoss: 0.297943\nTrain Epoch: 57 [400/2000 (20%)]\tLoss: 0.121654\nTrain Epoch: 57 [800/2000 (40%)]\tLoss: 0.238013\nTrain Epoch: 57 [1200/2000 (60%)]\tLoss: 0.188753\nTrain Epoch: 57 [1600/2000 (80%)]\tLoss: 0.186846\n\nevaluating...\nTest set:\tAverage loss: 0.9985, Average CER: 0.206360 Average WER: 0.7314\n\nTrain Epoch: 58 [0/2000 (0%)]\tLoss: 0.099503\nTrain Epoch: 58 [400/2000 (20%)]\tLoss: 0.081704\nTrain Epoch: 58 [800/2000 (40%)]\tLoss: 0.077146\nTrain Epoch: 58 [1200/2000 (60%)]\tLoss: 0.106949\nTrain Epoch: 58 [1600/2000 (80%)]\tLoss: 0.122903\n\nevaluating...\nTest set:\tAverage loss: 1.0080, Average CER: 0.205172 Average WER: 0.7375\n\nTrain Epoch: 59 [0/2000 (0%)]\tLoss: 0.090624\nTrain Epoch: 59 [400/2000 (20%)]\tLoss: 0.042493\nTrain Epoch: 59 [800/2000 (40%)]\tLoss: 0.075977\nTrain Epoch: 59 [1200/2000 (60%)]\tLoss: 0.044925\nTrain Epoch: 59 [1600/2000 (80%)]\tLoss: 0.057295\n\nevaluating...\nTest set:\tAverage loss: 1.0695, Average CER: 0.195589 Average WER: 0.7083\n\nTrain Epoch: 60 [0/2000 (0%)]\tLoss: 0.111823\nTrain Epoch: 60 [400/2000 (20%)]\tLoss: 0.125792\nTrain Epoch: 60 [800/2000 (40%)]\tLoss: 0.085828\nTrain Epoch: 60 [1200/2000 (60%)]\tLoss: 0.035915\nTrain Epoch: 60 [1600/2000 (80%)]\tLoss: 0.038563\n\nevaluating...\nTest set:\tAverage loss: 1.0915, Average CER: 0.193240 Average WER: 0.7217\n\nTrain Epoch: 61 [0/2000 (0%)]\tLoss: 0.072477\nTrain Epoch: 61 [400/2000 (20%)]\tLoss: 0.094954\nTrain Epoch: 61 [800/2000 (40%)]\tLoss: 0.101776\nTrain Epoch: 61 [1200/2000 (60%)]\tLoss: 0.113627\nTrain Epoch: 61 [1600/2000 (80%)]\tLoss: 0.035313\n\nevaluating...\nTest set:\tAverage loss: 1.0811, Average CER: 0.189772 Average WER: 0.7100\n\nTrain Epoch: 62 [0/2000 (0%)]\tLoss: 0.046587\nTrain Epoch: 62 [400/2000 (20%)]\tLoss: 0.040224\nTrain Epoch: 62 [800/2000 (40%)]\tLoss: 0.053136\nTrain Epoch: 62 [1200/2000 (60%)]\tLoss: 0.053836\nTrain Epoch: 62 [1600/2000 (80%)]\tLoss: 0.035633\n\nevaluating...\nTest set:\tAverage loss: 1.1625, Average CER: 0.196864 Average WER: 0.7112\n\nTrain Epoch: 63 [0/2000 (0%)]\tLoss: 0.042759\nTrain Epoch: 63 [400/2000 (20%)]\tLoss: 0.009507\nTrain Epoch: 63 [800/2000 (40%)]\tLoss: 0.132179\nTrain Epoch: 63 [1200/2000 (60%)]\tLoss: 0.124439\nTrain Epoch: 63 [1600/2000 (80%)]\tLoss: 0.083081\n\nevaluating...\nTest set:\tAverage loss: 1.1025, Average CER: 0.201699 Average WER: 0.7455\n\nTrain Epoch: 64 [0/2000 (0%)]\tLoss: 0.093466\nTrain Epoch: 64 [400/2000 (20%)]\tLoss: 0.067727\nTrain Epoch: 64 [800/2000 (40%)]\tLoss: 0.117404\nTrain Epoch: 64 [1200/2000 (60%)]\tLoss: 0.109597\nTrain Epoch: 64 [1600/2000 (80%)]\tLoss: 0.132662\n\nevaluating...\nTest set:\tAverage loss: 1.1415, Average CER: 0.208275 Average WER: 0.7293\n\nTrain Epoch: 65 [0/2000 (0%)]\tLoss: 0.077617\nTrain Epoch: 65 [400/2000 (20%)]\tLoss: 0.141643\nTrain Epoch: 65 [800/2000 (40%)]\tLoss: 0.092427\nTrain Epoch: 65 [1200/2000 (60%)]\tLoss: 0.142648\nTrain Epoch: 65 [1600/2000 (80%)]\tLoss: 0.090047\n\nevaluating...\nTest set:\tAverage loss: 1.1960, Average CER: 0.216336 Average WER: 0.7595\n\nTrain Epoch: 66 [0/2000 (0%)]\tLoss: 0.104932\nTrain Epoch: 66 [400/2000 (20%)]\tLoss: 0.100273\nTrain Epoch: 66 [800/2000 (40%)]\tLoss: 0.137958\nTrain Epoch: 66 [1200/2000 (60%)]\tLoss: 0.163861\nTrain Epoch: 66 [1600/2000 (80%)]\tLoss: 0.161905\n\nevaluating...\nTest set:\tAverage loss: 1.0737, Average CER: 0.201494 Average WER: 0.7226\n\nTrain Epoch: 67 [0/2000 (0%)]\tLoss: 0.062059\nTrain Epoch: 67 [400/2000 (20%)]\tLoss: 0.055680\nTrain Epoch: 67 [800/2000 (40%)]\tLoss: 0.059987\nTrain Epoch: 67 [1200/2000 (60%)]\tLoss: 0.074548\nTrain Epoch: 67 [1600/2000 (80%)]\tLoss: 0.079185\n\nevaluating...\nTest set:\tAverage loss: 1.1149, Average CER: 0.200891 Average WER: 0.7339\n\nTrain Epoch: 68 [0/2000 (0%)]\tLoss: 0.020045\nTrain Epoch: 68 [400/2000 (20%)]\tLoss: 0.042177\nTrain Epoch: 68 [800/2000 (40%)]\tLoss: 0.081990\nTrain Epoch: 68 [1200/2000 (60%)]\tLoss: 0.028266\nTrain Epoch: 68 [1600/2000 (80%)]\tLoss: 0.063735\n\nevaluating...\nTest set:\tAverage loss: 1.0598, Average CER: 0.191756 Average WER: 0.7077\n\nTrain Epoch: 69 [0/2000 (0%)]\tLoss: 0.050662\nTrain Epoch: 69 [400/2000 (20%)]\tLoss: 0.032103\nTrain Epoch: 69 [800/2000 (40%)]\tLoss: 0.049225\nTrain Epoch: 69 [1200/2000 (60%)]\tLoss: 0.026186\nTrain Epoch: 69 [1600/2000 (80%)]\tLoss: 0.015652\n\nevaluating...\nTest set:\tAverage loss: 1.1004, Average CER: 0.185103 Average WER: 0.6916\n\nTrain Epoch: 70 [0/2000 (0%)]\tLoss: 0.022104\nTrain Epoch: 70 [400/2000 (20%)]\tLoss: 0.015118\nTrain Epoch: 70 [800/2000 (40%)]\tLoss: 0.024934\nTrain Epoch: 70 [1200/2000 (60%)]\tLoss: 0.048682\nTrain Epoch: 70 [1600/2000 (80%)]\tLoss: 0.032149\n\nevaluating...\nTest set:\tAverage loss: 1.1384, Average CER: 0.182822 Average WER: 0.6884\n\nTrain Epoch: 71 [0/2000 (0%)]\tLoss: 0.012204\nTrain Epoch: 71 [400/2000 (20%)]\tLoss: 0.013702\nTrain Epoch: 71 [800/2000 (40%)]\tLoss: 0.031431\nTrain Epoch: 71 [1200/2000 (60%)]\tLoss: 0.019314\nTrain Epoch: 71 [1600/2000 (80%)]\tLoss: 0.026602\n\nevaluating...\nTest set:\tAverage loss: 1.2249, Average CER: 0.206040 Average WER: 0.7555\n\nTrain Epoch: 72 [0/2000 (0%)]\tLoss: 0.051114\nTrain Epoch: 72 [400/2000 (20%)]\tLoss: 0.047495\nTrain Epoch: 72 [800/2000 (40%)]\tLoss: 0.026888\nTrain Epoch: 72 [1200/2000 (60%)]\tLoss: 0.127962\nTrain Epoch: 72 [1600/2000 (80%)]\tLoss: 0.111058\n\nevaluating...\nTest set:\tAverage loss: 1.2047, Average CER: 0.206187 Average WER: 0.7365\n\nTrain Epoch: 73 [0/2000 (0%)]\tLoss: 0.080957\nTrain Epoch: 73 [400/2000 (20%)]\tLoss: 0.073374\nTrain Epoch: 73 [800/2000 (40%)]\tLoss: 0.136196\nTrain Epoch: 73 [1200/2000 (60%)]\tLoss: 0.197110\nTrain Epoch: 73 [1600/2000 (80%)]\tLoss: 0.167508\n\nevaluating...\nTest set:\tAverage loss: 1.1438, Average CER: 0.205586 Average WER: 0.7360\n\nTrain Epoch: 74 [0/2000 (0%)]\tLoss: 0.086885\nTrain Epoch: 74 [400/2000 (20%)]\tLoss: 0.064219\nTrain Epoch: 74 [800/2000 (40%)]\tLoss: 0.130330\nTrain Epoch: 74 [1200/2000 (60%)]\tLoss: 0.115536\nTrain Epoch: 74 [1600/2000 (80%)]\tLoss: 0.110009\n\nevaluating...\nTest set:\tAverage loss: 1.1145, Average CER: 0.197022 Average WER: 0.7171\n\nTrain Epoch: 75 [0/2000 (0%)]\tLoss: 0.026220\nTrain Epoch: 75 [400/2000 (20%)]\tLoss: 0.066332\nTrain Epoch: 75 [800/2000 (40%)]\tLoss: 0.022368\nTrain Epoch: 75 [1200/2000 (60%)]\tLoss: 0.126503\nTrain Epoch: 75 [1600/2000 (80%)]\tLoss: 0.066397\n\nevaluating...\nTest set:\tAverage loss: 1.1882, Average CER: 0.196042 Average WER: 0.7084\n\nTrain Epoch: 76 [0/2000 (0%)]\tLoss: 0.036447\nTrain Epoch: 76 [400/2000 (20%)]\tLoss: 0.107549\nTrain Epoch: 76 [800/2000 (40%)]\tLoss: 0.037105\nTrain Epoch: 76 [1200/2000 (60%)]\tLoss: 0.061980\nTrain Epoch: 76 [1600/2000 (80%)]\tLoss: 0.096293\n\nevaluating...\nTest set:\tAverage loss: 1.1338, Average CER: 0.197547 Average WER: 0.7169\n\nTrain Epoch: 77 [0/2000 (0%)]\tLoss: 0.031106\nTrain Epoch: 77 [400/2000 (20%)]\tLoss: 0.028060\nTrain Epoch: 77 [800/2000 (40%)]\tLoss: 0.031327\nTrain Epoch: 77 [1200/2000 (60%)]\tLoss: 0.047294\nTrain Epoch: 77 [1600/2000 (80%)]\tLoss: 0.055060\n\nevaluating...\nTest set:\tAverage loss: 1.1263, Average CER: 0.191613 Average WER: 0.7198\n\nTrain Epoch: 78 [0/2000 (0%)]\tLoss: 0.028626\nTrain Epoch: 78 [400/2000 (20%)]\tLoss: 0.021348\nTrain Epoch: 78 [800/2000 (40%)]\tLoss: 0.033001\nTrain Epoch: 78 [1200/2000 (60%)]\tLoss: 0.061841\nTrain Epoch: 78 [1600/2000 (80%)]\tLoss: 0.101345\n\nevaluating...\nTest set:\tAverage loss: 1.1768, Average CER: 0.190260 Average WER: 0.7012\n\nTrain Epoch: 79 [0/2000 (0%)]\tLoss: 0.025445\nTrain Epoch: 79 [400/2000 (20%)]\tLoss: 0.017610\nTrain Epoch: 79 [800/2000 (40%)]\tLoss: 0.056170\nTrain Epoch: 79 [1200/2000 (60%)]\tLoss: 0.052466\nTrain Epoch: 79 [1600/2000 (80%)]\tLoss: 0.051864\n\nevaluating...\nTest set:\tAverage loss: 1.2028, Average CER: 0.195800 Average WER: 0.7077\n\nTrain Epoch: 80 [0/2000 (0%)]\tLoss: 0.082062\nTrain Epoch: 80 [400/2000 (20%)]\tLoss: 0.046628\nTrain Epoch: 80 [800/2000 (40%)]\tLoss: 0.023874\nTrain Epoch: 80 [1200/2000 (60%)]\tLoss: 0.083802\nTrain Epoch: 80 [1600/2000 (80%)]\tLoss: 0.063560\n\nevaluating...\nTest set:\tAverage loss: 1.1935, Average CER: 0.198123 Average WER: 0.7379\n\nTrain Epoch: 81 [0/2000 (0%)]\tLoss: 0.063654\nTrain Epoch: 81 [400/2000 (20%)]\tLoss: 0.026648\nTrain Epoch: 81 [800/2000 (40%)]\tLoss: 0.065009\nTrain Epoch: 81 [1200/2000 (60%)]\tLoss: 0.077082\nTrain Epoch: 81 [1600/2000 (80%)]\tLoss: 0.032832\n\nevaluating...\nTest set:\tAverage loss: 1.1895, Average CER: 0.194742 Average WER: 0.7182\n\nTrain Epoch: 82 [0/2000 (0%)]\tLoss: 0.055522\nTrain Epoch: 82 [400/2000 (20%)]\tLoss: 0.077108\nTrain Epoch: 82 [800/2000 (40%)]\tLoss: 0.052842\nTrain Epoch: 82 [1200/2000 (60%)]\tLoss: 0.110695\nTrain Epoch: 82 [1600/2000 (80%)]\tLoss: 0.058596\n\nevaluating...\nTest set:\tAverage loss: 1.1410, Average CER: 0.198222 Average WER: 0.7178\n\nTrain Epoch: 83 [0/2000 (0%)]\tLoss: 0.044099\nTrain Epoch: 83 [400/2000 (20%)]\tLoss: 0.052802\nTrain Epoch: 83 [800/2000 (40%)]\tLoss: 0.065285\nTrain Epoch: 83 [1200/2000 (60%)]\tLoss: 0.063504\nTrain Epoch: 83 [1600/2000 (80%)]\tLoss: 0.062741\n\nevaluating...\nTest set:\tAverage loss: 1.2812, Average CER: 0.201345 Average WER: 0.7238\n\nTrain Epoch: 84 [0/2000 (0%)]\tLoss: 0.092625\nTrain Epoch: 84 [400/2000 (20%)]\tLoss: 0.047985\nTrain Epoch: 84 [800/2000 (40%)]\tLoss: 0.068970\nTrain Epoch: 84 [1200/2000 (60%)]\tLoss: 0.050353\nTrain Epoch: 84 [1600/2000 (80%)]\tLoss: 0.024812\n\nevaluating...\nTest set:\tAverage loss: 1.1070, Average CER: 0.182178 Average WER: 0.7002\n\nTrain Epoch: 85 [0/2000 (0%)]\tLoss: 0.017225\nTrain Epoch: 85 [400/2000 (20%)]\tLoss: 0.019931\nTrain Epoch: 85 [800/2000 (40%)]\tLoss: 0.022672\nTrain Epoch: 85 [1200/2000 (60%)]\tLoss: 0.022998\nTrain Epoch: 85 [1600/2000 (80%)]\tLoss: 0.017747\n\nevaluating...\nTest set:\tAverage loss: 1.1559, Average CER: 0.187549 Average WER: 0.7024\n\nTrain Epoch: 86 [0/2000 (0%)]\tLoss: 0.011862\nTrain Epoch: 86 [400/2000 (20%)]\tLoss: 0.035151\nTrain Epoch: 86 [800/2000 (40%)]\tLoss: 0.029896\nTrain Epoch: 86 [1200/2000 (60%)]\tLoss: 0.026840\nTrain Epoch: 86 [1600/2000 (80%)]\tLoss: 0.011264\n\nevaluating...\nTest set:\tAverage loss: 1.1473, Average CER: 0.177917 Average WER: 0.6812\n\nTrain Epoch: 87 [0/2000 (0%)]\tLoss: 0.009908\nTrain Epoch: 87 [400/2000 (20%)]\tLoss: 0.006290\nTrain Epoch: 87 [800/2000 (40%)]\tLoss: 0.008860\nTrain Epoch: 87 [1200/2000 (60%)]\tLoss: 0.005561\nTrain Epoch: 87 [1600/2000 (80%)]\tLoss: 0.014580\n\nevaluating...\nTest set:\tAverage loss: 1.1903, Average CER: 0.177203 Average WER: 0.6933\n\nTrain Epoch: 88 [0/2000 (0%)]\tLoss: 0.005441\nTrain Epoch: 88 [400/2000 (20%)]\tLoss: 0.001654\nTrain Epoch: 88 [800/2000 (40%)]\tLoss: 0.013816\nTrain Epoch: 88 [1200/2000 (60%)]\tLoss: 0.009775\nTrain Epoch: 88 [1600/2000 (80%)]\tLoss: 0.018713\n\nevaluating...\nTest set:\tAverage loss: 1.2498, Average CER: 0.181137 Average WER: 0.6870\n\nTrain Epoch: 89 [0/2000 (0%)]\tLoss: 0.033397\nTrain Epoch: 89 [400/2000 (20%)]\tLoss: 0.003478\nTrain Epoch: 89 [800/2000 (40%)]\tLoss: 0.011062\nTrain Epoch: 89 [1200/2000 (60%)]\tLoss: 0.009558\nTrain Epoch: 89 [1600/2000 (80%)]\tLoss: 0.004025\n\nevaluating...\nTest set:\tAverage loss: 1.2133, Average CER: 0.172039 Average WER: 0.6644\n\nTrain Epoch: 90 [0/2000 (0%)]\tLoss: 0.003484\nTrain Epoch: 90 [400/2000 (20%)]\tLoss: 0.021028\nTrain Epoch: 90 [800/2000 (40%)]\tLoss: 0.003380\nTrain Epoch: 90 [1200/2000 (60%)]\tLoss: 0.002687\nTrain Epoch: 90 [1600/2000 (80%)]\tLoss: 0.006035\n\nevaluating...\nTest set:\tAverage loss: 1.2255, Average CER: 0.172132 Average WER: 0.6571\n\nTrain Epoch: 91 [0/2000 (0%)]\tLoss: 0.000796\nTrain Epoch: 91 [400/2000 (20%)]\tLoss: 0.004064\nTrain Epoch: 91 [800/2000 (40%)]\tLoss: 0.000574\nTrain Epoch: 91 [1200/2000 (60%)]\tLoss: 0.002785\nTrain Epoch: 91 [1600/2000 (80%)]\tLoss: 0.002442\n\nevaluating...\nTest set:\tAverage loss: 1.2221, Average CER: 0.171908 Average WER: 0.6650\n\nTrain Epoch: 92 [0/2000 (0%)]\tLoss: 0.001558\nTrain Epoch: 92 [400/2000 (20%)]\tLoss: 0.001910\nTrain Epoch: 92 [800/2000 (40%)]\tLoss: 0.002313\nTrain Epoch: 92 [1200/2000 (60%)]\tLoss: 0.001784\nTrain Epoch: 92 [1600/2000 (80%)]\tLoss: 0.001247\n\nevaluating...\nTest set:\tAverage loss: 1.3222, Average CER: 0.179760 Average WER: 0.6855\n\nTrain Epoch: 93 [0/2000 (0%)]\tLoss: 0.055795\nTrain Epoch: 93 [400/2000 (20%)]\tLoss: 0.043620\nTrain Epoch: 93 [800/2000 (40%)]\tLoss: 0.063254\nTrain Epoch: 93 [1200/2000 (60%)]\tLoss: 0.075402\nTrain Epoch: 93 [1600/2000 (80%)]\tLoss: 0.090403\n\nevaluating...\nTest set:\tAverage loss: 1.3187, Average CER: 0.207333 Average WER: 0.7321\n\nTrain Epoch: 94 [0/2000 (0%)]\tLoss: 0.109356\nTrain Epoch: 94 [400/2000 (20%)]\tLoss: 0.080274\nTrain Epoch: 94 [800/2000 (40%)]\tLoss: 0.034964\nTrain Epoch: 94 [1200/2000 (60%)]\tLoss: 0.157656\nTrain Epoch: 94 [1600/2000 (80%)]\tLoss: 0.103673\n\nevaluating...\nTest set:\tAverage loss: 1.2667, Average CER: 0.201108 Average WER: 0.7154\n\nTrain Epoch: 95 [0/2000 (0%)]\tLoss: 0.074649\nTrain Epoch: 95 [400/2000 (20%)]\tLoss: 0.060362\nTrain Epoch: 95 [800/2000 (40%)]\tLoss: 0.045363\nTrain Epoch: 95 [1200/2000 (60%)]\tLoss: 0.083458\nTrain Epoch: 95 [1600/2000 (80%)]\tLoss: 0.021924\n\nevaluating...\nTest set:\tAverage loss: 1.1542, Average CER: 0.189556 Average WER: 0.6981\n\nTrain Epoch: 96 [0/2000 (0%)]\tLoss: 0.050072\nTrain Epoch: 96 [400/2000 (20%)]\tLoss: 0.083452\nTrain Epoch: 96 [800/2000 (40%)]\tLoss: 0.045249\nTrain Epoch: 96 [1200/2000 (60%)]\tLoss: 0.066757\nTrain Epoch: 96 [1600/2000 (80%)]\tLoss: 0.030657\n\nevaluating...\nTest set:\tAverage loss: 1.1250, Average CER: 0.178632 Average WER: 0.6698\n\nTrain Epoch: 97 [0/2000 (0%)]\tLoss: 0.041074\nTrain Epoch: 97 [400/2000 (20%)]\tLoss: 0.028799\nTrain Epoch: 97 [800/2000 (40%)]\tLoss: 0.025246\nTrain Epoch: 97 [1200/2000 (60%)]\tLoss: 0.039374\nTrain Epoch: 97 [1600/2000 (80%)]\tLoss: 0.086843\n\nevaluating...\nTest set:\tAverage loss: 1.2383, Average CER: 0.192726 Average WER: 0.7116\n\nTrain Epoch: 98 [0/2000 (0%)]\tLoss: 0.048279\nTrain Epoch: 98 [400/2000 (20%)]\tLoss: 0.060536\nTrain Epoch: 98 [800/2000 (40%)]\tLoss: 0.074439\nTrain Epoch: 98 [1200/2000 (60%)]\tLoss: 0.046042\nTrain Epoch: 98 [1600/2000 (80%)]\tLoss: 0.056795\n\nevaluating...\nTest set:\tAverage loss: 1.2174, Average CER: 0.193238 Average WER: 0.7023\n\nTrain Epoch: 99 [0/2000 (0%)]\tLoss: 0.016652\nTrain Epoch: 99 [400/2000 (20%)]\tLoss: 0.038889\nTrain Epoch: 99 [800/2000 (40%)]\tLoss: 0.029898\nTrain Epoch: 99 [1200/2000 (60%)]\tLoss: 0.088277\nTrain Epoch: 99 [1600/2000 (80%)]\tLoss: 0.052893\n\nevaluating...\nTest set:\tAverage loss: 1.1724, Average CER: 0.186007 Average WER: 0.6816\n\nTrain Epoch: 100 [0/2000 (0%)]\tLoss: 0.014145\nTrain Epoch: 100 [400/2000 (20%)]\tLoss: 0.023250\nTrain Epoch: 100 [800/2000 (40%)]\tLoss: 0.019560\nTrain Epoch: 100 [1200/2000 (60%)]\tLoss: 0.017896\nTrain Epoch: 100 [1600/2000 (80%)]\tLoss: 0.018673\n\nevaluating...\nTest set:\tAverage loss: 1.2008, Average CER: 0.180211 Average WER: 0.6825\n\nTrain Epoch: 101 [0/2000 (0%)]\tLoss: 0.010256\nTrain Epoch: 101 [400/2000 (20%)]\tLoss: 0.015993\nTrain Epoch: 101 [800/2000 (40%)]\tLoss: 0.005732\nTrain Epoch: 101 [1200/2000 (60%)]\tLoss: 0.035164\nTrain Epoch: 101 [1600/2000 (80%)]\tLoss: 0.033066\n\nevaluating...\nTest set:\tAverage loss: 1.2033, Average CER: 0.178909 Average WER: 0.6655\n\nTrain Epoch: 102 [0/2000 (0%)]\tLoss: 0.006282\nTrain Epoch: 102 [400/2000 (20%)]\tLoss: 0.003302\nTrain Epoch: 102 [800/2000 (40%)]\tLoss: 0.007705\nTrain Epoch: 102 [1200/2000 (60%)]\tLoss: 0.025175\nTrain Epoch: 102 [1600/2000 (80%)]\tLoss: 0.001350\n\nevaluating...\nTest set:\tAverage loss: 1.2238, Average CER: 0.174077 Average WER: 0.6560\n\nTrain Epoch: 103 [0/2000 (0%)]\tLoss: 0.004618\nTrain Epoch: 103 [400/2000 (20%)]\tLoss: 0.001593\nTrain Epoch: 103 [800/2000 (40%)]\tLoss: 0.010381\nTrain Epoch: 103 [1200/2000 (60%)]\tLoss: 0.020798\nTrain Epoch: 103 [1600/2000 (80%)]\tLoss: 0.020628\n\nevaluating...\nTest set:\tAverage loss: 1.2606, Average CER: 0.177984 Average WER: 0.6749\n\nTrain Epoch: 104 [0/2000 (0%)]\tLoss: 0.006478\nTrain Epoch: 104 [400/2000 (20%)]\tLoss: 0.009512\nTrain Epoch: 104 [800/2000 (40%)]\tLoss: 0.013586\nTrain Epoch: 104 [1200/2000 (60%)]\tLoss: 0.002112\nTrain Epoch: 104 [1600/2000 (80%)]\tLoss: 0.006837\n\nevaluating...\nTest set:\tAverage loss: 1.2785, Average CER: 0.175156 Average WER: 0.6599\n\nTrain Epoch: 105 [0/2000 (0%)]\tLoss: 0.002116\nTrain Epoch: 105 [400/2000 (20%)]\tLoss: 0.001979\nTrain Epoch: 105 [800/2000 (40%)]\tLoss: 0.001126\nTrain Epoch: 105 [1200/2000 (60%)]\tLoss: 0.012942\nTrain Epoch: 105 [1600/2000 (80%)]\tLoss: 0.001105\n\nevaluating...\nTest set:\tAverage loss: 1.2523, Average CER: 0.171373 Average WER: 0.6614\n\nTrain Epoch: 106 [0/2000 (0%)]\tLoss: 0.001873\nTrain Epoch: 106 [400/2000 (20%)]\tLoss: 0.001659\nTrain Epoch: 106 [800/2000 (40%)]\tLoss: 0.002909\nTrain Epoch: 106 [1200/2000 (60%)]\tLoss: 0.001150\nTrain Epoch: 106 [1600/2000 (80%)]\tLoss: 0.003057\n\nevaluating...\nTest set:\tAverage loss: 1.2401, Average CER: 0.170622 Average WER: 0.6535\n\nTrain Epoch: 107 [0/2000 (0%)]\tLoss: 0.002219\nTrain Epoch: 107 [400/2000 (20%)]\tLoss: 0.000807\nTrain Epoch: 107 [800/2000 (40%)]\tLoss: 0.001017\nTrain Epoch: 107 [1200/2000 (60%)]\tLoss: 0.000930\nTrain Epoch: 107 [1600/2000 (80%)]\tLoss: 0.000712\n\nevaluating...\nTest set:\tAverage loss: 1.2818, Average CER: 0.169344 Average WER: 0.6485\n\nTrain Epoch: 108 [0/2000 (0%)]\tLoss: 0.004188\nTrain Epoch: 108 [400/2000 (20%)]\tLoss: 0.002012\nTrain Epoch: 108 [800/2000 (40%)]\tLoss: 0.002094\nTrain Epoch: 108 [1200/2000 (60%)]\tLoss: 0.001391\nTrain Epoch: 108 [1600/2000 (80%)]\tLoss: 0.001172\n\nevaluating...\nTest set:\tAverage loss: 1.2803, Average CER: 0.168131 Average WER: 0.6448\n\nTrain Epoch: 109 [0/2000 (0%)]\tLoss: 0.001859\nTrain Epoch: 109 [400/2000 (20%)]\tLoss: 0.001488\nTrain Epoch: 109 [800/2000 (40%)]\tLoss: 0.009770\nTrain Epoch: 109 [1200/2000 (60%)]\tLoss: 0.001171\nTrain Epoch: 109 [1600/2000 (80%)]\tLoss: 0.000468\n\nevaluating...\nTest set:\tAverage loss: 1.2947, Average CER: 0.167652 Average WER: 0.6437\n\nTrain Epoch: 110 [0/2000 (0%)]\tLoss: 0.000915\nTrain Epoch: 110 [400/2000 (20%)]\tLoss: 0.000492\nTrain Epoch: 110 [800/2000 (40%)]\tLoss: 0.000208\nTrain Epoch: 110 [1200/2000 (60%)]\tLoss: 0.000419\nTrain Epoch: 110 [1600/2000 (80%)]\tLoss: 0.000515\n\nevaluating...\nTest set:\tAverage loss: 1.3071, Average CER: 0.165717 Average WER: 0.6426\n\nTrain Epoch: 111 [0/2000 (0%)]\tLoss: 0.000495\nTrain Epoch: 111 [400/2000 (20%)]\tLoss: 0.000251\nTrain Epoch: 111 [800/2000 (40%)]\tLoss: 0.000371\nTrain Epoch: 111 [1200/2000 (60%)]\tLoss: 0.000972\nTrain Epoch: 111 [1600/2000 (80%)]\tLoss: 0.000355\n\nevaluating...\nTest set:\tAverage loss: 1.2970, Average CER: 0.165338 Average WER: 0.6450\n\nTrain Epoch: 112 [0/2000 (0%)]\tLoss: 0.000257\nTrain Epoch: 112 [400/2000 (20%)]\tLoss: 0.000169\nTrain Epoch: 112 [800/2000 (40%)]\tLoss: 0.000280\nTrain Epoch: 112 [1200/2000 (60%)]\tLoss: 0.000142\nTrain Epoch: 112 [1600/2000 (80%)]\tLoss: 0.000210\n\nevaluating...\nTest set:\tAverage loss: 1.2995, Average CER: 0.163404 Average WER: 0.6352\n\nTrain Epoch: 113 [0/2000 (0%)]\tLoss: 0.000255\nTrain Epoch: 113 [400/2000 (20%)]\tLoss: 0.000079\nTrain Epoch: 113 [800/2000 (40%)]\tLoss: 0.000474\nTrain Epoch: 113 [1200/2000 (60%)]\tLoss: 0.000151\nTrain Epoch: 113 [1600/2000 (80%)]\tLoss: 0.000148\n\nevaluating...\nTest set:\tAverage loss: 1.3019, Average CER: 0.162924 Average WER: 0.6366\n\nTrain Epoch: 114 [0/2000 (0%)]\tLoss: 0.000165\nTrain Epoch: 114 [400/2000 (20%)]\tLoss: 0.000111\nTrain Epoch: 114 [800/2000 (40%)]\tLoss: 0.000182\nTrain Epoch: 114 [1200/2000 (60%)]\tLoss: 0.000157\nTrain Epoch: 114 [1600/2000 (80%)]\tLoss: 0.000141\n\nevaluating...\nTest set:\tAverage loss: 1.3049, Average CER: 0.163474 Average WER: 0.6357\n\nTrain Epoch: 115 [0/2000 (0%)]\tLoss: 0.000162\nTrain Epoch: 115 [400/2000 (20%)]\tLoss: 0.000179\nTrain Epoch: 115 [800/2000 (40%)]\tLoss: 0.000083\nTrain Epoch: 115 [1200/2000 (60%)]\tLoss: 0.000167\nTrain Epoch: 115 [1600/2000 (80%)]\tLoss: 0.000180\n\nevaluating...\nTest set:\tAverage loss: 1.3056, Average CER: 0.163774 Average WER: 0.6371\n\nTrain Epoch: 116 [0/2000 (0%)]\tLoss: 0.000509\nTrain Epoch: 116 [400/2000 (20%)]\tLoss: 0.000212\nTrain Epoch: 116 [800/2000 (40%)]\tLoss: 0.000069\nTrain Epoch: 116 [1200/2000 (60%)]\tLoss: 0.000127\nTrain Epoch: 116 [1600/2000 (80%)]\tLoss: 0.000136\n\nevaluating...\nTest set:\tAverage loss: 1.3114, Average CER: 0.163691 Average WER: 0.6361\n\nTrain Epoch: 117 [0/2000 (0%)]\tLoss: 0.000092\nTrain Epoch: 117 [400/2000 (20%)]\tLoss: 0.000069\nTrain Epoch: 117 [800/2000 (40%)]\tLoss: 0.000342\nTrain Epoch: 117 [1200/2000 (60%)]\tLoss: 0.000104\nTrain Epoch: 117 [1600/2000 (80%)]\tLoss: 0.000160\n\nevaluating...\nTest set:\tAverage loss: 1.3094, Average CER: 0.162310 Average WER: 0.6329\n\nTrain Epoch: 118 [0/2000 (0%)]\tLoss: 0.000129\nTrain Epoch: 118 [400/2000 (20%)]\tLoss: 0.000088\nTrain Epoch: 118 [800/2000 (40%)]\tLoss: 0.000074\nTrain Epoch: 118 [1200/2000 (60%)]\tLoss: 0.000048\nTrain Epoch: 118 [1600/2000 (80%)]\tLoss: 0.000186\n\nevaluating...\nTest set:\tAverage loss: 1.3139, Average CER: 0.162947 Average WER: 0.6369\n\nTrain Epoch: 119 [0/2000 (0%)]\tLoss: 0.000099\nTrain Epoch: 119 [400/2000 (20%)]\tLoss: 0.000087\nTrain Epoch: 119 [800/2000 (40%)]\tLoss: 0.000156\nTrain Epoch: 119 [1200/2000 (60%)]\tLoss: 0.000102\nTrain Epoch: 119 [1600/2000 (80%)]\tLoss: 0.000121\n\nevaluating...\nTest set:\tAverage loss: 1.3199, Average CER: 0.162295 Average WER: 0.6348\n\nTrain Epoch: 120 [0/2000 (0%)]\tLoss: 0.000107\nTrain Epoch: 120 [400/2000 (20%)]\tLoss: 0.000167\nTrain Epoch: 120 [800/2000 (40%)]\tLoss: 0.000223\nTrain Epoch: 120 [1200/2000 (60%)]\tLoss: 0.000057\nTrain Epoch: 120 [1600/2000 (80%)]\tLoss: 0.000070\n\nevaluating...\nTest set:\tAverage loss: 1.3200, Average CER: 0.162347 Average WER: 0.6364\n\nTrain Epoch: 121 [0/2000 (0%)]\tLoss: 0.000049\nTrain Epoch: 121 [400/2000 (20%)]\tLoss: 0.000101\nTrain Epoch: 121 [800/2000 (40%)]\tLoss: 0.000171\nTrain Epoch: 121 [1200/2000 (60%)]\tLoss: 0.000065\nTrain Epoch: 121 [1600/2000 (80%)]\tLoss: 0.000110\n\nevaluating...\nTest set:\tAverage loss: 1.3254, Average CER: 0.163115 Average WER: 0.6355\n\nTrain Epoch: 122 [0/2000 (0%)]\tLoss: 0.000146\nTrain Epoch: 122 [400/2000 (20%)]\tLoss: 0.000062\nTrain Epoch: 122 [800/2000 (40%)]\tLoss: 0.000109\nTrain Epoch: 122 [1200/2000 (60%)]\tLoss: 0.000135\nTrain Epoch: 122 [1600/2000 (80%)]\tLoss: 0.000077\n\nevaluating...\nTest set:\tAverage loss: 1.3296, Average CER: 0.163688 Average WER: 0.6393\n\nTrain Epoch: 123 [0/2000 (0%)]\tLoss: 0.000187\nTrain Epoch: 123 [400/2000 (20%)]\tLoss: 0.000067\nTrain Epoch: 123 [800/2000 (40%)]\tLoss: 0.000047\nTrain Epoch: 123 [1200/2000 (60%)]\tLoss: 0.000090\nTrain Epoch: 123 [1600/2000 (80%)]\tLoss: 0.000094\n\nevaluating...\nTest set:\tAverage loss: 1.3306, Average CER: 0.163063 Average WER: 0.6369\n\nTrain Epoch: 124 [0/2000 (0%)]\tLoss: 0.000090\nTrain Epoch: 124 [400/2000 (20%)]\tLoss: 0.000112\nTrain Epoch: 124 [800/2000 (40%)]\tLoss: 0.000090\nTrain Epoch: 124 [1200/2000 (60%)]\tLoss: 0.000065\nTrain Epoch: 124 [1600/2000 (80%)]\tLoss: 0.000094\n\nevaluating...\nTest set:\tAverage loss: 1.3246, Average CER: 0.162711 Average WER: 0.6335\n\nTrain Epoch: 125 [0/2000 (0%)]\tLoss: 0.000071\nTrain Epoch: 125 [400/2000 (20%)]\tLoss: 0.000060\nTrain Epoch: 125 [800/2000 (40%)]\tLoss: 0.000096\nTrain Epoch: 125 [1200/2000 (60%)]\tLoss: 0.000061\nTrain Epoch: 125 [1600/2000 (80%)]\tLoss: 0.000072\n\nevaluating...\nTest set:\tAverage loss: 1.3334, Average CER: 0.163183 Average WER: 0.6363\n\nTrain Epoch: 126 [0/2000 (0%)]\tLoss: 0.000060\nTrain Epoch: 126 [400/2000 (20%)]\tLoss: 0.000075\nTrain Epoch: 126 [800/2000 (40%)]\tLoss: 0.000076\nTrain Epoch: 126 [1200/2000 (60%)]\tLoss: 0.000055\nTrain Epoch: 126 [1600/2000 (80%)]\tLoss: 0.000076\n\nevaluating...\nTest set:\tAverage loss: 1.3329, Average CER: 0.162924 Average WER: 0.6354\n\nTrain Epoch: 127 [0/2000 (0%)]\tLoss: 0.000111\nTrain Epoch: 127 [400/2000 (20%)]\tLoss: 0.000092\nTrain Epoch: 127 [800/2000 (40%)]\tLoss: 0.000026\nTrain Epoch: 127 [1200/2000 (60%)]\tLoss: 0.000035\nTrain Epoch: 127 [1600/2000 (80%)]\tLoss: 0.000043\n\nevaluating...\nTest set:\tAverage loss: 1.3376, Average CER: 0.162478 Average WER: 0.6354\n\nTrain Epoch: 128 [0/2000 (0%)]\tLoss: 0.000053\nTrain Epoch: 128 [400/2000 (20%)]\tLoss: 0.000229\nTrain Epoch: 128 [800/2000 (40%)]\tLoss: 0.000072\nTrain Epoch: 128 [1200/2000 (60%)]\tLoss: 0.000183\nTrain Epoch: 128 [1600/2000 (80%)]\tLoss: 0.000047\n\nevaluating...\nTest set:\tAverage loss: 1.3385, Average CER: 0.162629 Average WER: 0.6339\n\nTrain Epoch: 129 [0/2000 (0%)]\tLoss: 0.000037\nTrain Epoch: 129 [400/2000 (20%)]\tLoss: 0.000046\nTrain Epoch: 129 [800/2000 (40%)]\tLoss: 0.000078\nTrain Epoch: 129 [1200/2000 (60%)]\tLoss: 0.000046\nTrain Epoch: 129 [1600/2000 (80%)]\tLoss: 0.000046\n\nevaluating...\nTest set:\tAverage loss: 1.3385, Average CER: 0.163061 Average WER: 0.6353\n\nTrain Epoch: 130 [0/2000 (0%)]\tLoss: 0.000178\nTrain Epoch: 130 [400/2000 (20%)]\tLoss: 0.000122\nTrain Epoch: 130 [800/2000 (40%)]\tLoss: 0.000070\nTrain Epoch: 130 [1200/2000 (60%)]\tLoss: 0.000047\nTrain Epoch: 130 [1600/2000 (80%)]\tLoss: 0.000056\n\nevaluating...\nTest set:\tAverage loss: 1.3412, Average CER: 0.162772 Average WER: 0.6355\n\nTrain Epoch: 131 [0/2000 (0%)]\tLoss: 0.000044\nTrain Epoch: 131 [400/2000 (20%)]\tLoss: 0.000089\nTrain Epoch: 131 [800/2000 (40%)]\tLoss: 0.000078\nTrain Epoch: 131 [1200/2000 (60%)]\tLoss: 0.000078\nTrain Epoch: 131 [1600/2000 (80%)]\tLoss: 0.000106\n\nevaluating...\nTest set:\tAverage loss: 1.3456, Average CER: 0.163338 Average WER: 0.6375\n\nTrain Epoch: 132 [0/2000 (0%)]\tLoss: 0.000094\nTrain Epoch: 132 [400/2000 (20%)]\tLoss: 0.000036\nTrain Epoch: 132 [800/2000 (40%)]\tLoss: 0.000064\nTrain Epoch: 132 [1200/2000 (60%)]\tLoss: 0.000071\nTrain Epoch: 132 [1600/2000 (80%)]\tLoss: 0.000060\n\nevaluating...\nTest set:\tAverage loss: 1.3477, Average CER: 0.163085 Average WER: 0.6365\n\nTrain Epoch: 133 [0/2000 (0%)]\tLoss: 0.000043\nTrain Epoch: 133 [400/2000 (20%)]\tLoss: 0.000039\nTrain Epoch: 133 [800/2000 (40%)]\tLoss: 0.000088\nTrain Epoch: 133 [1200/2000 (60%)]\tLoss: 0.000028\nTrain Epoch: 133 [1600/2000 (80%)]\tLoss: 0.000040\n\nevaluating...\nTest set:\tAverage loss: 1.3508, Average CER: 0.162142 Average WER: 0.6338\n\nTrain Epoch: 134 [0/2000 (0%)]\tLoss: 0.000062\nTrain Epoch: 134 [400/2000 (20%)]\tLoss: 0.000064\nTrain Epoch: 134 [800/2000 (40%)]\tLoss: 0.000055\nTrain Epoch: 134 [1200/2000 (60%)]\tLoss: 0.000050\nTrain Epoch: 134 [1600/2000 (80%)]\tLoss: 0.000079\n\nevaluating...\nTest set:\tAverage loss: 1.3531, Average CER: 0.162460 Average WER: 0.6322\n\nTrain Epoch: 135 [0/2000 (0%)]\tLoss: 0.000163\nTrain Epoch: 135 [400/2000 (20%)]\tLoss: 0.000092\nTrain Epoch: 135 [800/2000 (40%)]\tLoss: 0.000058\nTrain Epoch: 135 [1200/2000 (60%)]\tLoss: 0.000063\nTrain Epoch: 135 [1600/2000 (80%)]\tLoss: 0.000047\n\nevaluating...\nTest set:\tAverage loss: 1.3534, Average CER: 0.162104 Average WER: 0.6359\n\nTrain Epoch: 136 [0/2000 (0%)]\tLoss: 0.000072\nTrain Epoch: 136 [400/2000 (20%)]\tLoss: 0.000045\nTrain Epoch: 136 [800/2000 (40%)]\tLoss: 0.000043\nTrain Epoch: 136 [1200/2000 (60%)]\tLoss: 0.000050\nTrain Epoch: 136 [1600/2000 (80%)]\tLoss: 0.000046\n\nevaluating...\nTest set:\tAverage loss: 1.3517, Average CER: 0.161956 Average WER: 0.6347\n\nTrain Epoch: 137 [0/2000 (0%)]\tLoss: 0.000060\nTrain Epoch: 137 [400/2000 (20%)]\tLoss: 0.000058\nTrain Epoch: 137 [800/2000 (40%)]\tLoss: 0.000044\nTrain Epoch: 137 [1200/2000 (60%)]\tLoss: 0.000050\nTrain Epoch: 137 [1600/2000 (80%)]\tLoss: 0.000094\n\nevaluating...\nTest set:\tAverage loss: 1.3574, Average CER: 0.162715 Average WER: 0.6373\n\nTrain Epoch: 138 [0/2000 (0%)]\tLoss: 0.000043\nTrain Epoch: 138 [400/2000 (20%)]\tLoss: 0.000054\nTrain Epoch: 138 [800/2000 (40%)]\tLoss: 0.000073\nTrain Epoch: 138 [1200/2000 (60%)]\tLoss: 0.000119\nTrain Epoch: 138 [1600/2000 (80%)]\tLoss: 0.000033\n\nevaluating...\nTest set:\tAverage loss: 1.3575, Average CER: 0.162666 Average WER: 0.6362\n\nTrain Epoch: 139 [0/2000 (0%)]\tLoss: 0.000082\nTrain Epoch: 139 [400/2000 (20%)]\tLoss: 0.000068\nTrain Epoch: 139 [800/2000 (40%)]\tLoss: 0.000035\nTrain Epoch: 139 [1200/2000 (60%)]\tLoss: 0.000072\nTrain Epoch: 139 [1600/2000 (80%)]\tLoss: 0.000029\n\nevaluating...\nTest set:\tAverage loss: 1.3601, Average CER: 0.161928 Average WER: 0.6374\n\nTrain Epoch: 140 [0/2000 (0%)]\tLoss: 0.000070\nTrain Epoch: 140 [400/2000 (20%)]\tLoss: 0.000050\nTrain Epoch: 140 [800/2000 (40%)]\tLoss: 0.000090\nTrain Epoch: 140 [1200/2000 (60%)]\tLoss: 0.003535\nTrain Epoch: 140 [1600/2000 (80%)]\tLoss: 0.000068\n\nevaluating...\nTest set:\tAverage loss: 1.4059, Average CER: 0.166457 Average WER: 0.6556\n\nTrain Epoch: 141 [0/2000 (0%)]\tLoss: 0.000119\nTrain Epoch: 141 [400/2000 (20%)]\tLoss: 0.000086\nTrain Epoch: 141 [800/2000 (40%)]\tLoss: 0.001217\nTrain Epoch: 141 [1200/2000 (60%)]\tLoss: 0.013607\nTrain Epoch: 141 [1600/2000 (80%)]\tLoss: 0.003673\n\nevaluating...\nTest set:\tAverage loss: 1.4819, Average CER: 0.188383 Average WER: 0.6971\n\nTrain Epoch: 142 [0/2000 (0%)]\tLoss: 0.015594\nTrain Epoch: 142 [400/2000 (20%)]\tLoss: 0.021449\nTrain Epoch: 142 [800/2000 (40%)]\tLoss: 0.004351\nTrain Epoch: 142 [1200/2000 (60%)]\tLoss: 0.018277\nTrain Epoch: 142 [1600/2000 (80%)]\tLoss: 0.017293\n\nevaluating...\nTest set:\tAverage loss: 1.3761, Average CER: 0.183438 Average WER: 0.7016\n\nTrain Epoch: 143 [0/2000 (0%)]\tLoss: 0.012029\nTrain Epoch: 143 [400/2000 (20%)]\tLoss: 0.003311\nTrain Epoch: 143 [800/2000 (40%)]\tLoss: 0.008022\nTrain Epoch: 143 [1200/2000 (60%)]\tLoss: 0.018650\nTrain Epoch: 143 [1600/2000 (80%)]\tLoss: 0.001780\n\nevaluating...\nTest set:\tAverage loss: 1.3514, Average CER: 0.178617 Average WER: 0.6811\n\nTrain Epoch: 144 [0/2000 (0%)]\tLoss: 0.001130\nTrain Epoch: 144 [400/2000 (20%)]\tLoss: 0.001804\nTrain Epoch: 144 [800/2000 (40%)]\tLoss: 0.000541\nTrain Epoch: 144 [1200/2000 (60%)]\tLoss: 0.000574\nTrain Epoch: 144 [1600/2000 (80%)]\tLoss: 0.001836\n\nevaluating...\nTest set:\tAverage loss: 1.3263, Average CER: 0.170982 Average WER: 0.6592\n\nTrain Epoch: 145 [0/2000 (0%)]\tLoss: 0.002202\nTrain Epoch: 145 [400/2000 (20%)]\tLoss: 0.000523\nTrain Epoch: 145 [800/2000 (40%)]\tLoss: 0.001117\nTrain Epoch: 145 [1200/2000 (60%)]\tLoss: 0.001106\nTrain Epoch: 145 [1600/2000 (80%)]\tLoss: 0.001108\n\nevaluating...\nTest set:\tAverage loss: 1.3096, Average CER: 0.171167 Average WER: 0.6533\n\nTrain Epoch: 146 [0/2000 (0%)]\tLoss: 0.005763\nTrain Epoch: 146 [400/2000 (20%)]\tLoss: 0.001222\nTrain Epoch: 146 [800/2000 (40%)]\tLoss: 0.000310\nTrain Epoch: 146 [1200/2000 (60%)]\tLoss: 0.001690\nTrain Epoch: 146 [1600/2000 (80%)]\tLoss: 0.000305\n\nevaluating...\nTest set:\tAverage loss: 1.3109, Average CER: 0.169535 Average WER: 0.6502\n\nTrain Epoch: 147 [0/2000 (0%)]\tLoss: 0.000327\nTrain Epoch: 147 [400/2000 (20%)]\tLoss: 0.000261\nTrain Epoch: 147 [800/2000 (40%)]\tLoss: 0.000173\nTrain Epoch: 147 [1200/2000 (60%)]\tLoss: 0.000118\nTrain Epoch: 147 [1600/2000 (80%)]\tLoss: 0.000226\n\nevaluating...\nTest set:\tAverage loss: 1.3083, Average CER: 0.167764 Average WER: 0.6471\n\nTrain Epoch: 148 [0/2000 (0%)]\tLoss: 0.000318\nTrain Epoch: 148 [400/2000 (20%)]\tLoss: 0.000189\nTrain Epoch: 148 [800/2000 (40%)]\tLoss: 0.000212\nTrain Epoch: 148 [1200/2000 (60%)]\tLoss: 0.000254\nTrain Epoch: 148 [1600/2000 (80%)]\tLoss: 0.000942\n\nevaluating...\nTest set:\tAverage loss: 1.3142, Average CER: 0.166553 Average WER: 0.6436\n\nTrain Epoch: 149 [0/2000 (0%)]\tLoss: 0.000272\nTrain Epoch: 149 [400/2000 (20%)]\tLoss: 0.000193\nTrain Epoch: 149 [800/2000 (40%)]\tLoss: 0.000174\nTrain Epoch: 149 [1200/2000 (60%)]\tLoss: 0.000129\nTrain Epoch: 149 [1600/2000 (80%)]\tLoss: 0.000188\n\nevaluating...\nTest set:\tAverage loss: 1.3141, Average CER: 0.167387 Average WER: 0.6475\n\nTrain Epoch: 150 [0/2000 (0%)]\tLoss: 0.000079\nTrain Epoch: 150 [400/2000 (20%)]\tLoss: 0.000920\nTrain Epoch: 150 [800/2000 (40%)]\tLoss: 0.000199\nTrain Epoch: 150 [1200/2000 (60%)]\tLoss: 0.000301\nTrain Epoch: 150 [1600/2000 (80%)]\tLoss: 0.000261\n\nevaluating...\nTest set:\tAverage loss: 1.3163, Average CER: 0.168272 Average WER: 0.6492\n\nTrain Epoch: 151 [0/2000 (0%)]\tLoss: 0.000112\nTrain Epoch: 151 [400/2000 (20%)]\tLoss: 0.000205\nTrain Epoch: 151 [800/2000 (40%)]\tLoss: 0.000180\nTrain Epoch: 151 [1200/2000 (60%)]\tLoss: 0.000130\nTrain Epoch: 151 [1600/2000 (80%)]\tLoss: 0.000119\n\nevaluating...\nTest set:\tAverage loss: 1.3176, Average CER: 0.167766 Average WER: 0.6448\n\nTrain Epoch: 152 [0/2000 (0%)]\tLoss: 0.000408\nTrain Epoch: 152 [400/2000 (20%)]\tLoss: 0.000080\nTrain Epoch: 152 [800/2000 (40%)]\tLoss: 0.000079\nTrain Epoch: 152 [1200/2000 (60%)]\tLoss: 0.000133\nTrain Epoch: 152 [1600/2000 (80%)]\tLoss: 0.000164\n\nevaluating...\nTest set:\tAverage loss: 1.3204, Average CER: 0.167141 Average WER: 0.6435\n\nTrain Epoch: 153 [0/2000 (0%)]\tLoss: 0.000139\nTrain Epoch: 153 [400/2000 (20%)]\tLoss: 0.000157\nTrain Epoch: 153 [800/2000 (40%)]\tLoss: 0.000124\nTrain Epoch: 153 [1200/2000 (60%)]\tLoss: 0.000119\nTrain Epoch: 153 [1600/2000 (80%)]\tLoss: 0.000391\n\nevaluating...\nTest set:\tAverage loss: 1.3236, Average CER: 0.167534 Average WER: 0.6465\n\nTrain Epoch: 154 [0/2000 (0%)]\tLoss: 0.000098\nTrain Epoch: 154 [400/2000 (20%)]\tLoss: 0.000150\nTrain Epoch: 154 [800/2000 (40%)]\tLoss: 0.000216\nTrain Epoch: 154 [1200/2000 (60%)]\tLoss: 0.000099\nTrain Epoch: 154 [1600/2000 (80%)]\tLoss: 0.000491\n\nevaluating...\nTest set:\tAverage loss: 1.3250, Average CER: 0.167201 Average WER: 0.6466\n\nTrain Epoch: 155 [0/2000 (0%)]\tLoss: 0.000082\nTrain Epoch: 155 [400/2000 (20%)]\tLoss: 0.000085\nTrain Epoch: 155 [800/2000 (40%)]\tLoss: 0.000105\nTrain Epoch: 155 [1200/2000 (60%)]\tLoss: 0.000094\nTrain Epoch: 155 [1600/2000 (80%)]\tLoss: 0.000254\n\nevaluating...\nTest set:\tAverage loss: 1.3273, Average CER: 0.167204 Average WER: 0.6448\n\nTrain Epoch: 156 [0/2000 (0%)]\tLoss: 0.000183\nTrain Epoch: 156 [400/2000 (20%)]\tLoss: 0.000048\nTrain Epoch: 156 [800/2000 (40%)]\tLoss: 0.000257\nTrain Epoch: 156 [1200/2000 (60%)]\tLoss: 0.000172\nTrain Epoch: 156 [1600/2000 (80%)]\tLoss: 0.000068\n\nevaluating...\nTest set:\tAverage loss: 1.3265, Average CER: 0.167154 Average WER: 0.6456\n\nTrain Epoch: 157 [0/2000 (0%)]\tLoss: 0.000107\nTrain Epoch: 157 [400/2000 (20%)]\tLoss: 0.000139\nTrain Epoch: 157 [800/2000 (40%)]\tLoss: 0.000207\nTrain Epoch: 157 [1200/2000 (60%)]\tLoss: 0.000163\nTrain Epoch: 157 [1600/2000 (80%)]\tLoss: 0.000102\n\nevaluating...\nTest set:\tAverage loss: 1.3279, Average CER: 0.167016 Average WER: 0.6441\n\nTrain Epoch: 158 [0/2000 (0%)]\tLoss: 0.000139\nTrain Epoch: 158 [400/2000 (20%)]\tLoss: 0.000088\nTrain Epoch: 158 [800/2000 (40%)]\tLoss: 0.000130\nTrain Epoch: 158 [1200/2000 (60%)]\tLoss: 0.000201\nTrain Epoch: 158 [1600/2000 (80%)]\tLoss: 0.000157\n\nevaluating...\nTest set:\tAverage loss: 1.3277, Average CER: 0.166823 Average WER: 0.6446\n\nTrain Epoch: 159 [0/2000 (0%)]\tLoss: 0.000093\nTrain Epoch: 159 [400/2000 (20%)]\tLoss: 0.000114\nTrain Epoch: 159 [800/2000 (40%)]\tLoss: 0.000218\nTrain Epoch: 159 [1200/2000 (60%)]\tLoss: 0.000156\nTrain Epoch: 159 [1600/2000 (80%)]\tLoss: 0.000054\n\nevaluating...\nTest set:\tAverage loss: 1.3299, Average CER: 0.166834 Average WER: 0.6440\n\nTrain Epoch: 160 [0/2000 (0%)]\tLoss: 0.000115\nTrain Epoch: 160 [400/2000 (20%)]\tLoss: 0.000065\nTrain Epoch: 160 [800/2000 (40%)]\tLoss: 0.000095\nTrain Epoch: 160 [1200/2000 (60%)]\tLoss: 0.000113\nTrain Epoch: 160 [1600/2000 (80%)]\tLoss: 0.000174\n\nevaluating...\nTest set:\tAverage loss: 1.3296, Average CER: 0.166791 Average WER: 0.6451\n\nCPU times: user 1h 27min 46s, sys: 3min 39s, total: 1h 31min 25s\nWall time: 1h 31min 11s\n","output_type":"stream"}]},{"cell_type":"code","source":"#use_cuda = torch.cuda.is_available()\n#device = torch.device(\"cpu\")\nneeded_device = torch.device(\"cpu\")\nmodel = torch.load('/kaggle/input/dop-test-files/model_for_making_dataset_v4(1242).pt', map_location=torch.device('cpu'))\n\n#1543 1882 1372\n\nmodel.to(needed_device)\nprint(needed_device)\n#predict(model, '/kaggle/input/upd-speech/mono_voice/1964.wav', device)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:11:44.889066Z","iopub.execute_input":"2024-05-25T17:11:44.889589Z","iopub.status.idle":"2024-05-25T17:11:44.929998Z","shell.execute_reply.started":"2024-05-25T17:11:44.889543Z","shell.execute_reply":"2024-05-25T17:11:44.928474Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"d = {'X_test': X_test, 'label': y_test}\ndf_test = pd.DataFrame(data=d)\ndf_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:11:46.740490Z","iopub.execute_input":"2024-05-25T17:11:46.741773Z","iopub.status.idle":"2024-05-25T17:11:57.528502Z","shell.execute_reply.started":"2024-05-25T17:11:46.741725Z","shell.execute_reply":"2024-05-25T17:11:57.527027Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                              X_test  \\\n0  [[tensor(3.7671e-06), tensor(-1.9214e-05), ten...   \n1  [[tensor(-4.0955e-06), tensor(1.0839e-05), ten...   \n2  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n3  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n4  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n\n                                  label  \n0    довольствуйтесь тем что у вас есть  \n1                          читать стихи  \n2                     прячусь за столом  \n3          морской бриз дул у побережья  \n4  не могли бы вы повторить это ещё раз  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X_test</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[tensor(3.7671e-06), tensor(-1.9214e-05), ten...</td>\n      <td>довольствуйтесь тем что у вас есть</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[tensor(-4.0955e-06), tensor(1.0839e-05), ten...</td>\n      <td>читать стихи</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>прячусь за столом</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>морской бриз дул у побережья</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>не могли бы вы повторить это ещё раз</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"y_test[:5]","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:13:48.522346Z","iopub.execute_input":"2024-05-25T17:13:48.524488Z","iopub.status.idle":"2024-05-25T17:13:48.536249Z","shell.execute_reply.started":"2024-05-25T17:13:48.524396Z","shell.execute_reply":"2024-05-25T17:13:48.534741Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"['довольствуйтесь тем что у вас есть',\n 'читать стихи',\n 'прячусь за столом',\n 'морской бриз дул у побережья',\n 'не могли бы вы повторить это ещё раз']"},"metadata":{}}]},{"cell_type":"code","source":"def count_test_cer(row, model):\n    prediction = predict_with_tensor_v2(model, row['X_test'])\n    return cer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:13:48.984199Z","iopub.execute_input":"2024-05-25T17:13:48.984688Z","iopub.status.idle":"2024-05-25T17:13:48.992305Z","shell.execute_reply.started":"2024-05-25T17:13:48.984644Z","shell.execute_reply":"2024-05-25T17:13:48.990846Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def count_test_wer(row, model):\n    prediction = predict_with_tensor_v2(model, row['X_test'])\n    return wer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:13:49.592847Z","iopub.execute_input":"2024-05-25T17:13:49.593956Z","iopub.status.idle":"2024-05-25T17:13:49.600456Z","shell.execute_reply.started":"2024-05-25T17:13:49.593903Z","shell.execute_reply":"2024-05-25T17:13:49.599071Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def write_preds(row, model):\n    return predict_with_tensor_v2(model, row['X_test'])","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:13:50.412233Z","iopub.execute_input":"2024-05-25T17:13:50.413601Z","iopub.status.idle":"2024-05-25T17:13:50.419529Z","shell.execute_reply.started":"2024-05-25T17:13:50.413541Z","shell.execute_reply":"2024-05-25T17:13:50.417973Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df_test['CER'] = df_test.apply(count_test_cer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:13:52.867873Z","iopub.execute_input":"2024-05-25T17:13:52.868888Z","iopub.status.idle":"2024-05-25T17:16:17.094046Z","shell.execute_reply.started":"2024-05-25T17:13:52.868832Z","shell.execute_reply":"2024-05-25T17:16:17.092598Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"df_test['WER'] = df_test.apply(count_test_wer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:16:17.096532Z","iopub.execute_input":"2024-05-25T17:16:17.096930Z","iopub.status.idle":"2024-05-25T17:18:34.705964Z","shell.execute_reply.started":"2024-05-25T17:16:17.096893Z","shell.execute_reply":"2024-05-25T17:18:34.704548Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df_test['preds'] = df_test.apply(write_preds, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:18:34.708136Z","iopub.execute_input":"2024-05-25T17:18:34.708599Z","iopub.status.idle":"2024-05-25T17:20:51.007809Z","shell.execute_reply.started":"2024-05-25T17:18:34.708556Z","shell.execute_reply":"2024-05-25T17:20:51.006332Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df_test.loc[df_test['CER'] > 0]","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:21:40.249111Z","iopub.execute_input":"2024-05-25T17:21:40.250265Z","iopub.status.idle":"2024-05-25T17:21:50.779295Z","shell.execute_reply.started":"2024-05-25T17:21:40.250200Z","shell.execute_reply":"2024-05-25T17:21:50.777929Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                                                X_test  \\\n0    [[tensor(3.7671e-06), tensor(-1.9214e-05), ten...   \n1    [[tensor(-4.0955e-06), tensor(1.0839e-05), ten...   \n2    [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n3    [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n4    [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n..                                                 ...   \n591  [[tensor(3.8635e-10), tensor(1.0290e-09), tens...   \n592  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n593  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n595  [[tensor(6.8048e-06), tensor(6.8541e-05), tens...   \n596  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n\n                                    label       CER       WER  \\\n0      довольствуйтесь тем что у вас есть  0.205882  0.666667   \n1                            читать стихи  0.250000  1.000000   \n2                       прячусь за столом  0.176471  0.666667   \n3            морской бриз дул у побережья  0.214286  0.600000   \n4    не могли бы вы повторить это ещё раз  0.194444  0.625000   \n..                                    ...       ...       ...   \n591                время не ждёт смертный  0.409091  0.750000   \n592                       получил монетку  0.133333  0.500000   \n593                 свеча горит уже сутки  0.047619  0.250000   \n595      когда нибудь я стану президентом  0.312500  0.800000   \n596                       ты моё солнышко  0.133333  0.666667   \n\n                                  preds  \n0      довольствуйтесь тема урвать есть  \n1                         пищать стихий  \n2                      прячусь застояло  \n3            морской без уд у побережий  \n4      не моле быв поварить это еще раз  \n..                                  ...  \n591               время невесть флейтны  \n592                     получен монетку  \n593                свеча горит же сутки  \n595  кока липнуть рас стану президентам  \n596                     ты моя солнышка  \n\n[456 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X_test</th>\n      <th>label</th>\n      <th>CER</th>\n      <th>WER</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[tensor(3.7671e-06), tensor(-1.9214e-05), ten...</td>\n      <td>довольствуйтесь тем что у вас есть</td>\n      <td>0.205882</td>\n      <td>0.666667</td>\n      <td>довольствуйтесь тема урвать есть</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[tensor(-4.0955e-06), tensor(1.0839e-05), ten...</td>\n      <td>читать стихи</td>\n      <td>0.250000</td>\n      <td>1.000000</td>\n      <td>пищать стихий</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>прячусь за столом</td>\n      <td>0.176471</td>\n      <td>0.666667</td>\n      <td>прячусь застояло</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>морской бриз дул у побережья</td>\n      <td>0.214286</td>\n      <td>0.600000</td>\n      <td>морской без уд у побережий</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>не могли бы вы повторить это ещё раз</td>\n      <td>0.194444</td>\n      <td>0.625000</td>\n      <td>не моле быв поварить это еще раз</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>591</th>\n      <td>[[tensor(3.8635e-10), tensor(1.0290e-09), tens...</td>\n      <td>время не ждёт смертный</td>\n      <td>0.409091</td>\n      <td>0.750000</td>\n      <td>время невесть флейтны</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>получил монетку</td>\n      <td>0.133333</td>\n      <td>0.500000</td>\n      <td>получен монетку</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>свеча горит уже сутки</td>\n      <td>0.047619</td>\n      <td>0.250000</td>\n      <td>свеча горит же сутки</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>[[tensor(6.8048e-06), tensor(6.8541e-05), tens...</td>\n      <td>когда нибудь я стану президентом</td>\n      <td>0.312500</td>\n      <td>0.800000</td>\n      <td>кока липнуть рас стану президентам</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>ты моё солнышко</td>\n      <td>0.133333</td>\n      <td>0.666667</td>\n      <td>ты моя солнышка</td>\n    </tr>\n  </tbody>\n</table>\n<p>456 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:21:57.938839Z","iopub.execute_input":"2024-05-25T17:21:57.939319Z","iopub.status.idle":"2024-05-25T17:22:08.473060Z","shell.execute_reply.started":"2024-05-25T17:21:57.939275Z","shell.execute_reply":"2024-05-25T17:22:08.471693Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                                                X_test  \\\n0    [[tensor(3.7671e-06), tensor(-1.9214e-05), ten...   \n1    [[tensor(-4.0955e-06), tensor(1.0839e-05), ten...   \n2    [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n3    [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n4    [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n..                                                 ...   \n592  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n593  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n594  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n595  [[tensor(6.8048e-06), tensor(6.8541e-05), tens...   \n596  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   \n\n                                    label       CER       WER  \\\n0      довольствуйтесь тем что у вас есть  0.205882  0.666667   \n1                            читать стихи  0.250000  1.000000   \n2                       прячусь за столом  0.176471  0.666667   \n3            морской бриз дул у побережья  0.214286  0.600000   \n4    не могли бы вы повторить это ещё раз  0.194444  0.625000   \n..                                    ...       ...       ...   \n592                       получил монетку  0.133333  0.500000   \n593                 свеча горит уже сутки  0.047619  0.250000   \n594         оставайся здесь или иди домой  0.000000  0.000000   \n595      когда нибудь я стану президентом  0.312500  0.800000   \n596                       ты моё солнышко  0.133333  0.666667   \n\n                                  preds  \n0      довольствуйтесь тема урвать есть  \n1                         пищать стихий  \n2                      прячусь застояло  \n3            морской без уд у побережий  \n4      не моле быв поварить это еще раз  \n..                                  ...  \n592                     получен монетку  \n593                свеча горит же сутки  \n594       оставайся здесь или иди домой  \n595  кока липнуть рас стану президентам  \n596                     ты моя солнышка  \n\n[597 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X_test</th>\n      <th>label</th>\n      <th>CER</th>\n      <th>WER</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[tensor(3.7671e-06), tensor(-1.9214e-05), ten...</td>\n      <td>довольствуйтесь тем что у вас есть</td>\n      <td>0.205882</td>\n      <td>0.666667</td>\n      <td>довольствуйтесь тема урвать есть</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[tensor(-4.0955e-06), tensor(1.0839e-05), ten...</td>\n      <td>читать стихи</td>\n      <td>0.250000</td>\n      <td>1.000000</td>\n      <td>пищать стихий</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>прячусь за столом</td>\n      <td>0.176471</td>\n      <td>0.666667</td>\n      <td>прячусь застояло</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>морской бриз дул у побережья</td>\n      <td>0.214286</td>\n      <td>0.600000</td>\n      <td>морской без уд у побережий</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>не могли бы вы повторить это ещё раз</td>\n      <td>0.194444</td>\n      <td>0.625000</td>\n      <td>не моле быв поварить это еще раз</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>получил монетку</td>\n      <td>0.133333</td>\n      <td>0.500000</td>\n      <td>получен монетку</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>свеча горит уже сутки</td>\n      <td>0.047619</td>\n      <td>0.250000</td>\n      <td>свеча горит же сутки</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>оставайся здесь или иди домой</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>оставайся здесь или иди домой</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>[[tensor(6.8048e-06), tensor(6.8541e-05), tens...</td>\n      <td>когда нибудь я стану президентом</td>\n      <td>0.312500</td>\n      <td>0.800000</td>\n      <td>кока липнуть рас стану президентам</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n      <td>ты моё солнышко</td>\n      <td>0.133333</td>\n      <td>0.666667</td>\n      <td>ты моя солнышка</td>\n    </tr>\n  </tbody>\n</table>\n<p>597 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ИСПОЛЬЗОВАЛ МАЛЫЙ СЛОВАРЬ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model(3024_seed_data).pt with hunspell. WITHOUT HUNSPELL: CER = Average CER: 0.166791 Average WER: 0.6451","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['CER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:04:49.056490Z","iopub.execute_input":"2024-05-25T16:04:49.057082Z","iopub.status.idle":"2024-05-25T16:04:49.065840Z","shell.execute_reply.started":"2024-05-25T16:04:49.057036Z","shell.execute_reply":"2024-05-25T16:04:49.064572Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"0.21567754638943595"},"metadata":{}}]},{"cell_type":"code","source":"df_test['WER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:04:51.749217Z","iopub.execute_input":"2024-05-25T16:04:51.750349Z","iopub.status.idle":"2024-05-25T16:04:51.757655Z","shell.execute_reply.started":"2024-05-25T16:04:51.750300Z","shell.execute_reply":"2024-05-25T16:04:51.756495Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.5388918762820896"},"metadata":{}}]},{"cell_type":"code","source":"#using model_for_making_dataset_v4(1242).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:19:41.442613Z","iopub.execute_input":"2024-05-25T16:19:41.443296Z","iopub.status.idle":"2024-05-25T16:19:41.455403Z","shell.execute_reply.started":"2024-05-25T16:19:41.443235Z","shell.execute_reply":"2024-05-25T16:19:41.452786Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"CER:  0.20335438266839373\nWER:  0.5319660471670522\n","output_type":"stream"}]},{"cell_type":"code","source":"#using model_for_making_dataset_v5(2204).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:37:13.375118Z","iopub.execute_input":"2024-05-25T16:37:13.375680Z","iopub.status.idle":"2024-05-25T16:37:13.386340Z","shell.execute_reply.started":"2024-05-25T16:37:13.375629Z","shell.execute_reply":"2024-05-25T16:37:13.384681Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"CER:  0.21687054803888606\nWER:  0.5454120651356832\n","output_type":"stream"}]},{"cell_type":"code","source":"#using model_for_making_dataset_v6(20024).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:49:05.441566Z","iopub.execute_input":"2024-05-25T16:49:05.442097Z","iopub.status.idle":"2024-05-25T16:49:05.451177Z","shell.execute_reply.started":"2024-05-25T16:49:05.442048Z","shell.execute_reply":"2024-05-25T16:49:05.449638Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"CER:  0.22134616631615445\nWER:  0.5656132207639746\n","output_type":"stream"}]},{"cell_type":"code","source":"#using model_for_making_dataset_v7(3016).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:59:36.002854Z","iopub.execute_input":"2024-05-25T16:59:36.004626Z","iopub.status.idle":"2024-05-25T16:59:36.014691Z","shell.execute_reply.started":"2024-05-25T16:59:36.004549Z","shell.execute_reply":"2024-05-25T16:59:36.012949Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"CER:  0.20773727224290658\nWER:  0.5441696819837524\n","output_type":"stream"}]},{"cell_type":"code","source":"#ДАЛЕЕ ИСПОЛЬЗУЕТСЯ БОЛЬШИЙ СЛОВАРЬ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v4(1242).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:22:32.461589Z","iopub.execute_input":"2024-05-25T17:22:32.462082Z","iopub.status.idle":"2024-05-25T17:22:32.471480Z","shell.execute_reply.started":"2024-05-25T17:22:32.462038Z","shell.execute_reply":"2024-05-25T17:22:32.469970Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"CER:  0.20843165809280773\nWER:  0.5455305628672463\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:26:24.339488Z","iopub.execute_input":"2023-05-24T10:26:24.340405Z","iopub.status.idle":"2023-05-24T10:26:24.350954Z","shell.execute_reply.started":"2023-05-24T10:26:24.340364Z","shell.execute_reply":"2023-05-24T10:26:24.349831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:16:11.401175Z","iopub.execute_input":"2023-05-13T16:16:11.401879Z","iopub.status.idle":"2023-05-13T16:16:11.430020Z","shell.execute_reply.started":"2023-05-13T16:16:11.401838Z","shell.execute_reply":"2023-05-13T16:16:11.428960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(directory):\n    total_duration = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            filepath = os.path.join(directory, filename)\n            with wave.open(filepath, 'r') as wav_file:\n                frames = wav_file.getnframes()\n                rate = wav_file.getframerate()\n                duration = frames / float(rate)\n                total_duration += duration\n    return total_duration\n\ndirectory = '/kaggle/input/upd-speech/mono_voice'\ntotal_duration = get_wav_duration(directory)\nprint('Total duration of WAV files:', total_duration, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:15.415086Z","iopub.execute_input":"2023-07-05T10:09:15.415876Z","iopub.status.idle":"2023-07-05T10:09:18.755936Z","shell.execute_reply.started":"2023-07-05T10:09:15.415836Z","shell.execute_reply":"2023-07-05T10:09:18.754693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return '{:02d}:{:02d}:{:02d}'.format(int(hours), int(minutes), int(seconds))\nseconds = 3661\nformatted_time = format_time(total_duration)\nprint(formatted_time)  # Output: '01:01:01'","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:23.353548Z","iopub.execute_input":"2023-07-05T10:09:23.354296Z","iopub.status.idle":"2023-07-05T10:09:23.361628Z","shell.execute_reply.started":"2023-07-05T10:09:23.354254Z","shell.execute_reply":"2023-07-05T10:09:23.360431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}