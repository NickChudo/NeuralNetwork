{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5321255,"sourceType":"datasetVersion","datasetId":3091651},{"sourceId":5618710,"sourceType":"datasetVersion","datasetId":3230790},{"sourceId":5677279,"sourceType":"datasetVersion","datasetId":2989949},{"sourceId":5677449,"sourceType":"datasetVersion","datasetId":3071831},{"sourceId":5760288,"sourceType":"datasetVersion","datasetId":3311237},{"sourceId":8515857,"sourceType":"datasetVersion","datasetId":3213578},{"sourceId":8537485,"sourceType":"datasetVersion","datasetId":5099750},{"sourceId":8537499,"sourceType":"datasetVersion","datasetId":5099761},{"sourceId":8537514,"sourceType":"datasetVersion","datasetId":5099772},{"sourceId":8537530,"sourceType":"datasetVersion","datasetId":5099776},{"sourceId":8560616,"sourceType":"datasetVersion","datasetId":4230886},{"sourceId":8562283,"sourceType":"datasetVersion","datasetId":5118180}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np \nimport matplotlib\nfrom transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:30:56.505613Z","iopub.execute_input":"2024-06-09T10:30:56.506492Z","iopub.status.idle":"2024-06-09T10:31:01.556915Z","shell.execute_reply.started":"2024-06-09T10:30:56.506454Z","shell.execute_reply":"2024-06-09T10:31:01.555754Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        int_sequence = []\n        for c in text:\n            ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MFCC(n_mfcc=20)\n)\n\n\nvalid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//3)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=34, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:31:01.559170Z","iopub.execute_input":"2024-06-09T10:31:01.559870Z","iopub.status.idle":"2024-06-09T10:31:01.745770Z","shell.execute_reply.started":"2024-06-09T10:31:01.559835Z","shell.execute_reply":"2024-06-09T10:31:01.744611Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"class BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:31:01.747002Z","iopub.execute_input":"2024-06-09T10:31:01.747286Z","iopub.status.idle":"2024-06-09T10:31:01.755437Z","shell.execute_reply.started":"2024-06-09T10:31:01.747257Z","shell.execute_reply":"2024-06-09T10:31:01.754230Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Поменял там, где происходит загрузка, сохраняется id звукового файла, а потом в excel файле по колонке old_id ищется текст\n#И того звук и текст к нему\n\nimport pandas as pd\nimport librosa\n\n# file = pd.read_excel('/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches v1.xlsx')\n# #y = [sentence for sentence in file['text']]\n# y = []\n# dir_name = \"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"\n# files_in_dir = os.listdir(dir_name)\n\n# X = []\n# i = 1\n\n# for e in os.listdir(\"/kaggle/input/2700-audio/OneDrive-2023-12-25/Speeches/\"):\n#     file_name = e\n#     for old_id in range(0, 2073):\n#         if file_name.startswith(str(file['old_id'][old_id]) + '.'):\n#             y.extend([''.join(file['text'][old_id])])\n#             sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n#             sampl = sampl[np.newaxis, :]\n#             X.append(torch.Tensor(sampl))\n#             break\n\n# file = pd.read_excel('/kaggle/input/dataset-with-3-speakers/Speeches v1.xlsx')\n# #y = [sentence for sentence in file['text']]\n# y = []\n# dir_name = \"/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/\"\n# files_in_dir = os.listdir(dir_name)\n\n# X = []\n# i = 1\n\n# for e in os.listdir(\"/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/\"):\n#     file_name = e\n#     for old_id in range(0, 2073):\n#         if file_name.startswith(str(file['old_id'][old_id])[:-1]):\n#             y.extend([''.join(file['text'][old_id])])\n#             sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n#             sampl = sampl[np.newaxis, :]\n#             X.append(torch.Tensor(sampl))\n#             break\n\nfile = pd.read_excel('/kaggle/input/dataset-with-3-speakers/Speeches v1.xlsx')\n#y = [sentence for sentence in file['text']]\ny = []\ndir_name = \"/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in os.listdir(\"/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/\"):\n    file_name = e\n    for old_id in range(0, 2073):\n        if file_name.startswith(str(file['old_id'][old_id])[:-1]):\n            if file_name.startswith(str(file['old_id'][old_id])[:-1] + '3'):\n                label = [''.join(file['text'][file['old_id'][old_id]])]\n                y.extend(label)\n                sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n                sampl = sampl[np.newaxis, :]\n                X.append(torch.Tensor(sampl))\n            else: \n                label = [''.join(file['text'][old_id])]\n                y.extend(label)\n                sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n                sampl = sampl[np.newaxis, :]\n                X.append(torch.Tensor(sampl))\n            break","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:31:01.757501Z","iopub.execute_input":"2024-06-09T10:31:01.757805Z","iopub.status.idle":"2024-06-09T10:33:31.532650Z","shell.execute_reply.started":"2024-06-09T10:31:01.757777Z","shell.execute_reply":"2024-06-09T10:33:31.531625Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"y[24]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:29:02.274606Z","iopub.execute_input":"2024-06-09T10:29:02.275035Z","iopub.status.idle":"2024-06-09T10:29:02.282343Z","shell.execute_reply.started":"2024-06-09T10:29:02.274996Z","shell.execute_reply":"2024-06-09T10:29:02.281191Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Сладкий шоколад'"},"metadata":{}}]},{"cell_type":"code","source":"#waveform, sample_rate = torchaudio.load('/kaggle/input/dataset-with-3-speakers/Disorder Russian Speech/3000.3.wav')  # Загрузка аудиофайла\ntorchaudio.save('/kaggle/working/audio.wav', X[24], 16000)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:29:05.054557Z","iopub.execute_input":"2024-06-09T10:29:05.055018Z","iopub.status.idle":"2024-06-09T10:29:05.061783Z","shell.execute_reply.started":"2024-06-09T10:29:05.054973Z","shell.execute_reply":"2024-06-09T10:29:05.060577Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import random\npairs = list(zip(X, y))\nrandom.Random(20024).shuffle(pairs)\nX, y = zip(*pairs)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:33:31.534766Z","iopub.execute_input":"2024-06-09T10:33:31.535630Z","iopub.status.idle":"2024-06-09T10:33:31.546928Z","shell.execute_reply.started":"2024-06-09T10:33:31.535582Z","shell.execute_reply":"2024-06-09T10:33:31.545748Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"y[:3]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:33:40.588392Z","iopub.execute_input":"2024-06-09T10:33:40.589337Z","iopub.status.idle":"2024-06-09T10:33:40.596933Z","shell.execute_reply.started":"2024-06-09T10:33:40.589294Z","shell.execute_reply":"2024-06-09T10:33:40.595729Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('Деньги это власть',\n 'Твоя поддержка была для меня невероятно важна',\n 'Купить лекарства от насморка')"},"metadata":{}}]},{"cell_type":"code","source":"X[:3]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:33:40.862168Z","iopub.execute_input":"2024-06-09T10:33:40.862890Z","iopub.status.idle":"2024-06-09T10:33:40.893272Z","shell.execute_reply.started":"2024-06-09T10:33:40.862848Z","shell.execute_reply":"2024-06-09T10:33:40.892367Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -4.3147e-05,\n          -4.8934e-05, -5.3347e-05]]),\n tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0008,  0.0014,  0.0000]]),\n tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0025, 0.0000]]))"},"metadata":{}}]},{"cell_type":"code","source":"torchaudio.save('/kaggle/working/audio.wav', X[540], 16000)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.960331Z","iopub.status.idle":"2024-06-09T08:55:22.961016Z","shell.execute_reply.started":"2024-06-09T08:55:22.960649Z","shell.execute_reply":"2024-06-09T08:55:22.960687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"waveform, sample_rate = torchaudio.load('/kaggle/working/audio.wav')  # Загрузка аудиофайла\ntorchaudio.play(waveform, sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.962680Z","iopub.status.idle":"2024-06-09T08:55:22.963346Z","shell.execute_reply.started":"2024-06-09T08:55:22.963012Z","shell.execute_reply":"2024-06-09T08:55:22.963047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('4', 'четыре').replace('Р-220', 'р двести двадцать').replace('6', 'шесть').replace(\"-\", \" \")\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    sentence = \" \".join(sentence.split())\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:33:45.317559Z","iopub.execute_input":"2024-06-09T10:33:45.318182Z","iopub.status.idle":"2024-06-09T10:33:45.355910Z","shell.execute_reply.started":"2024-06-09T10:33:45.318137Z","shell.execute_reply":"2024-06-09T10:33:45.354838Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train = X[:2800]\nX_test = X[2800:]\ny_train = y[:2800]\ny_test = y[2800:]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:33:46.854667Z","iopub.execute_input":"2024-06-09T10:33:46.855056Z","iopub.status.idle":"2024-06-09T10:33:46.861016Z","shell.execute_reply.started":"2024-06-09T10:33:46.855020Z","shell.execute_reply":"2024-06-09T10:33:46.859864Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:33:47.233667Z","iopub.execute_input":"2024-06-09T10:33:47.234656Z","iopub.status.idle":"2024-06-09T10:33:47.243214Z","shell.execute_reply.started":"2024-06-09T10:33:47.234606Z","shell.execute_reply":"2024-06-09T10:33:47.242031Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 32, kernel_size=(4,4), stride=(3,3), padding=(2,2)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.Conv2d(128, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n        )\n        \n        self.fc_1 = nn.Sequential(\n            nn.Linear(896, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n        )\n        \n        self.BiGRU_1 = BidirectionalGRU(270, 270, 0, True)\n        self.BiGRU_2 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_3 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_4 = BidirectionalGRU(540, 270, 0.5, True)\n        \n        self.fc_2 = nn.Sequential(\n            nn.Linear(540, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.fc_1(x)\n        x = self.BiGRU_1(x)\n        x = self.BiGRU_2(x)\n        x = self.BiGRU_3(x)\n        x = self.BiGRU_4(x)\n        x = self.fc_2(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:33:47.895334Z","iopub.execute_input":"2024-06-09T10:33:47.895735Z","iopub.status.idle":"2024-06-09T10:33:47.912525Z","shell.execute_reply.started":"2024-06-09T10:33:47.895699Z","shell.execute_reply":"2024-06-09T10:33:47.911455Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Зададим название выбронной модели из хаба\nMODEL_NAME = 'UrukHan/t5-russian-spell'\nMAX_INPUT = 256\n\n# Загрузка модели и токенизатора\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\ncorrector = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.975356Z","iopub.status.idle":"2024-06-09T08:55:22.976001Z","shell.execute_reply.started":"2024-06-09T08:55:22.975647Z","shell.execute_reply":"2024-06-09T08:55:22.975681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        \"\"\"decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        for j in range(len(decoded_preds)):\n            train_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n            train_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n    avg_cer = sum(train_cer)/len(train_cer)\n    avg_wer = sum(train_wer)/len(train_wer)\n            \n    print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(train_loss, avg_cer, avg_wer))\"\"\"\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n            \n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n            \n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n    \n   \n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model_for_correction_test.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:33:49.955541Z","iopub.execute_input":"2024-06-09T10:33:49.955927Z","iopub.status.idle":"2024-06-09T10:33:49.986785Z","shell.execute_reply.started":"2024-06-09T10:33:49.955885Z","shell.execute_reply":"2024-06-09T10:33:49.985668Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict(model, file_name, device):\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    sampl = librosa.load(file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    print(spectrogram_tensor.size())\n\n    with torch.no_grad():\n        spectrogram_tensor.to(device)\n        output = model(spectrogram_tensor)\n        print(output.size())\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:33:51.095769Z","iopub.execute_input":"2024-06-09T10:33:51.096469Z","iopub.status.idle":"2024-06-09T10:33:51.107504Z","shell.execute_reply.started":"2024-06-09T10:33:51.096423Z","shell.execute_reply":"2024-06-09T10:33:51.106254Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#накрутить сюда корректор ошибок, обучение без него\ndef predict_with_tensor(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    #sampl = librosa.load(file_name, sr=16000)[0]\n    #sampl = sampl[np.newaxis, :]\n    #sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    input_sequences = decodes[0]\n                \n    task_prefix = \"Spell correct: \"\n\n    if type(input_sequences) != list: input_sequences = [input_sequences]\n    encoded = tokenizer(\n      [task_prefix + sequence for sequence in input_sequences],\n      padding=\"longest\",\n      max_length=MAX_INPUT,\n      truncation=True,\n      return_tensors=\"pt\",\n    )\n\n    predicts = corrector.generate(**encoded.to(needed_device))   # # Прогнозирование\n\n    input_sequences = tokenizer.batch_decode(predicts, skip_special_tokens=True)[0]\n    input_sequences = remove_characters(input_sequences)\n\n    return input_sequences\n\n    #return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.984075Z","iopub.status.idle":"2024-06-09T08:55:22.984737Z","shell.execute_reply.started":"2024-06-09T08:55:22.984397Z","shell.execute_reply":"2024-06-09T08:55:22.984432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import hunspell\nimport os\n\n#пробуем hunspell для исправления ошибок\ndef load_hunspell_russian_dict():\n    dict_path = \"/kaggle/input/dop-test-files\"  \n    \n    ru_dic = os.path.join(dict_path, \"ru_RU_big.dic\")\n    ru_aff = os.path.join(dict_path, \"ru_RU_big.aff\")\n    \n    # Create Hunspell instance for Russian\n    hunspell_instance = hunspell.HunSpell(ru_dic, ru_aff)\n    return hunspell_instance\n\ndef correct_mistakes(text, hunspell_instance):\n    corrected_text = []\n    words = text.split()\n    \n    for word in words:\n        if hunspell_instance.spell(word):\n            corrected_text.append(word)\n        else:\n            suggestions = hunspell_instance.suggest(word)\n            if suggestions:\n                corrected_text.append(suggestions[0])  # Choose the first suggestion\n            else:\n                corrected_text.append(word)  # No suggestion, keep the original word\n    \n    return \" \".join(corrected_text)\n\n# Example usage\nhunspell_instance = load_hunspell_russian_dict()\n#text = \"Привет, как дила?\"\n#corrected_text = correct_mistakes(text, hunspell_instance)\n#print(corrected_text)\n\ndef predict_with_tensor_v2(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    corrected_output = correct_mistakes(decodes[0], hunspell_instance)\n\n    #return decodes[0]\n    return corrected_output","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.986898Z","iopub.status.idle":"2024-06-09T08:55:22.987697Z","shell.execute_reply.started":"2024-06-09T08:55:22.987243Z","shell.execute_reply":"2024-06-09T08:55:22.987282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install safetensors","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.989390Z","iopub.status.idle":"2024-06-09T08:55:22.990124Z","shell.execute_reply.started":"2024-06-09T08:55:22.989719Z","shell.execute_reply":"2024-06-09T08:55:22.989755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.991616Z","iopub.status.idle":"2024-06-09T08:55:22.992276Z","shell.execute_reply.started":"2024-06-09T08:55:22.991947Z","shell.execute_reply":"2024-06-09T08:55:22.991983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.994234Z","iopub.status.idle":"2024-06-09T08:55:22.994933Z","shell.execute_reply.started":"2024-06-09T08:55:22.994550Z","shell.execute_reply":"2024-06-09T08:55:22.994601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntoken = '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'\n\n# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# # Load the model and tokenizer\n# model = AutoModelForSequenceClassification.from_pretrained(\"<your-username>/<your-model-name>\")\n# tokenizer = AutoTokenizer.from_pretrained(\"<your-username>/<your-model-name>\")\n\n# Use the model for inference\n#os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n\nmodel = T5ForConditionalGeneration.from_pretrained('NickChudo/t5_small_fine_tuned_10_epochs_model', \n                                                   use_auth_token=token, \n                                                   from_tf=False)\ntokenizer = T5Tokenizer.from_pretrained('NickChudo/t5_small_fine_tuned_10_epochs_tokenizer', \n                                                   use_auth_token=token,\n                                                   from_tf=False)\n\ndef correct_mistakes(text, model, tokenizer):\n    input_text = \"correct: \" + text\n    inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n    \n    outputs = model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return corrected_text\n\n\n#corrected_text = correct_mistakes(text, model, tokenizer)\n\ndef predict_with_tensor_v3(model, sampl):\n    needed_device = torch.device(\"cpu\")\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    with torch.no_grad():\n        spectrogram_tensor.to(needed_device)\n        output = model(spectrogram_tensor)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n            \n    #print(decodes[0])        \n    corrected_output = correct_mistakes(decodes[0], model, tokenizer)\n\n    #return decodes[0]\n    return corrected_output","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:22.997975Z","iopub.status.idle":"2024-06-09T08:55:22.998663Z","shell.execute_reply.started":"2024-06-09T08:55:22.998308Z","shell.execute_reply":"2024-06-09T08:55:22.998347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlearning_rate = 0.002\nbatch_size = 20\nepochs = 100\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-09T10:34:24.435107Z","iopub.execute_input":"2024-06-09T10:34:24.435618Z","iopub.status.idle":"2024-06-09T11:40:57.957445Z","shell.execute_reply.started":"2024-06-09T10:34:24.435580Z","shell.execute_reply":"2024-06-09T11:40:57.956102Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel1(\n  (conv): Sequential(\n    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Conv2d(1, 32, kernel_size=(4, 4), stride=(3, 3), padding=(2, 2))\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): GELU(approximate='none')\n    (4): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): GELU(approximate='none')\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): GELU(approximate='none')\n  )\n  (fc_1): Sequential(\n    (0): Linear(in_features=896, out_features=270, bias=True)\n    (1): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=270, out_features=270, bias=True)\n    (4): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (5): GELU(approximate='none')\n    (6): Linear(in_features=270, out_features=270, bias=True)\n    (7): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (8): GELU(approximate='none')\n  )\n  (BiGRU_1): BidirectionalGRU(\n    (BiGRU): GRU(270, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_2): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_3): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_4): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (fc_2): Sequential(\n    (0): Linear(in_features=540, out_features=35, bias=True)\n  )\n  (softmax): LogSoftmax(dim=2)\n)\nNum Model Parameters 5422923\nTrain Epoch: 1 [0/2800 (0%)]\tLoss: 27.910843\nTrain Epoch: 1 [400/2800 (14%)]\tLoss: 4.097538\nTrain Epoch: 1 [800/2800 (29%)]\tLoss: 3.593157\nTrain Epoch: 1 [1200/2800 (43%)]\tLoss: 3.448010\nTrain Epoch: 1 [1600/2800 (57%)]\tLoss: 3.341356\nTrain Epoch: 1 [2000/2800 (71%)]\tLoss: 3.398081\nTrain Epoch: 1 [2400/2800 (86%)]\tLoss: 3.308175\n\nevaluating...\nTest set:\tAverage loss: 3.3112, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 2 [0/2800 (0%)]\tLoss: 3.314719\nTrain Epoch: 2 [400/2800 (14%)]\tLoss: 3.318744\nTrain Epoch: 2 [800/2800 (29%)]\tLoss: 3.357254\nTrain Epoch: 2 [1200/2800 (43%)]\tLoss: 3.362186\nTrain Epoch: 2 [1600/2800 (57%)]\tLoss: 3.274372\nTrain Epoch: 2 [2000/2800 (71%)]\tLoss: 3.286857\nTrain Epoch: 2 [2400/2800 (86%)]\tLoss: 3.309580\n\nevaluating...\nTest set:\tAverage loss: 3.2659, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 3 [0/2800 (0%)]\tLoss: 3.240240\nTrain Epoch: 3 [400/2800 (14%)]\tLoss: 3.233972\nTrain Epoch: 3 [800/2800 (29%)]\tLoss: 3.162353\nTrain Epoch: 3 [1200/2800 (43%)]\tLoss: 3.209652\nTrain Epoch: 3 [1600/2800 (57%)]\tLoss: 3.131789\nTrain Epoch: 3 [2000/2800 (71%)]\tLoss: 3.027837\nTrain Epoch: 3 [2400/2800 (86%)]\tLoss: 2.881769\n\nevaluating...\nTest set:\tAverage loss: 2.7931, Average CER: 0.997382 Average WER: 1.0041\n\nTrain Epoch: 4 [0/2800 (0%)]\tLoss: 2.834082\nTrain Epoch: 4 [400/2800 (14%)]\tLoss: 2.644107\nTrain Epoch: 4 [800/2800 (29%)]\tLoss: 2.609996\nTrain Epoch: 4 [1200/2800 (43%)]\tLoss: 2.557269\nTrain Epoch: 4 [1600/2800 (57%)]\tLoss: 2.426993\nTrain Epoch: 4 [2000/2800 (71%)]\tLoss: 2.307602\nTrain Epoch: 4 [2400/2800 (86%)]\tLoss: 2.110372\n\nevaluating...\nTest set:\tAverage loss: 2.0666, Average CER: 0.686870 Average WER: 1.0289\n\nTrain Epoch: 5 [0/2800 (0%)]\tLoss: 2.104741\nTrain Epoch: 5 [400/2800 (14%)]\tLoss: 1.859420\nTrain Epoch: 5 [800/2800 (29%)]\tLoss: 1.871312\nTrain Epoch: 5 [1200/2800 (43%)]\tLoss: 1.942019\nTrain Epoch: 5 [1600/2800 (57%)]\tLoss: 2.056528\nTrain Epoch: 5 [2000/2800 (71%)]\tLoss: 1.519966\nTrain Epoch: 5 [2400/2800 (86%)]\tLoss: 1.599999\n\nevaluating...\nTest set:\tAverage loss: 1.5524, Average CER: 0.473764 Average WER: 0.9863\n\nTrain Epoch: 6 [0/2800 (0%)]\tLoss: 1.526573\nTrain Epoch: 6 [400/2800 (14%)]\tLoss: 1.588867\nTrain Epoch: 6 [800/2800 (29%)]\tLoss: 1.502548\nTrain Epoch: 6 [1200/2800 (43%)]\tLoss: 1.402965\nTrain Epoch: 6 [1600/2800 (57%)]\tLoss: 1.439314\nTrain Epoch: 6 [2000/2800 (71%)]\tLoss: 1.502822\nTrain Epoch: 6 [2400/2800 (86%)]\tLoss: 1.564598\n\nevaluating...\nTest set:\tAverage loss: 1.3572, Average CER: 0.418281 Average WER: 0.9677\n\nTrain Epoch: 7 [0/2800 (0%)]\tLoss: 1.412265\nTrain Epoch: 7 [400/2800 (14%)]\tLoss: 1.104599\nTrain Epoch: 7 [800/2800 (29%)]\tLoss: 1.241620\nTrain Epoch: 7 [1200/2800 (43%)]\tLoss: 0.974663\nTrain Epoch: 7 [1600/2800 (57%)]\tLoss: 1.047370\nTrain Epoch: 7 [2000/2800 (71%)]\tLoss: 1.167490\nTrain Epoch: 7 [2400/2800 (86%)]\tLoss: 1.067911\n\nevaluating...\nTest set:\tAverage loss: 1.1885, Average CER: 0.362142 Average WER: 0.9604\n\nTrain Epoch: 8 [0/2800 (0%)]\tLoss: 1.104796\nTrain Epoch: 8 [400/2800 (14%)]\tLoss: 1.224868\nTrain Epoch: 8 [800/2800 (29%)]\tLoss: 0.985732\nTrain Epoch: 8 [1200/2800 (43%)]\tLoss: 0.918277\nTrain Epoch: 8 [1600/2800 (57%)]\tLoss: 0.925417\nTrain Epoch: 8 [2000/2800 (71%)]\tLoss: 0.925678\nTrain Epoch: 8 [2400/2800 (86%)]\tLoss: 1.124831\n\nevaluating...\nTest set:\tAverage loss: 1.1285, Average CER: 0.341278 Average WER: 0.9213\n\nTrain Epoch: 9 [0/2800 (0%)]\tLoss: 0.874828\nTrain Epoch: 9 [400/2800 (14%)]\tLoss: 1.025223\nTrain Epoch: 9 [800/2800 (29%)]\tLoss: 1.180974\nTrain Epoch: 9 [1200/2800 (43%)]\tLoss: 0.928753\nTrain Epoch: 9 [1600/2800 (57%)]\tLoss: 0.895724\nTrain Epoch: 9 [2000/2800 (71%)]\tLoss: 0.925924\nTrain Epoch: 9 [2400/2800 (86%)]\tLoss: 0.885260\n\nevaluating...\nTest set:\tAverage loss: 1.1125, Average CER: 0.326409 Average WER: 0.9265\n\nTrain Epoch: 10 [0/2800 (0%)]\tLoss: 0.987319\nTrain Epoch: 10 [400/2800 (14%)]\tLoss: 0.696611\nTrain Epoch: 10 [800/2800 (29%)]\tLoss: 0.782434\nTrain Epoch: 10 [1200/2800 (43%)]\tLoss: 0.817422\nTrain Epoch: 10 [1600/2800 (57%)]\tLoss: 0.596126\nTrain Epoch: 10 [2000/2800 (71%)]\tLoss: 0.776559\nTrain Epoch: 10 [2400/2800 (86%)]\tLoss: 0.956622\n\nevaluating...\nTest set:\tAverage loss: 1.0483, Average CER: 0.311498 Average WER: 0.9100\n\nTrain Epoch: 11 [0/2800 (0%)]\tLoss: 0.688819\nTrain Epoch: 11 [400/2800 (14%)]\tLoss: 0.616894\nTrain Epoch: 11 [800/2800 (29%)]\tLoss: 0.572490\nTrain Epoch: 11 [1200/2800 (43%)]\tLoss: 0.807125\nTrain Epoch: 11 [1600/2800 (57%)]\tLoss: 0.703197\nTrain Epoch: 11 [2000/2800 (71%)]\tLoss: 0.597237\nTrain Epoch: 11 [2400/2800 (86%)]\tLoss: 0.603283\n\nevaluating...\nTest set:\tAverage loss: 1.0232, Average CER: 0.304491 Average WER: 0.8847\n\nTrain Epoch: 12 [0/2800 (0%)]\tLoss: 0.584724\nTrain Epoch: 12 [400/2800 (14%)]\tLoss: 0.654829\nTrain Epoch: 12 [800/2800 (29%)]\tLoss: 0.615797\nTrain Epoch: 12 [1200/2800 (43%)]\tLoss: 0.489643\nTrain Epoch: 12 [1600/2800 (57%)]\tLoss: 0.601271\nTrain Epoch: 12 [2000/2800 (71%)]\tLoss: 0.646225\nTrain Epoch: 12 [2400/2800 (86%)]\tLoss: 0.888815\n\nevaluating...\nTest set:\tAverage loss: 0.9867, Average CER: 0.294465 Average WER: 0.8735\n\nTrain Epoch: 13 [0/2800 (0%)]\tLoss: 0.470550\nTrain Epoch: 13 [400/2800 (14%)]\tLoss: 0.436315\nTrain Epoch: 13 [800/2800 (29%)]\tLoss: 0.499768\nTrain Epoch: 13 [1200/2800 (43%)]\tLoss: 0.690211\nTrain Epoch: 13 [1600/2800 (57%)]\tLoss: 0.724046\nTrain Epoch: 13 [2000/2800 (71%)]\tLoss: 0.787676\nTrain Epoch: 13 [2400/2800 (86%)]\tLoss: 0.784870\n\nevaluating...\nTest set:\tAverage loss: 1.0092, Average CER: 0.293745 Average WER: 0.8660\n\nTrain Epoch: 14 [0/2800 (0%)]\tLoss: 0.544619\nTrain Epoch: 14 [400/2800 (14%)]\tLoss: 0.493128\nTrain Epoch: 14 [800/2800 (29%)]\tLoss: 0.387537\nTrain Epoch: 14 [1200/2800 (43%)]\tLoss: 0.630171\nTrain Epoch: 14 [1600/2800 (57%)]\tLoss: 0.644911\nTrain Epoch: 14 [2000/2800 (71%)]\tLoss: 0.669079\nTrain Epoch: 14 [2400/2800 (86%)]\tLoss: 0.547022\n\nevaluating...\nTest set:\tAverage loss: 1.0083, Average CER: 0.286814 Average WER: 0.8830\n\nTrain Epoch: 15 [0/2800 (0%)]\tLoss: 0.417212\nTrain Epoch: 15 [400/2800 (14%)]\tLoss: 0.571243\nTrain Epoch: 15 [800/2800 (29%)]\tLoss: 0.556300\nTrain Epoch: 15 [1200/2800 (43%)]\tLoss: 0.575314\nTrain Epoch: 15 [1600/2800 (57%)]\tLoss: 0.667767\nTrain Epoch: 15 [2000/2800 (71%)]\tLoss: 0.561156\nTrain Epoch: 15 [2400/2800 (86%)]\tLoss: 0.449428\n\nevaluating...\nTest set:\tAverage loss: 0.9724, Average CER: 0.275324 Average WER: 0.8296\n\nTrain Epoch: 16 [0/2800 (0%)]\tLoss: 0.464447\nTrain Epoch: 16 [400/2800 (14%)]\tLoss: 0.341982\nTrain Epoch: 16 [800/2800 (29%)]\tLoss: 0.423811\nTrain Epoch: 16 [1200/2800 (43%)]\tLoss: 0.480644\nTrain Epoch: 16 [1600/2800 (57%)]\tLoss: 0.626332\nTrain Epoch: 16 [2000/2800 (71%)]\tLoss: 0.524908\nTrain Epoch: 16 [2400/2800 (86%)]\tLoss: 0.596894\n\nevaluating...\nTest set:\tAverage loss: 1.0453, Average CER: 0.290804 Average WER: 0.8874\n\nTrain Epoch: 17 [0/2800 (0%)]\tLoss: 0.438096\nTrain Epoch: 17 [400/2800 (14%)]\tLoss: 0.452024\nTrain Epoch: 17 [800/2800 (29%)]\tLoss: 0.357145\nTrain Epoch: 17 [1200/2800 (43%)]\tLoss: 0.403963\nTrain Epoch: 17 [1600/2800 (57%)]\tLoss: 0.433748\nTrain Epoch: 17 [2000/2800 (71%)]\tLoss: 0.362199\nTrain Epoch: 17 [2400/2800 (86%)]\tLoss: 0.348945\n\nevaluating...\nTest set:\tAverage loss: 1.0685, Average CER: 0.287023 Average WER: 0.8456\n\nTrain Epoch: 18 [0/2800 (0%)]\tLoss: 0.478499\nTrain Epoch: 18 [400/2800 (14%)]\tLoss: 0.474112\nTrain Epoch: 18 [800/2800 (29%)]\tLoss: 0.509314\nTrain Epoch: 18 [1200/2800 (43%)]\tLoss: 0.359561\nTrain Epoch: 18 [1600/2800 (57%)]\tLoss: 0.502556\nTrain Epoch: 18 [2000/2800 (71%)]\tLoss: 0.395517\nTrain Epoch: 18 [2400/2800 (86%)]\tLoss: 0.422474\n\nevaluating...\nTest set:\tAverage loss: 1.0118, Average CER: 0.274221 Average WER: 0.8398\n\nTrain Epoch: 19 [0/2800 (0%)]\tLoss: 0.304673\nTrain Epoch: 19 [400/2800 (14%)]\tLoss: 0.321096\nTrain Epoch: 19 [800/2800 (29%)]\tLoss: 0.316241\nTrain Epoch: 19 [1200/2800 (43%)]\tLoss: 0.428095\nTrain Epoch: 19 [1600/2800 (57%)]\tLoss: 0.405893\nTrain Epoch: 19 [2000/2800 (71%)]\tLoss: 0.412707\nTrain Epoch: 19 [2400/2800 (86%)]\tLoss: 0.564268\n\nevaluating...\nTest set:\tAverage loss: 1.0732, Average CER: 0.282704 Average WER: 0.8458\n\nTrain Epoch: 20 [0/2800 (0%)]\tLoss: 0.424643\nTrain Epoch: 20 [400/2800 (14%)]\tLoss: 0.429334\nTrain Epoch: 20 [800/2800 (29%)]\tLoss: 0.480870\nTrain Epoch: 20 [1200/2800 (43%)]\tLoss: 0.421807\nTrain Epoch: 20 [1600/2800 (57%)]\tLoss: 0.397199\nTrain Epoch: 20 [2000/2800 (71%)]\tLoss: 0.352286\nTrain Epoch: 20 [2400/2800 (86%)]\tLoss: 0.397519\n\nevaluating...\nTest set:\tAverage loss: 1.0560, Average CER: 0.268965 Average WER: 0.8299\n\nTrain Epoch: 21 [0/2800 (0%)]\tLoss: 0.330096\nTrain Epoch: 21 [400/2800 (14%)]\tLoss: 0.160555\nTrain Epoch: 21 [800/2800 (29%)]\tLoss: 0.282384\nTrain Epoch: 21 [1200/2800 (43%)]\tLoss: 0.338283\nTrain Epoch: 21 [1600/2800 (57%)]\tLoss: 0.223707\nTrain Epoch: 21 [2000/2800 (71%)]\tLoss: 0.331196\nTrain Epoch: 21 [2400/2800 (86%)]\tLoss: 0.452567\n\nevaluating...\nTest set:\tAverage loss: 1.0622, Average CER: 0.286376 Average WER: 0.8637\n\nTrain Epoch: 22 [0/2800 (0%)]\tLoss: 0.337554\nTrain Epoch: 22 [400/2800 (14%)]\tLoss: 0.393233\nTrain Epoch: 22 [800/2800 (29%)]\tLoss: 0.379985\nTrain Epoch: 22 [1200/2800 (43%)]\tLoss: 0.293282\nTrain Epoch: 22 [1600/2800 (57%)]\tLoss: 0.181491\nTrain Epoch: 22 [2000/2800 (71%)]\tLoss: 0.406779\nTrain Epoch: 22 [2400/2800 (86%)]\tLoss: 0.392733\n\nevaluating...\nTest set:\tAverage loss: 1.1216, Average CER: 0.288044 Average WER: 0.8627\n\nTrain Epoch: 23 [0/2800 (0%)]\tLoss: 0.539509\nTrain Epoch: 23 [400/2800 (14%)]\tLoss: 0.348068\nTrain Epoch: 23 [800/2800 (29%)]\tLoss: 0.230057\nTrain Epoch: 23 [1200/2800 (43%)]\tLoss: 0.592608\nTrain Epoch: 23 [1600/2800 (57%)]\tLoss: 0.381493\nTrain Epoch: 23 [2000/2800 (71%)]\tLoss: 0.252354\nTrain Epoch: 23 [2400/2800 (86%)]\tLoss: 0.400669\n\nevaluating...\nTest set:\tAverage loss: 1.0985, Average CER: 0.276604 Average WER: 0.8388\n\nTrain Epoch: 24 [0/2800 (0%)]\tLoss: 0.309649\nTrain Epoch: 24 [400/2800 (14%)]\tLoss: 0.222930\nTrain Epoch: 24 [800/2800 (29%)]\tLoss: 0.198373\nTrain Epoch: 24 [1200/2800 (43%)]\tLoss: 0.220831\nTrain Epoch: 24 [1600/2800 (57%)]\tLoss: 0.324596\nTrain Epoch: 24 [2000/2800 (71%)]\tLoss: 0.376068\nTrain Epoch: 24 [2400/2800 (86%)]\tLoss: 0.393296\n\nevaluating...\nTest set:\tAverage loss: 1.0290, Average CER: 0.258554 Average WER: 0.8093\n\nTrain Epoch: 25 [0/2800 (0%)]\tLoss: 0.252192\nTrain Epoch: 25 [400/2800 (14%)]\tLoss: 0.334506\nTrain Epoch: 25 [800/2800 (29%)]\tLoss: 0.227165\nTrain Epoch: 25 [1200/2800 (43%)]\tLoss: 0.253826\nTrain Epoch: 25 [1600/2800 (57%)]\tLoss: 0.351558\nTrain Epoch: 25 [2000/2800 (71%)]\tLoss: 0.314805\nTrain Epoch: 25 [2400/2800 (86%)]\tLoss: 0.322322\n\nevaluating...\nTest set:\tAverage loss: 1.0321, Average CER: 0.267515 Average WER: 0.8351\n\nTrain Epoch: 26 [0/2800 (0%)]\tLoss: 0.206994\nTrain Epoch: 26 [400/2800 (14%)]\tLoss: 0.389088\nTrain Epoch: 26 [800/2800 (29%)]\tLoss: 0.270522\nTrain Epoch: 26 [1200/2800 (43%)]\tLoss: 0.357033\nTrain Epoch: 26 [1600/2800 (57%)]\tLoss: 0.370571\nTrain Epoch: 26 [2000/2800 (71%)]\tLoss: 0.334791\nTrain Epoch: 26 [2400/2800 (86%)]\tLoss: 0.445003\n\nevaluating...\nTest set:\tAverage loss: 1.1124, Average CER: 0.277969 Average WER: 0.8474\n\nTrain Epoch: 27 [0/2800 (0%)]\tLoss: 0.336975\nTrain Epoch: 27 [400/2800 (14%)]\tLoss: 0.368995\nTrain Epoch: 27 [800/2800 (29%)]\tLoss: 0.374224\nTrain Epoch: 27 [1200/2800 (43%)]\tLoss: 0.252859\nTrain Epoch: 27 [1600/2800 (57%)]\tLoss: 0.356032\nTrain Epoch: 27 [2000/2800 (71%)]\tLoss: 0.292070\nTrain Epoch: 27 [2400/2800 (86%)]\tLoss: 0.490194\n\nevaluating...\nTest set:\tAverage loss: 1.1194, Average CER: 0.291454 Average WER: 0.8457\n\nTrain Epoch: 28 [0/2800 (0%)]\tLoss: 0.376557\nTrain Epoch: 28 [400/2800 (14%)]\tLoss: 0.387461\nTrain Epoch: 28 [800/2800 (29%)]\tLoss: 0.258952\nTrain Epoch: 28 [1200/2800 (43%)]\tLoss: 0.284333\nTrain Epoch: 28 [1600/2800 (57%)]\tLoss: 0.280409\nTrain Epoch: 28 [2000/2800 (71%)]\tLoss: 0.426551\nTrain Epoch: 28 [2400/2800 (86%)]\tLoss: 0.447585\n\nevaluating...\nTest set:\tAverage loss: 1.0664, Average CER: 0.259268 Average WER: 0.7922\n\nTrain Epoch: 29 [0/2800 (0%)]\tLoss: 0.246304\nTrain Epoch: 29 [400/2800 (14%)]\tLoss: 0.385958\nTrain Epoch: 29 [800/2800 (29%)]\tLoss: 0.281745\nTrain Epoch: 29 [1200/2800 (43%)]\tLoss: 0.183536\nTrain Epoch: 29 [1600/2800 (57%)]\tLoss: 0.286883\nTrain Epoch: 29 [2000/2800 (71%)]\tLoss: 0.257341\nTrain Epoch: 29 [2400/2800 (86%)]\tLoss: 0.309880\n\nevaluating...\nTest set:\tAverage loss: 1.1499, Average CER: 0.275194 Average WER: 0.8186\n\nTrain Epoch: 30 [0/2800 (0%)]\tLoss: 0.245871\nTrain Epoch: 30 [400/2800 (14%)]\tLoss: 0.273649\nTrain Epoch: 30 [800/2800 (29%)]\tLoss: 0.191652\nTrain Epoch: 30 [1200/2800 (43%)]\tLoss: 0.276553\nTrain Epoch: 30 [1600/2800 (57%)]\tLoss: 0.371067\nTrain Epoch: 30 [2000/2800 (71%)]\tLoss: 0.267540\nTrain Epoch: 30 [2400/2800 (86%)]\tLoss: 0.235849\n\nevaluating...\nTest set:\tAverage loss: 1.1414, Average CER: 0.274201 Average WER: 0.8361\n\nTrain Epoch: 31 [0/2800 (0%)]\tLoss: 0.355068\nTrain Epoch: 31 [400/2800 (14%)]\tLoss: 0.374621\nTrain Epoch: 31 [800/2800 (29%)]\tLoss: 0.383032\nTrain Epoch: 31 [1200/2800 (43%)]\tLoss: 0.284068\nTrain Epoch: 31 [1600/2800 (57%)]\tLoss: 0.277981\nTrain Epoch: 31 [2000/2800 (71%)]\tLoss: 0.503809\nTrain Epoch: 31 [2400/2800 (86%)]\tLoss: 0.293183\n\nevaluating...\nTest set:\tAverage loss: 1.1338, Average CER: 0.280124 Average WER: 0.8323\n\nTrain Epoch: 32 [0/2800 (0%)]\tLoss: 0.297463\nTrain Epoch: 32 [400/2800 (14%)]\tLoss: 0.229847\nTrain Epoch: 32 [800/2800 (29%)]\tLoss: 0.244890\nTrain Epoch: 32 [1200/2800 (43%)]\tLoss: 0.295939\nTrain Epoch: 32 [1600/2800 (57%)]\tLoss: 0.361643\nTrain Epoch: 32 [2000/2800 (71%)]\tLoss: 0.185883\nTrain Epoch: 32 [2400/2800 (86%)]\tLoss: 0.278035\n\nevaluating...\nTest set:\tAverage loss: 1.0597, Average CER: 0.247091 Average WER: 0.7870\n\nTrain Epoch: 33 [0/2800 (0%)]\tLoss: 0.258108\nTrain Epoch: 33 [400/2800 (14%)]\tLoss: 0.119130\nTrain Epoch: 33 [800/2800 (29%)]\tLoss: 0.261102\nTrain Epoch: 33 [1200/2800 (43%)]\tLoss: 0.157336\nTrain Epoch: 33 [1600/2800 (57%)]\tLoss: 0.280554\nTrain Epoch: 33 [2000/2800 (71%)]\tLoss: 0.184087\nTrain Epoch: 33 [2400/2800 (86%)]\tLoss: 0.194333\n\nevaluating...\nTest set:\tAverage loss: 1.0740, Average CER: 0.245586 Average WER: 0.7798\n\nTrain Epoch: 34 [0/2800 (0%)]\tLoss: 0.220897\nTrain Epoch: 34 [400/2800 (14%)]\tLoss: 0.179934\nTrain Epoch: 34 [800/2800 (29%)]\tLoss: 0.157782\nTrain Epoch: 34 [1200/2800 (43%)]\tLoss: 0.442339\nTrain Epoch: 34 [1600/2800 (57%)]\tLoss: 0.168559\nTrain Epoch: 34 [2000/2800 (71%)]\tLoss: 0.286786\nTrain Epoch: 34 [2400/2800 (86%)]\tLoss: 0.247417\n\nevaluating...\nTest set:\tAverage loss: 1.1389, Average CER: 0.259503 Average WER: 0.8022\n\nTrain Epoch: 35 [0/2800 (0%)]\tLoss: 0.223122\nTrain Epoch: 35 [400/2800 (14%)]\tLoss: 0.252029\nTrain Epoch: 35 [800/2800 (29%)]\tLoss: 0.193796\nTrain Epoch: 35 [1200/2800 (43%)]\tLoss: 0.133164\nTrain Epoch: 35 [1600/2800 (57%)]\tLoss: 0.368665\nTrain Epoch: 35 [2000/2800 (71%)]\tLoss: 0.292881\nTrain Epoch: 35 [2400/2800 (86%)]\tLoss: 0.266516\n\nevaluating...\nTest set:\tAverage loss: 1.1775, Average CER: 0.258015 Average WER: 0.8104\n\nTrain Epoch: 36 [0/2800 (0%)]\tLoss: 0.171946\nTrain Epoch: 36 [400/2800 (14%)]\tLoss: 0.104145\nTrain Epoch: 36 [800/2800 (29%)]\tLoss: 0.183160\nTrain Epoch: 36 [1200/2800 (43%)]\tLoss: 0.231905\nTrain Epoch: 36 [1600/2800 (57%)]\tLoss: 0.129900\nTrain Epoch: 36 [2000/2800 (71%)]\tLoss: 0.177452\nTrain Epoch: 36 [2400/2800 (86%)]\tLoss: 0.216335\n\nevaluating...\nTest set:\tAverage loss: 1.1307, Average CER: 0.248111 Average WER: 0.7653\n\nTrain Epoch: 37 [0/2800 (0%)]\tLoss: 0.150172\nTrain Epoch: 37 [400/2800 (14%)]\tLoss: 0.098092\nTrain Epoch: 37 [800/2800 (29%)]\tLoss: 0.163184\nTrain Epoch: 37 [1200/2800 (43%)]\tLoss: 0.129026\nTrain Epoch: 37 [1600/2800 (57%)]\tLoss: 0.201247\nTrain Epoch: 37 [2000/2800 (71%)]\tLoss: 0.211605\nTrain Epoch: 37 [2400/2800 (86%)]\tLoss: 0.099201\n\nevaluating...\nTest set:\tAverage loss: 1.1617, Average CER: 0.237342 Average WER: 0.7745\n\nTrain Epoch: 38 [0/2800 (0%)]\tLoss: 0.094180\nTrain Epoch: 38 [400/2800 (14%)]\tLoss: 0.150643\nTrain Epoch: 38 [800/2800 (29%)]\tLoss: 0.050456\nTrain Epoch: 38 [1200/2800 (43%)]\tLoss: 0.103188\nTrain Epoch: 38 [1600/2800 (57%)]\tLoss: 0.054981\nTrain Epoch: 38 [2000/2800 (71%)]\tLoss: 0.097572\nTrain Epoch: 38 [2400/2800 (86%)]\tLoss: 0.107548\n\nevaluating...\nTest set:\tAverage loss: 1.2211, Average CER: 0.240183 Average WER: 0.7783\n\nTrain Epoch: 39 [0/2800 (0%)]\tLoss: 0.168623\nTrain Epoch: 39 [400/2800 (14%)]\tLoss: 0.114190\nTrain Epoch: 39 [800/2800 (29%)]\tLoss: 0.136673\nTrain Epoch: 39 [1200/2800 (43%)]\tLoss: 0.155113\nTrain Epoch: 39 [1600/2800 (57%)]\tLoss: 0.165356\nTrain Epoch: 39 [2000/2800 (71%)]\tLoss: 0.165402\nTrain Epoch: 39 [2400/2800 (86%)]\tLoss: 0.183650\n\nevaluating...\nTest set:\tAverage loss: 1.2813, Average CER: 0.253008 Average WER: 0.7947\n\nTrain Epoch: 40 [0/2800 (0%)]\tLoss: 0.127035\nTrain Epoch: 40 [400/2800 (14%)]\tLoss: 0.131101\nTrain Epoch: 40 [800/2800 (29%)]\tLoss: 0.145643\nTrain Epoch: 40 [1200/2800 (43%)]\tLoss: 0.112698\nTrain Epoch: 40 [1600/2800 (57%)]\tLoss: 0.209753\nTrain Epoch: 40 [2000/2800 (71%)]\tLoss: 0.105745\nTrain Epoch: 40 [2400/2800 (86%)]\tLoss: 0.157692\n\nevaluating...\nTest set:\tAverage loss: 1.2123, Average CER: 0.241176 Average WER: 0.7900\n\nTrain Epoch: 41 [0/2800 (0%)]\tLoss: 0.052570\nTrain Epoch: 41 [400/2800 (14%)]\tLoss: 0.100095\nTrain Epoch: 41 [800/2800 (29%)]\tLoss: 0.067534\nTrain Epoch: 41 [1200/2800 (43%)]\tLoss: 0.106375\nTrain Epoch: 41 [1600/2800 (57%)]\tLoss: 0.090675\nTrain Epoch: 41 [2000/2800 (71%)]\tLoss: 0.063325\nTrain Epoch: 41 [2400/2800 (86%)]\tLoss: 0.140781\n\nevaluating...\nTest set:\tAverage loss: 1.2597, Average CER: 0.244308 Average WER: 0.7966\n\nTrain Epoch: 42 [0/2800 (0%)]\tLoss: 0.084805\nTrain Epoch: 42 [400/2800 (14%)]\tLoss: 0.107185\nTrain Epoch: 42 [800/2800 (29%)]\tLoss: 0.162755\nTrain Epoch: 42 [1200/2800 (43%)]\tLoss: 0.119804\nTrain Epoch: 42 [1600/2800 (57%)]\tLoss: 0.088327\nTrain Epoch: 42 [2000/2800 (71%)]\tLoss: 0.120424\nTrain Epoch: 42 [2400/2800 (86%)]\tLoss: 0.113305\n\nevaluating...\nTest set:\tAverage loss: 1.2192, Average CER: 0.238450 Average WER: 0.7573\n\nTrain Epoch: 43 [0/2800 (0%)]\tLoss: 0.074670\nTrain Epoch: 43 [400/2800 (14%)]\tLoss: 0.166542\nTrain Epoch: 43 [800/2800 (29%)]\tLoss: 0.163866\nTrain Epoch: 43 [1200/2800 (43%)]\tLoss: 0.114584\nTrain Epoch: 43 [1600/2800 (57%)]\tLoss: 0.189234\nTrain Epoch: 43 [2000/2800 (71%)]\tLoss: 0.209308\nTrain Epoch: 43 [2400/2800 (86%)]\tLoss: 0.301824\n\nevaluating...\nTest set:\tAverage loss: 1.2616, Average CER: 0.255171 Average WER: 0.7882\n\nTrain Epoch: 44 [0/2800 (0%)]\tLoss: 0.237245\nTrain Epoch: 44 [400/2800 (14%)]\tLoss: 0.114163\nTrain Epoch: 44 [800/2800 (29%)]\tLoss: 0.124976\nTrain Epoch: 44 [1200/2800 (43%)]\tLoss: 0.091405\nTrain Epoch: 44 [1600/2800 (57%)]\tLoss: 0.075767\nTrain Epoch: 44 [2000/2800 (71%)]\tLoss: 0.145453\nTrain Epoch: 44 [2400/2800 (86%)]\tLoss: 0.129745\n\nevaluating...\nTest set:\tAverage loss: 1.1894, Average CER: 0.237425 Average WER: 0.7650\n\nTrain Epoch: 45 [0/2800 (0%)]\tLoss: 0.100589\nTrain Epoch: 45 [400/2800 (14%)]\tLoss: 0.067226\nTrain Epoch: 45 [800/2800 (29%)]\tLoss: 0.085243\nTrain Epoch: 45 [1200/2800 (43%)]\tLoss: 0.130345\nTrain Epoch: 45 [1600/2800 (57%)]\tLoss: 0.056089\nTrain Epoch: 45 [2000/2800 (71%)]\tLoss: 0.074236\nTrain Epoch: 45 [2400/2800 (86%)]\tLoss: 0.114920\n\nevaluating...\nTest set:\tAverage loss: 1.2933, Average CER: 0.235627 Average WER: 0.7581\n\nTrain Epoch: 46 [0/2800 (0%)]\tLoss: 0.036317\nTrain Epoch: 46 [400/2800 (14%)]\tLoss: 0.081318\nTrain Epoch: 46 [800/2800 (29%)]\tLoss: 0.063894\nTrain Epoch: 46 [1200/2800 (43%)]\tLoss: 0.073075\nTrain Epoch: 46 [1600/2800 (57%)]\tLoss: 0.067729\nTrain Epoch: 46 [2000/2800 (71%)]\tLoss: 0.100202\nTrain Epoch: 46 [2400/2800 (86%)]\tLoss: 0.073778\n\nevaluating...\nTest set:\tAverage loss: 1.2740, Average CER: 0.236019 Average WER: 0.7443\n\nTrain Epoch: 47 [0/2800 (0%)]\tLoss: 0.057235\nTrain Epoch: 47 [400/2800 (14%)]\tLoss: 0.074588\nTrain Epoch: 47 [800/2800 (29%)]\tLoss: 0.133205\nTrain Epoch: 47 [1200/2800 (43%)]\tLoss: 0.286696\nTrain Epoch: 47 [1600/2800 (57%)]\tLoss: 0.101328\nTrain Epoch: 47 [2000/2800 (71%)]\tLoss: 0.101108\nTrain Epoch: 47 [2400/2800 (86%)]\tLoss: 0.207608\n\nevaluating...\nTest set:\tAverage loss: 1.2402, Average CER: 0.231757 Average WER: 0.7538\n\nTrain Epoch: 48 [0/2800 (0%)]\tLoss: 0.089091\nTrain Epoch: 48 [400/2800 (14%)]\tLoss: 0.067219\nTrain Epoch: 48 [800/2800 (29%)]\tLoss: 0.093571\nTrain Epoch: 48 [1200/2800 (43%)]\tLoss: 0.039454\nTrain Epoch: 48 [1600/2800 (57%)]\tLoss: 0.063259\nTrain Epoch: 48 [2000/2800 (71%)]\tLoss: 0.088450\nTrain Epoch: 48 [2400/2800 (86%)]\tLoss: 0.136271\n\nevaluating...\nTest set:\tAverage loss: 1.3022, Average CER: 0.226210 Average WER: 0.7552\n\nTrain Epoch: 49 [0/2800 (0%)]\tLoss: 0.039380\nTrain Epoch: 49 [400/2800 (14%)]\tLoss: 0.031697\nTrain Epoch: 49 [800/2800 (29%)]\tLoss: 0.077924\nTrain Epoch: 49 [1200/2800 (43%)]\tLoss: 0.026147\nTrain Epoch: 49 [1600/2800 (57%)]\tLoss: 0.090740\nTrain Epoch: 49 [2000/2800 (71%)]\tLoss: 0.074083\nTrain Epoch: 49 [2400/2800 (86%)]\tLoss: 0.044919\n\nevaluating...\nTest set:\tAverage loss: 1.3336, Average CER: 0.233987 Average WER: 0.7609\n\nTrain Epoch: 50 [0/2800 (0%)]\tLoss: 0.103364\nTrain Epoch: 50 [400/2800 (14%)]\tLoss: 0.061212\nTrain Epoch: 50 [800/2800 (29%)]\tLoss: 0.090789\nTrain Epoch: 50 [1200/2800 (43%)]\tLoss: 0.211148\nTrain Epoch: 50 [1600/2800 (57%)]\tLoss: 0.099564\nTrain Epoch: 50 [2000/2800 (71%)]\tLoss: 0.150049\nTrain Epoch: 50 [2400/2800 (86%)]\tLoss: 0.173442\n\nevaluating...\nTest set:\tAverage loss: 1.3199, Average CER: 0.240008 Average WER: 0.7738\n\nTrain Epoch: 51 [0/2800 (0%)]\tLoss: 0.094749\nTrain Epoch: 51 [400/2800 (14%)]\tLoss: 0.031534\nTrain Epoch: 51 [800/2800 (29%)]\tLoss: 0.051262\nTrain Epoch: 51 [1200/2800 (43%)]\tLoss: 0.096063\nTrain Epoch: 51 [1600/2800 (57%)]\tLoss: 0.107218\nTrain Epoch: 51 [2000/2800 (71%)]\tLoss: 0.088352\nTrain Epoch: 51 [2400/2800 (86%)]\tLoss: 0.170400\n\nevaluating...\nTest set:\tAverage loss: 1.2937, Average CER: 0.237169 Average WER: 0.7688\n\nTrain Epoch: 52 [0/2800 (0%)]\tLoss: 0.044409\nTrain Epoch: 52 [400/2800 (14%)]\tLoss: 0.123530\nTrain Epoch: 52 [800/2800 (29%)]\tLoss: 0.081351\nTrain Epoch: 52 [1200/2800 (43%)]\tLoss: 0.026530\nTrain Epoch: 52 [1600/2800 (57%)]\tLoss: 0.063799\nTrain Epoch: 52 [2000/2800 (71%)]\tLoss: 0.065626\nTrain Epoch: 52 [2400/2800 (86%)]\tLoss: 0.146987\n\nevaluating...\nTest set:\tAverage loss: 1.3129, Average CER: 0.232644 Average WER: 0.7543\n\nTrain Epoch: 53 [0/2800 (0%)]\tLoss: 0.045766\nTrain Epoch: 53 [400/2800 (14%)]\tLoss: 0.078720\nTrain Epoch: 53 [800/2800 (29%)]\tLoss: 0.030145\nTrain Epoch: 53 [1200/2800 (43%)]\tLoss: 0.073624\nTrain Epoch: 53 [1600/2800 (57%)]\tLoss: 0.085747\nTrain Epoch: 53 [2000/2800 (71%)]\tLoss: 0.077996\nTrain Epoch: 53 [2400/2800 (86%)]\tLoss: 0.063142\n\nevaluating...\nTest set:\tAverage loss: 1.3420, Average CER: 0.229979 Average WER: 0.7411\n\nTrain Epoch: 54 [0/2800 (0%)]\tLoss: 0.024667\nTrain Epoch: 54 [400/2800 (14%)]\tLoss: 0.032710\nTrain Epoch: 54 [800/2800 (29%)]\tLoss: 0.041366\nTrain Epoch: 54 [1200/2800 (43%)]\tLoss: 0.039614\nTrain Epoch: 54 [1600/2800 (57%)]\tLoss: 0.028793\nTrain Epoch: 54 [2000/2800 (71%)]\tLoss: 0.071213\nTrain Epoch: 54 [2400/2800 (86%)]\tLoss: 0.051212\n\nevaluating...\nTest set:\tAverage loss: 1.3877, Average CER: 0.228845 Average WER: 0.7339\n\nTrain Epoch: 55 [0/2800 (0%)]\tLoss: 0.017745\nTrain Epoch: 55 [400/2800 (14%)]\tLoss: 0.021336\nTrain Epoch: 55 [800/2800 (29%)]\tLoss: 0.042633\nTrain Epoch: 55 [1200/2800 (43%)]\tLoss: 0.037161\nTrain Epoch: 55 [1600/2800 (57%)]\tLoss: 0.034048\nTrain Epoch: 55 [2000/2800 (71%)]\tLoss: 0.029074\nTrain Epoch: 55 [2400/2800 (86%)]\tLoss: 0.032402\n\nevaluating...\nTest set:\tAverage loss: 1.3770, Average CER: 0.215223 Average WER: 0.7224\n\nTrain Epoch: 56 [0/2800 (0%)]\tLoss: 0.058015\nTrain Epoch: 56 [400/2800 (14%)]\tLoss: 0.063939\nTrain Epoch: 56 [800/2800 (29%)]\tLoss: 0.026838\nTrain Epoch: 56 [1200/2800 (43%)]\tLoss: 0.020665\nTrain Epoch: 56 [1600/2800 (57%)]\tLoss: 0.040829\nTrain Epoch: 56 [2000/2800 (71%)]\tLoss: 0.023349\nTrain Epoch: 56 [2400/2800 (86%)]\tLoss: 0.036188\n\nevaluating...\nTest set:\tAverage loss: 1.3848, Average CER: 0.214089 Average WER: 0.6965\n\nTrain Epoch: 57 [0/2800 (0%)]\tLoss: 0.020244\nTrain Epoch: 57 [400/2800 (14%)]\tLoss: 0.034337\nTrain Epoch: 57 [800/2800 (29%)]\tLoss: 0.010852\nTrain Epoch: 57 [1200/2800 (43%)]\tLoss: 0.007347\nTrain Epoch: 57 [1600/2800 (57%)]\tLoss: 0.014713\nTrain Epoch: 57 [2000/2800 (71%)]\tLoss: 0.029009\nTrain Epoch: 57 [2400/2800 (86%)]\tLoss: 0.072554\n\nevaluating...\nTest set:\tAverage loss: 1.4567, Average CER: 0.224467 Average WER: 0.7363\n\nTrain Epoch: 58 [0/2800 (0%)]\tLoss: 0.013635\nTrain Epoch: 58 [400/2800 (14%)]\tLoss: 0.022727\nTrain Epoch: 58 [800/2800 (29%)]\tLoss: 0.022526\nTrain Epoch: 58 [1200/2800 (43%)]\tLoss: 0.027824\nTrain Epoch: 58 [1600/2800 (57%)]\tLoss: 0.031836\nTrain Epoch: 58 [2000/2800 (71%)]\tLoss: 0.051500\nTrain Epoch: 58 [2400/2800 (86%)]\tLoss: 0.050601\n\nevaluating...\nTest set:\tAverage loss: 1.4451, Average CER: 0.222146 Average WER: 0.7299\n\nTrain Epoch: 59 [0/2800 (0%)]\tLoss: 0.030383\nTrain Epoch: 59 [400/2800 (14%)]\tLoss: 0.025241\nTrain Epoch: 59 [800/2800 (29%)]\tLoss: 0.023281\nTrain Epoch: 59 [1200/2800 (43%)]\tLoss: 0.050501\nTrain Epoch: 59 [1600/2800 (57%)]\tLoss: 0.029998\nTrain Epoch: 59 [2000/2800 (71%)]\tLoss: 0.047813\nTrain Epoch: 59 [2400/2800 (86%)]\tLoss: 0.051348\n\nevaluating...\nTest set:\tAverage loss: 1.4947, Average CER: 0.230295 Average WER: 0.7465\n\nTrain Epoch: 60 [0/2800 (0%)]\tLoss: 0.043143\nTrain Epoch: 60 [400/2800 (14%)]\tLoss: 0.071957\nTrain Epoch: 60 [800/2800 (29%)]\tLoss: 0.036171\nTrain Epoch: 60 [1200/2800 (43%)]\tLoss: 0.070917\nTrain Epoch: 60 [1600/2800 (57%)]\tLoss: 0.065945\nTrain Epoch: 60 [2000/2800 (71%)]\tLoss: 0.146710\nTrain Epoch: 60 [2400/2800 (86%)]\tLoss: 0.081239\n\nevaluating...\nTest set:\tAverage loss: 1.4448, Average CER: 0.227309 Average WER: 0.7390\n\nTrain Epoch: 61 [0/2800 (0%)]\tLoss: 0.077489\nTrain Epoch: 61 [400/2800 (14%)]\tLoss: 0.087744\nTrain Epoch: 61 [800/2800 (29%)]\tLoss: 0.061672\nTrain Epoch: 61 [1200/2800 (43%)]\tLoss: 0.081359\nTrain Epoch: 61 [1600/2800 (57%)]\tLoss: 0.103724\nTrain Epoch: 61 [2000/2800 (71%)]\tLoss: 0.065838\nTrain Epoch: 61 [2400/2800 (86%)]\tLoss: 0.130325\n\nevaluating...\nTest set:\tAverage loss: 1.4355, Average CER: 0.239540 Average WER: 0.7495\n\nTrain Epoch: 62 [0/2800 (0%)]\tLoss: 0.048956\nTrain Epoch: 62 [400/2800 (14%)]\tLoss: 0.103834\nTrain Epoch: 62 [800/2800 (29%)]\tLoss: 0.082033\nTrain Epoch: 62 [1200/2800 (43%)]\tLoss: 0.122971\nTrain Epoch: 62 [1600/2800 (57%)]\tLoss: 0.081180\nTrain Epoch: 62 [2000/2800 (71%)]\tLoss: 0.088494\nTrain Epoch: 62 [2400/2800 (86%)]\tLoss: 0.073612\n\nevaluating...\nTest set:\tAverage loss: 1.3244, Average CER: 0.221461 Average WER: 0.7302\n\nTrain Epoch: 63 [0/2800 (0%)]\tLoss: 0.060021\nTrain Epoch: 63 [400/2800 (14%)]\tLoss: 0.032027\nTrain Epoch: 63 [800/2800 (29%)]\tLoss: 0.027325\nTrain Epoch: 63 [1200/2800 (43%)]\tLoss: 0.026701\nTrain Epoch: 63 [1600/2800 (57%)]\tLoss: 0.035179\nTrain Epoch: 63 [2000/2800 (71%)]\tLoss: 0.030417\nTrain Epoch: 63 [2400/2800 (86%)]\tLoss: 0.019026\n\nevaluating...\nTest set:\tAverage loss: 1.3171, Average CER: 0.208302 Average WER: 0.7084\n\nTrain Epoch: 64 [0/2800 (0%)]\tLoss: 0.004877\nTrain Epoch: 64 [400/2800 (14%)]\tLoss: 0.006205\nTrain Epoch: 64 [800/2800 (29%)]\tLoss: 0.025311\nTrain Epoch: 64 [1200/2800 (43%)]\tLoss: 0.014645\nTrain Epoch: 64 [1600/2800 (57%)]\tLoss: 0.012883\nTrain Epoch: 64 [2000/2800 (71%)]\tLoss: 0.013761\nTrain Epoch: 64 [2400/2800 (86%)]\tLoss: 0.010598\n\nevaluating...\nTest set:\tAverage loss: 1.3848, Average CER: 0.205084 Average WER: 0.7166\n\nTrain Epoch: 65 [0/2800 (0%)]\tLoss: 0.020252\nTrain Epoch: 65 [400/2800 (14%)]\tLoss: 0.005247\nTrain Epoch: 65 [800/2800 (29%)]\tLoss: 0.009021\nTrain Epoch: 65 [1200/2800 (43%)]\tLoss: 0.010792\nTrain Epoch: 65 [1600/2800 (57%)]\tLoss: 0.005352\nTrain Epoch: 65 [2000/2800 (71%)]\tLoss: 0.007113\nTrain Epoch: 65 [2400/2800 (86%)]\tLoss: 0.010448\n\nevaluating...\nTest set:\tAverage loss: 1.3701, Average CER: 0.199397 Average WER: 0.6864\n\nTrain Epoch: 66 [0/2800 (0%)]\tLoss: 0.001492\nTrain Epoch: 66 [400/2800 (14%)]\tLoss: 0.003966\nTrain Epoch: 66 [800/2800 (29%)]\tLoss: 0.001282\nTrain Epoch: 66 [1200/2800 (43%)]\tLoss: 0.002297\nTrain Epoch: 66 [1600/2800 (57%)]\tLoss: 0.003190\nTrain Epoch: 66 [2000/2800 (71%)]\tLoss: 0.004171\nTrain Epoch: 66 [2400/2800 (86%)]\tLoss: 0.003025\n\nevaluating...\nTest set:\tAverage loss: 1.3748, Average CER: 0.196954 Average WER: 0.6841\n\nTrain Epoch: 67 [0/2800 (0%)]\tLoss: 0.001442\nTrain Epoch: 67 [400/2800 (14%)]\tLoss: 0.001743\nTrain Epoch: 67 [800/2800 (29%)]\tLoss: 0.001893\nTrain Epoch: 67 [1200/2800 (43%)]\tLoss: 0.003597\nTrain Epoch: 67 [1600/2800 (57%)]\tLoss: 0.001166\nTrain Epoch: 67 [2000/2800 (71%)]\tLoss: 0.001608\nTrain Epoch: 67 [2400/2800 (86%)]\tLoss: 0.000733\n\nevaluating...\nTest set:\tAverage loss: 1.4005, Average CER: 0.193987 Average WER: 0.6769\n\nTrain Epoch: 68 [0/2800 (0%)]\tLoss: 0.001205\nTrain Epoch: 68 [400/2800 (14%)]\tLoss: 0.000410\nTrain Epoch: 68 [800/2800 (29%)]\tLoss: 0.000610\nTrain Epoch: 68 [1200/2800 (43%)]\tLoss: 0.001452\nTrain Epoch: 68 [1600/2800 (57%)]\tLoss: 0.000761\nTrain Epoch: 68 [2000/2800 (71%)]\tLoss: 0.000751\nTrain Epoch: 68 [2400/2800 (86%)]\tLoss: 0.000413\n\nevaluating...\nTest set:\tAverage loss: 1.4245, Average CER: 0.197187 Average WER: 0.6823\n\nTrain Epoch: 69 [0/2800 (0%)]\tLoss: 0.000506\nTrain Epoch: 69 [400/2800 (14%)]\tLoss: 0.000935\nTrain Epoch: 69 [800/2800 (29%)]\tLoss: 0.001180\nTrain Epoch: 69 [1200/2800 (43%)]\tLoss: 0.000590\nTrain Epoch: 69 [1600/2800 (57%)]\tLoss: 0.000482\nTrain Epoch: 69 [2000/2800 (71%)]\tLoss: 0.000304\nTrain Epoch: 69 [2400/2800 (86%)]\tLoss: 0.000707\n\nevaluating...\nTest set:\tAverage loss: 1.4246, Average CER: 0.194920 Average WER: 0.6813\n\nTrain Epoch: 70 [0/2800 (0%)]\tLoss: 0.000604\nTrain Epoch: 70 [400/2800 (14%)]\tLoss: 0.000868\nTrain Epoch: 70 [800/2800 (29%)]\tLoss: 0.000292\nTrain Epoch: 70 [1200/2800 (43%)]\tLoss: 0.000647\nTrain Epoch: 70 [1600/2800 (57%)]\tLoss: 0.000322\nTrain Epoch: 70 [2000/2800 (71%)]\tLoss: 0.000322\nTrain Epoch: 70 [2400/2800 (86%)]\tLoss: 0.000632\n\nevaluating...\nTest set:\tAverage loss: 1.4424, Average CER: 0.195502 Average WER: 0.6759\n\nTrain Epoch: 71 [0/2800 (0%)]\tLoss: 0.000297\nTrain Epoch: 71 [400/2800 (14%)]\tLoss: 0.000287\nTrain Epoch: 71 [800/2800 (29%)]\tLoss: 0.000889\nTrain Epoch: 71 [1200/2800 (43%)]\tLoss: 0.000318\nTrain Epoch: 71 [1600/2800 (57%)]\tLoss: 0.000226\nTrain Epoch: 71 [2000/2800 (71%)]\tLoss: 0.000351\nTrain Epoch: 71 [2400/2800 (86%)]\tLoss: 0.000217\n\nevaluating...\nTest set:\tAverage loss: 1.4381, Average CER: 0.196374 Average WER: 0.6868\n\nTrain Epoch: 72 [0/2800 (0%)]\tLoss: 0.000265\nTrain Epoch: 72 [400/2800 (14%)]\tLoss: 0.000229\nTrain Epoch: 72 [800/2800 (29%)]\tLoss: 0.000282\nTrain Epoch: 72 [1200/2800 (43%)]\tLoss: 0.000232\nTrain Epoch: 72 [1600/2800 (57%)]\tLoss: 0.000211\nTrain Epoch: 72 [2000/2800 (71%)]\tLoss: 0.000579\nTrain Epoch: 72 [2400/2800 (86%)]\tLoss: 0.000322\n\nevaluating...\nTest set:\tAverage loss: 1.4452, Average CER: 0.195252 Average WER: 0.6851\n\nTrain Epoch: 73 [0/2800 (0%)]\tLoss: 0.000154\nTrain Epoch: 73 [400/2800 (14%)]\tLoss: 0.000731\nTrain Epoch: 73 [800/2800 (29%)]\tLoss: 0.000621\nTrain Epoch: 73 [1200/2800 (43%)]\tLoss: 0.000443\nTrain Epoch: 73 [1600/2800 (57%)]\tLoss: 0.000292\nTrain Epoch: 73 [2000/2800 (71%)]\tLoss: 0.000267\nTrain Epoch: 73 [2400/2800 (86%)]\tLoss: 0.000521\n\nevaluating...\nTest set:\tAverage loss: 1.4558, Average CER: 0.194032 Average WER: 0.6835\n\nTrain Epoch: 74 [0/2800 (0%)]\tLoss: 0.000682\nTrain Epoch: 74 [400/2800 (14%)]\tLoss: 0.000174\nTrain Epoch: 74 [800/2800 (29%)]\tLoss: 0.000326\nTrain Epoch: 74 [1200/2800 (43%)]\tLoss: 0.000260\nTrain Epoch: 74 [1600/2800 (57%)]\tLoss: 0.000336\nTrain Epoch: 74 [2000/2800 (71%)]\tLoss: 0.000185\nTrain Epoch: 74 [2400/2800 (86%)]\tLoss: 0.000145\n\nevaluating...\nTest set:\tAverage loss: 1.4717, Average CER: 0.196863 Average WER: 0.6853\n\nTrain Epoch: 75 [0/2800 (0%)]\tLoss: 0.000364\nTrain Epoch: 75 [400/2800 (14%)]\tLoss: 0.000337\nTrain Epoch: 75 [800/2800 (29%)]\tLoss: 0.000155\nTrain Epoch: 75 [1200/2800 (43%)]\tLoss: 0.000208\nTrain Epoch: 75 [1600/2800 (57%)]\tLoss: 0.000191\nTrain Epoch: 75 [2000/2800 (71%)]\tLoss: 0.000202\nTrain Epoch: 75 [2400/2800 (86%)]\tLoss: 0.000158\n\nevaluating...\nTest set:\tAverage loss: 1.4687, Average CER: 0.196312 Average WER: 0.6832\n\nTrain Epoch: 76 [0/2800 (0%)]\tLoss: 0.000180\nTrain Epoch: 76 [400/2800 (14%)]\tLoss: 0.000153\nTrain Epoch: 76 [800/2800 (29%)]\tLoss: 0.000389\nTrain Epoch: 76 [1200/2800 (43%)]\tLoss: 0.000716\nTrain Epoch: 76 [1600/2800 (57%)]\tLoss: 0.000230\nTrain Epoch: 76 [2000/2800 (71%)]\tLoss: 0.000165\nTrain Epoch: 76 [2400/2800 (86%)]\tLoss: 0.000132\n\nevaluating...\nTest set:\tAverage loss: 1.4818, Average CER: 0.194793 Average WER: 0.6834\n\nTrain Epoch: 77 [0/2800 (0%)]\tLoss: 0.000109\nTrain Epoch: 77 [400/2800 (14%)]\tLoss: 0.000774\nTrain Epoch: 77 [800/2800 (29%)]\tLoss: 0.000465\nTrain Epoch: 77 [1200/2800 (43%)]\tLoss: 0.000263\nTrain Epoch: 77 [1600/2800 (57%)]\tLoss: 0.000104\nTrain Epoch: 77 [2000/2800 (71%)]\tLoss: 0.000136\nTrain Epoch: 77 [2400/2800 (86%)]\tLoss: 0.000295\n\nevaluating...\nTest set:\tAverage loss: 1.4836, Average CER: 0.193166 Average WER: 0.6840\n\nTrain Epoch: 78 [0/2800 (0%)]\tLoss: 0.000158\nTrain Epoch: 78 [400/2800 (14%)]\tLoss: 0.000166\nTrain Epoch: 78 [800/2800 (29%)]\tLoss: 0.000111\nTrain Epoch: 78 [1200/2800 (43%)]\tLoss: 0.000095\nTrain Epoch: 78 [1600/2800 (57%)]\tLoss: 0.000099\nTrain Epoch: 78 [2000/2800 (71%)]\tLoss: 0.000137\nTrain Epoch: 78 [2400/2800 (86%)]\tLoss: 0.000311\n\nevaluating...\nTest set:\tAverage loss: 1.4887, Average CER: 0.192945 Average WER: 0.6794\n\nTrain Epoch: 79 [0/2800 (0%)]\tLoss: 0.000395\nTrain Epoch: 79 [400/2800 (14%)]\tLoss: 0.000274\nTrain Epoch: 79 [800/2800 (29%)]\tLoss: 0.000077\nTrain Epoch: 79 [1200/2800 (43%)]\tLoss: 0.000136\nTrain Epoch: 79 [1600/2800 (57%)]\tLoss: 0.000128\nTrain Epoch: 79 [2000/2800 (71%)]\tLoss: 0.000238\nTrain Epoch: 79 [2400/2800 (86%)]\tLoss: 0.000119\n\nevaluating...\nTest set:\tAverage loss: 1.4926, Average CER: 0.194240 Average WER: 0.6813\n\nTrain Epoch: 80 [0/2800 (0%)]\tLoss: 0.000216\nTrain Epoch: 80 [400/2800 (14%)]\tLoss: 0.000138\nTrain Epoch: 80 [800/2800 (29%)]\tLoss: 0.000113\nTrain Epoch: 80 [1200/2800 (43%)]\tLoss: 0.000100\nTrain Epoch: 80 [1600/2800 (57%)]\tLoss: 0.000136\nTrain Epoch: 80 [2000/2800 (71%)]\tLoss: 0.000124\nTrain Epoch: 80 [2400/2800 (86%)]\tLoss: 0.000092\n\nevaluating...\nTest set:\tAverage loss: 1.4992, Average CER: 0.193803 Average WER: 0.6799\n\nTrain Epoch: 81 [0/2800 (0%)]\tLoss: 0.002841\nTrain Epoch: 81 [400/2800 (14%)]\tLoss: 0.000111\nTrain Epoch: 81 [800/2800 (29%)]\tLoss: 0.000138\nTrain Epoch: 81 [1200/2800 (43%)]\tLoss: 0.000055\nTrain Epoch: 81 [1600/2800 (57%)]\tLoss: 0.000164\nTrain Epoch: 81 [2000/2800 (71%)]\tLoss: 0.000088\nTrain Epoch: 81 [2400/2800 (86%)]\tLoss: 0.000127\n\nevaluating...\nTest set:\tAverage loss: 1.4992, Average CER: 0.193723 Average WER: 0.6800\n\nTrain Epoch: 82 [0/2800 (0%)]\tLoss: 0.000084\nTrain Epoch: 82 [400/2800 (14%)]\tLoss: 0.000162\nTrain Epoch: 82 [800/2800 (29%)]\tLoss: 0.000105\nTrain Epoch: 82 [1200/2800 (43%)]\tLoss: 0.000073\nTrain Epoch: 82 [1600/2800 (57%)]\tLoss: 0.000055\nTrain Epoch: 82 [2000/2800 (71%)]\tLoss: 0.000083\nTrain Epoch: 82 [2400/2800 (86%)]\tLoss: 0.000139\n\nevaluating...\nTest set:\tAverage loss: 1.5036, Average CER: 0.194506 Average WER: 0.6828\n\nTrain Epoch: 83 [0/2800 (0%)]\tLoss: 0.000120\nTrain Epoch: 83 [400/2800 (14%)]\tLoss: 0.000174\nTrain Epoch: 83 [800/2800 (29%)]\tLoss: 0.000110\nTrain Epoch: 83 [1200/2800 (43%)]\tLoss: 0.000783\nTrain Epoch: 83 [1600/2800 (57%)]\tLoss: 0.000119\nTrain Epoch: 83 [2000/2800 (71%)]\tLoss: 0.000332\nTrain Epoch: 83 [2400/2800 (86%)]\tLoss: 0.000341\n\nevaluating...\nTest set:\tAverage loss: 1.5067, Average CER: 0.194201 Average WER: 0.6797\n\nTrain Epoch: 84 [0/2800 (0%)]\tLoss: 0.000149\nTrain Epoch: 84 [400/2800 (14%)]\tLoss: 0.000194\nTrain Epoch: 84 [800/2800 (29%)]\tLoss: 0.000150\nTrain Epoch: 84 [1200/2800 (43%)]\tLoss: 0.000314\nTrain Epoch: 84 [1600/2800 (57%)]\tLoss: 0.000104\nTrain Epoch: 84 [2000/2800 (71%)]\tLoss: 0.000068\nTrain Epoch: 84 [2400/2800 (86%)]\tLoss: 0.000083\n\nevaluating...\nTest set:\tAverage loss: 1.5153, Average CER: 0.194217 Average WER: 0.6810\n\nTrain Epoch: 85 [0/2800 (0%)]\tLoss: 0.000130\nTrain Epoch: 85 [400/2800 (14%)]\tLoss: 0.000127\nTrain Epoch: 85 [800/2800 (29%)]\tLoss: 0.000095\nTrain Epoch: 85 [1200/2800 (43%)]\tLoss: 0.000055\nTrain Epoch: 85 [1600/2800 (57%)]\tLoss: 0.000200\nTrain Epoch: 85 [2000/2800 (71%)]\tLoss: 0.000097\nTrain Epoch: 85 [2400/2800 (86%)]\tLoss: 0.000144\n\nevaluating...\nTest set:\tAverage loss: 1.5161, Average CER: 0.193579 Average WER: 0.6817\n\nTrain Epoch: 86 [0/2800 (0%)]\tLoss: 0.000088\nTrain Epoch: 86 [400/2800 (14%)]\tLoss: 0.000106\nTrain Epoch: 86 [800/2800 (29%)]\tLoss: 0.000050\nTrain Epoch: 86 [1200/2800 (43%)]\tLoss: 0.000103\nTrain Epoch: 86 [1600/2800 (57%)]\tLoss: 0.000283\nTrain Epoch: 86 [2000/2800 (71%)]\tLoss: 0.000079\nTrain Epoch: 86 [2400/2800 (86%)]\tLoss: 0.000063\n\nevaluating...\nTest set:\tAverage loss: 1.5251, Average CER: 0.193842 Average WER: 0.6846\n\nTrain Epoch: 87 [0/2800 (0%)]\tLoss: 0.000237\nTrain Epoch: 87 [400/2800 (14%)]\tLoss: 0.000104\nTrain Epoch: 87 [800/2800 (29%)]\tLoss: 0.000167\nTrain Epoch: 87 [1200/2800 (43%)]\tLoss: 0.000219\nTrain Epoch: 87 [1600/2800 (57%)]\tLoss: 0.000227\nTrain Epoch: 87 [2000/2800 (71%)]\tLoss: 0.000086\nTrain Epoch: 87 [2400/2800 (86%)]\tLoss: 0.000134\n\nevaluating...\nTest set:\tAverage loss: 1.5343, Average CER: 0.194350 Average WER: 0.6830\n\nTrain Epoch: 88 [0/2800 (0%)]\tLoss: 0.000068\nTrain Epoch: 88 [400/2800 (14%)]\tLoss: 0.000132\nTrain Epoch: 88 [800/2800 (29%)]\tLoss: 0.000203\nTrain Epoch: 88 [1200/2800 (43%)]\tLoss: 0.000067\nTrain Epoch: 88 [1600/2800 (57%)]\tLoss: 0.000140\nTrain Epoch: 88 [2000/2800 (71%)]\tLoss: 0.000084\nTrain Epoch: 88 [2400/2800 (86%)]\tLoss: 0.000065\n\nevaluating...\nTest set:\tAverage loss: 1.5304, Average CER: 0.193094 Average WER: 0.6834\n\nTrain Epoch: 89 [0/2800 (0%)]\tLoss: 0.000064\nTrain Epoch: 89 [400/2800 (14%)]\tLoss: 0.000042\nTrain Epoch: 89 [800/2800 (29%)]\tLoss: 0.000241\nTrain Epoch: 89 [1200/2800 (43%)]\tLoss: 0.000048\nTrain Epoch: 89 [1600/2800 (57%)]\tLoss: 0.000164\nTrain Epoch: 89 [2000/2800 (71%)]\tLoss: 0.000353\nTrain Epoch: 89 [2400/2800 (86%)]\tLoss: 0.000796\n\nevaluating...\nTest set:\tAverage loss: 1.5399, Average CER: 0.194727 Average WER: 0.6826\n\nTrain Epoch: 90 [0/2800 (0%)]\tLoss: 0.000137\nTrain Epoch: 90 [400/2800 (14%)]\tLoss: 0.000078\nTrain Epoch: 90 [800/2800 (29%)]\tLoss: 0.000096\nTrain Epoch: 90 [1200/2800 (43%)]\tLoss: 0.000195\nTrain Epoch: 90 [1600/2800 (57%)]\tLoss: 0.000071\nTrain Epoch: 90 [2000/2800 (71%)]\tLoss: 0.000072\nTrain Epoch: 90 [2400/2800 (86%)]\tLoss: 0.000050\n\nevaluating...\nTest set:\tAverage loss: 1.5451, Average CER: 0.193395 Average WER: 0.6828\n\nTrain Epoch: 91 [0/2800 (0%)]\tLoss: 0.000074\nTrain Epoch: 91 [400/2800 (14%)]\tLoss: 0.000069\nTrain Epoch: 91 [800/2800 (29%)]\tLoss: 0.000480\nTrain Epoch: 91 [1200/2800 (43%)]\tLoss: 0.000071\nTrain Epoch: 91 [1600/2800 (57%)]\tLoss: 0.000064\nTrain Epoch: 91 [2000/2800 (71%)]\tLoss: 0.000138\nTrain Epoch: 91 [2400/2800 (86%)]\tLoss: 0.000280\n\nevaluating...\nTest set:\tAverage loss: 1.5471, Average CER: 0.193181 Average WER: 0.6800\n\nTrain Epoch: 92 [0/2800 (0%)]\tLoss: 0.000106\nTrain Epoch: 92 [400/2800 (14%)]\tLoss: 0.000053\nTrain Epoch: 92 [800/2800 (29%)]\tLoss: 0.000067\nTrain Epoch: 92 [1200/2800 (43%)]\tLoss: 0.000078\nTrain Epoch: 92 [1600/2800 (57%)]\tLoss: 0.000068\nTrain Epoch: 92 [2000/2800 (71%)]\tLoss: 0.000089\nTrain Epoch: 92 [2400/2800 (86%)]\tLoss: 0.000072\n\nevaluating...\nTest set:\tAverage loss: 1.5435, Average CER: 0.192143 Average WER: 0.6788\n\nTrain Epoch: 93 [0/2800 (0%)]\tLoss: 0.000062\nTrain Epoch: 93 [400/2800 (14%)]\tLoss: 0.000053\nTrain Epoch: 93 [800/2800 (29%)]\tLoss: 0.000091\nTrain Epoch: 93 [1200/2800 (43%)]\tLoss: 0.000056\nTrain Epoch: 93 [1600/2800 (57%)]\tLoss: 0.000034\nTrain Epoch: 93 [2000/2800 (71%)]\tLoss: 0.000069\nTrain Epoch: 93 [2400/2800 (86%)]\tLoss: 0.000074\n\nevaluating...\nTest set:\tAverage loss: 1.5591, Average CER: 0.193442 Average WER: 0.6798\n\nTrain Epoch: 94 [0/2800 (0%)]\tLoss: 0.000059\nTrain Epoch: 94 [400/2800 (14%)]\tLoss: 0.000119\nTrain Epoch: 94 [800/2800 (29%)]\tLoss: 0.000278\nTrain Epoch: 94 [1200/2800 (43%)]\tLoss: 0.000133\nTrain Epoch: 94 [1600/2800 (57%)]\tLoss: 0.000061\nTrain Epoch: 94 [2000/2800 (71%)]\tLoss: 0.000064\nTrain Epoch: 94 [2400/2800 (86%)]\tLoss: 0.000082\n\nevaluating...\nTest set:\tAverage loss: 1.5549, Average CER: 0.192140 Average WER: 0.6787\n\nTrain Epoch: 95 [0/2800 (0%)]\tLoss: 0.000066\nTrain Epoch: 95 [400/2800 (14%)]\tLoss: 0.000103\nTrain Epoch: 95 [800/2800 (29%)]\tLoss: 0.000056\nTrain Epoch: 95 [1200/2800 (43%)]\tLoss: 0.000064\nTrain Epoch: 95 [1600/2800 (57%)]\tLoss: 0.000121\nTrain Epoch: 95 [2000/2800 (71%)]\tLoss: 0.000081\nTrain Epoch: 95 [2400/2800 (86%)]\tLoss: 0.000049\n\nevaluating...\nTest set:\tAverage loss: 1.5608, Average CER: 0.192439 Average WER: 0.6791\n\nTrain Epoch: 96 [0/2800 (0%)]\tLoss: 0.000059\nTrain Epoch: 96 [400/2800 (14%)]\tLoss: 0.000070\nTrain Epoch: 96 [800/2800 (29%)]\tLoss: 0.000118\nTrain Epoch: 96 [1200/2800 (43%)]\tLoss: 0.000065\nTrain Epoch: 96 [1600/2800 (57%)]\tLoss: 0.000057\nTrain Epoch: 96 [2000/2800 (71%)]\tLoss: 0.000079\nTrain Epoch: 96 [2400/2800 (86%)]\tLoss: 0.000065\n\nevaluating...\nTest set:\tAverage loss: 1.5628, Average CER: 0.192990 Average WER: 0.6832\n\nTrain Epoch: 97 [0/2800 (0%)]\tLoss: 0.000061\nTrain Epoch: 97 [400/2800 (14%)]\tLoss: 0.000085\nTrain Epoch: 97 [800/2800 (29%)]\tLoss: 0.000042\nTrain Epoch: 97 [1200/2800 (43%)]\tLoss: 0.000091\nTrain Epoch: 97 [1600/2800 (57%)]\tLoss: 0.000075\nTrain Epoch: 97 [2000/2800 (71%)]\tLoss: 0.000037\nTrain Epoch: 97 [2400/2800 (86%)]\tLoss: 0.000074\n\nevaluating...\nTest set:\tAverage loss: 1.5574, Average CER: 0.192548 Average WER: 0.6813\n\nTrain Epoch: 98 [0/2800 (0%)]\tLoss: 0.000140\nTrain Epoch: 98 [400/2800 (14%)]\tLoss: 0.000084\nTrain Epoch: 98 [800/2800 (29%)]\tLoss: 0.000046\nTrain Epoch: 98 [1200/2800 (43%)]\tLoss: 0.000040\nTrain Epoch: 98 [1600/2800 (57%)]\tLoss: 0.000054\nTrain Epoch: 98 [2000/2800 (71%)]\tLoss: 0.000077\nTrain Epoch: 98 [2400/2800 (86%)]\tLoss: 0.000129\n\nevaluating...\nTest set:\tAverage loss: 1.5634, Average CER: 0.192456 Average WER: 0.6807\n\nTrain Epoch: 99 [0/2800 (0%)]\tLoss: 0.000047\nTrain Epoch: 99 [400/2800 (14%)]\tLoss: 0.000105\nTrain Epoch: 99 [800/2800 (29%)]\tLoss: 0.000062\nTrain Epoch: 99 [1200/2800 (43%)]\tLoss: 0.000042\nTrain Epoch: 99 [1600/2800 (57%)]\tLoss: 0.000093\nTrain Epoch: 99 [2000/2800 (71%)]\tLoss: 0.000085\nTrain Epoch: 99 [2400/2800 (86%)]\tLoss: 0.000059\n\nevaluating...\nTest set:\tAverage loss: 1.5627, Average CER: 0.192206 Average WER: 0.6801\n\nTrain Epoch: 100 [0/2800 (0%)]\tLoss: 0.000074\nTrain Epoch: 100 [400/2800 (14%)]\tLoss: 0.000200\nTrain Epoch: 100 [800/2800 (29%)]\tLoss: 0.000050\nTrain Epoch: 100 [1200/2800 (43%)]\tLoss: 0.000529\nTrain Epoch: 100 [1600/2800 (57%)]\tLoss: 0.000155\nTrain Epoch: 100 [2000/2800 (71%)]\tLoss: 0.000052\nTrain Epoch: 100 [2400/2800 (86%)]\tLoss: 0.000098\n\nevaluating...\nTest set:\tAverage loss: 1.5635, Average CER: 0.192369 Average WER: 0.6794\n\nCPU times: user 1h 3min 31s, sys: 3min 10s, total: 1h 6min 41s\nWall time: 1h 6min 33s\n","output_type":"stream"}]},{"cell_type":"code","source":"#use_cuda = torch.cuda.is_available()\n#device = torch.device(\"cpu\")\nneeded_device = torch.device(\"cpu\")\nmodel = torch.load('/kaggle/input/dop-test-files/model_for_making_dataset_v6(20024).pt', map_location=torch.device('cpu'))\n\n#1543 1882 1372\n\nmodel.to(needed_device)\nprint(needed_device)\n#predict(model, '/kaggle/input/upd-speech/mono_voice/1964.wav', device)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.003288Z","iopub.status.idle":"2024-06-09T08:55:23.004019Z","shell.execute_reply.started":"2024-06-09T08:55:23.003632Z","shell.execute_reply":"2024-06-09T08:55:23.003668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = {'X_test': X_test, 'label': y_test}\ndf_test = pd.DataFrame(data=d)\ndf_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.005854Z","iopub.status.idle":"2024-06-09T08:55:23.006533Z","shell.execute_reply.started":"2024-06-09T08:55:23.006189Z","shell.execute_reply":"2024-06-09T08:55:23.006228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[:5]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.008255Z","iopub.status.idle":"2024-06-09T08:55:23.008943Z","shell.execute_reply.started":"2024-06-09T08:55:23.008573Z","shell.execute_reply":"2024-06-09T08:55:23.008611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_test_cer(row, model):\n    prediction = predict_with_tensor_v3(model, row['X_test'])\n    return cer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.011078Z","iopub.status.idle":"2024-06-09T08:55:23.011750Z","shell.execute_reply.started":"2024-06-09T08:55:23.011406Z","shell.execute_reply":"2024-06-09T08:55:23.011443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_test_wer(row, model):\n    prediction = predict_with_tensor_v3(model, row['X_test'])\n    return wer(row['label'], prediction)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.013505Z","iopub.status.idle":"2024-06-09T08:55:23.014209Z","shell.execute_reply.started":"2024-06-09T08:55:23.013856Z","shell.execute_reply":"2024-06-09T08:55:23.013894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_preds(row, model):\n    return predict_with_tensor_v3(model, row['X_test'])","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.016372Z","iopub.status.idle":"2024-06-09T08:55:23.017052Z","shell.execute_reply.started":"2024-06-09T08:55:23.016688Z","shell.execute_reply":"2024-06-09T08:55:23.016727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['CER'] = df_test.apply(count_test_cer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.018723Z","iopub.status.idle":"2024-06-09T08:55:23.019394Z","shell.execute_reply.started":"2024-06-09T08:55:23.019063Z","shell.execute_reply":"2024-06-09T08:55:23.019099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['WER'] = df_test.apply(count_test_wer, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.021600Z","iopub.status.idle":"2024-06-09T08:55:23.022289Z","shell.execute_reply.started":"2024-06-09T08:55:23.021957Z","shell.execute_reply":"2024-06-09T08:55:23.021992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['preds'] = df_test.apply(write_preds, axis=1, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.024111Z","iopub.status.idle":"2024-06-09T08:55:23.024770Z","shell.execute_reply.started":"2024-06-09T08:55:23.024431Z","shell.execute_reply":"2024-06-09T08:55:23.024466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.loc[df_test['CER'] > 0]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.026421Z","iopub.status.idle":"2024-06-09T08:55:23.027097Z","shell.execute_reply.started":"2024-06-09T08:55:23.026721Z","shell.execute_reply":"2024-06-09T08:55:23.026767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Проверяем затюненый корректор t5 (на 10 эпохах)","metadata":{}},{"cell_type":"code","source":"print('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.029040Z","iopub.status.idle":"2024-06-09T08:55:23.029661Z","shell.execute_reply.started":"2024-06-09T08:55:23.029336Z","shell.execute_reply":"2024-06-09T08:55:23.029369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ИСПОЛЬЗОВАЛ МАЛЫЙ СЛОВАРЬ","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.033020Z","iopub.status.idle":"2024-06-09T08:55:23.033700Z","shell.execute_reply.started":"2024-06-09T08:55:23.033365Z","shell.execute_reply":"2024-06-09T08:55:23.033399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model(3024_seed_data).pt with hunspell. WITHOUT HUNSPELL: CER = Average CER: 0.166791 Average WER: 0.6451","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.036199Z","iopub.status.idle":"2024-06-09T08:55:23.036893Z","shell.execute_reply.started":"2024-06-09T08:55:23.036521Z","shell.execute_reply":"2024-06-09T08:55:23.036558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['CER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.038843Z","iopub.status.idle":"2024-06-09T08:55:23.039513Z","shell.execute_reply.started":"2024-06-09T08:55:23.039180Z","shell.execute_reply":"2024-06-09T08:55:23.039216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['WER'].mean()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.041668Z","iopub.status.idle":"2024-06-09T08:55:23.042349Z","shell.execute_reply.started":"2024-06-09T08:55:23.042006Z","shell.execute_reply":"2024-06-09T08:55:23.042049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v4(1242).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.044398Z","iopub.status.idle":"2024-06-09T08:55:23.045044Z","shell.execute_reply.started":"2024-06-09T08:55:23.044704Z","shell.execute_reply":"2024-06-09T08:55:23.044735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v5(2204).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.046849Z","iopub.status.idle":"2024-06-09T08:55:23.047441Z","shell.execute_reply.started":"2024-06-09T08:55:23.047144Z","shell.execute_reply":"2024-06-09T08:55:23.047174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v6(20024).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.049422Z","iopub.status.idle":"2024-06-09T08:55:23.050101Z","shell.execute_reply.started":"2024-06-09T08:55:23.049761Z","shell.execute_reply":"2024-06-09T08:55:23.049814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v7(3016).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.052464Z","iopub.status.idle":"2024-06-09T08:55:23.053149Z","shell.execute_reply.started":"2024-06-09T08:55:23.052774Z","shell.execute_reply":"2024-06-09T08:55:23.052834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ДАЛЕЕ ИСПОЛЬЗУЕТСЯ БОЛЬШИЙ СЛОВАРЬ","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.054450Z","iopub.status.idle":"2024-06-09T08:55:23.055087Z","shell.execute_reply.started":"2024-06-09T08:55:23.054737Z","shell.execute_reply":"2024-06-09T08:55:23.054771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v4(1242).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.057101Z","iopub.status.idle":"2024-06-09T08:55:23.057793Z","shell.execute_reply.started":"2024-06-09T08:55:23.057436Z","shell.execute_reply":"2024-06-09T08:55:23.057471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v5(2204).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.059786Z","iopub.status.idle":"2024-06-09T08:55:23.060475Z","shell.execute_reply.started":"2024-06-09T08:55:23.060136Z","shell.execute_reply":"2024-06-09T08:55:23.060172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v6(20024).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.063125Z","iopub.status.idle":"2024-06-09T08:55:23.063903Z","shell.execute_reply.started":"2024-06-09T08:55:23.063448Z","shell.execute_reply":"2024-06-09T08:55:23.063483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using model_for_making_dataset_v7(3016).pt with hunspell. WITHOUT HUNSPELL: Average CER ~ 0.16 Average WER ~ 0.64\nprint('CER: ', df_test['CER'].mean())\nprint('WER: ', df_test['WER'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.066083Z","iopub.status.idle":"2024-06-09T08:55:23.066936Z","shell.execute_reply.started":"2024-06-09T08:55:23.066416Z","shell.execute_reply":"2024-06-09T08:55:23.066451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.069448Z","iopub.status.idle":"2024-06-09T08:55:23.070231Z","shell.execute_reply.started":"2024-06-09T08:55:23.069882Z","shell.execute_reply":"2024-06-09T08:55:23.069921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.072319Z","iopub.status.idle":"2024-06-09T08:55:23.073046Z","shell.execute_reply.started":"2024-06-09T08:55:23.072670Z","shell.execute_reply":"2024-06-09T08:55:23.072709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(directory):\n    total_duration = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            filepath = os.path.join(directory, filename)\n            with wave.open(filepath, 'r') as wav_file:\n                frames = wav_file.getnframes()\n                rate = wav_file.getframerate()\n                duration = frames / float(rate)\n                total_duration += duration\n    return total_duration\n\ndirectory = '/kaggle/input/upd-speech/mono_voice'\ntotal_duration = get_wav_duration(directory)\nprint('Total duration of WAV files:', total_duration, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.074771Z","iopub.status.idle":"2024-06-09T08:55:23.075468Z","shell.execute_reply.started":"2024-06-09T08:55:23.075122Z","shell.execute_reply":"2024-06-09T08:55:23.075158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return '{:02d}:{:02d}:{:02d}'.format(int(hours), int(minutes), int(seconds))\nseconds = 3661\nformatted_time = format_time(total_duration)\nprint(formatted_time)  # Output: '01:01:01'","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:55:23.076971Z","iopub.status.idle":"2024-06-09T08:55:23.077742Z","shell.execute_reply.started":"2024-06-09T08:55:23.077292Z","shell.execute_reply":"2024-06-09T08:55:23.077326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}