{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8456387,"sourceType":"datasetVersion","datasetId":3213578}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, GRU, Dense\nfrom transformers import AutoTokenizer\nfrom tensorflow.keras.models import Sequential\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:20:13.599141Z","iopub.execute_input":"2024-05-19T16:20:13.599862Z","iopub.status.idle":"2024-05-19T16:20:39.335425Z","shell.execute_reply.started":"2024-05-19T16:20:13.599829Z","shell.execute_reply":"2024-05-19T16:20:39.334628Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-19 16:20:16.789920: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-19 16:20:16.790059: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-19 16:20:17.057452: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Sample data preparation (assuming you have your data in a pandas DataFrame)\n# Replace this with your actual DataFrame\ndata = pd.DataFrame({\n    'input_text': ['i ike eting cakes', 'i go to shool everydy'],\n    'target_text': ['i like eating cakes', 'i go to school everyday']\n})\n\ndata = pd.read_csv('/kaggle/input/dop-test-files/errors.csv').drop(columns=['Unnamed: 0'])\ndata","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:07:19.103263Z","iopub.execute_input":"2024-05-19T09:07:19.104309Z","iopub.status.idle":"2024-05-19T09:07:19.130628Z","shell.execute_reply.started":"2024-05-19T09:07:19.104272Z","shell.execute_reply":"2024-05-19T09:07:19.129675Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                           label  \\\n0                              иностранный агент   \n1          свидетельствуют о проблемах с печенью   \n2     найдите способ быть полезными другим людям   \n3                  я уже поставил белье в стирку   \n4                                    круглый мяч   \n...                                          ...   \n1475                    летим в отпуск на гавайи   \n1476                           кротовая настойка   \n1477         думаю нам пора расходиться по домам   \n1478                 красивые цветы украшают сад   \n1479     кока кола зеро все еще такая же вкусная   \n\n                                           preds  \n0                               иностранный аген  \n1            свидетельствуют о праблемах спецнью  \n2     найдиче сьпособ быть полезном другим людем  \n3                    я уже поставил бельо встиру  \n4                                  круглый мядчь  \n...                                          ...  \n1475                     влетим в отпуск наговаи  \n1476                          кротовая на стойка  \n1477           домаю ном порарасходится по домам  \n1478                 красивые цветы укрошают сад  \n1479       укаколо зыру в сё ечу такаеже вкусное  \n\n[1480 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>иностранный агент</td>\n      <td>иностранный аген</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>свидетельствуют о проблемах с печенью</td>\n      <td>свидетельствуют о праблемах спецнью</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>найдите способ быть полезными другим людям</td>\n      <td>найдиче сьпособ быть полезном другим людем</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>я уже поставил белье в стирку</td>\n      <td>я уже поставил бельо встиру</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>круглый мяч</td>\n      <td>круглый мядчь</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1475</th>\n      <td>летим в отпуск на гавайи</td>\n      <td>влетим в отпуск наговаи</td>\n    </tr>\n    <tr>\n      <th>1476</th>\n      <td>кротовая настойка</td>\n      <td>кротовая на стойка</td>\n    </tr>\n    <tr>\n      <th>1477</th>\n      <td>думаю нам пора расходиться по домам</td>\n      <td>домаю ном порарасходится по домам</td>\n    </tr>\n    <tr>\n      <th>1478</th>\n      <td>красивые цветы украшают сад</td>\n      <td>красивые цветы укрошают сад</td>\n    </tr>\n    <tr>\n      <th>1479</th>\n      <td>кока кола зеро все еще такая же вкусная</td>\n      <td>укаколо зыру в сё ечу такаеже вкусное</td>\n    </tr>\n  </tbody>\n</table>\n<p>1480 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenization and Padding\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data['preds'].tolist() + data['label'].tolist())\n\n# Convert text to sequences\ninput_sequences = tokenizer.texts_to_sequences(data['preds'].tolist())\ntarget_sequences = tokenizer.texts_to_sequences(data['label'].tolist())\n\n# Padding sequences to the same length\nmax_seq_len = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in target_sequences))\npadded_input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='post')\npadded_target_sequences = pad_sequences(target_sequences, maxlen=max_seq_len, padding='post')\n\n# Define model parameters\nvocab_size = len(tokenizer.word_index) + 1\nembedding_dim = 1024\nrnn_units = 512","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:13:37.859781Z","iopub.execute_input":"2024-05-19T09:13:37.860609Z","iopub.status.idle":"2024-05-19T09:13:37.977520Z","shell.execute_reply.started":"2024-05-19T09:13:37.860575Z","shell.execute_reply":"2024-05-19T09:13:37.976634Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"input_sequences[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:27:23.950120Z","iopub.execute_input":"2024-05-19T09:27:23.950976Z","iopub.status.idle":"2024-05-19T09:27:23.958082Z","shell.execute_reply.started":"2024-05-19T09:27:23.950941Z","shell.execute_reply":"2024-05-19T09:27:23.956956Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"[[585, 1308],\n [586, 26, 1309, 1310],\n [1311, 1312, 30, 1313, 262, 1314],\n [4, 24, 381, 1315, 1316],\n [587, 1317],\n [263, 32, 1318],\n [588, 1319, 1320, 264],\n [589, 1321],\n [1322, 590, 2, 1323, 1324, 18, 203],\n [164, 204, 2, 265, 1325]]"},"metadata":{}}]},{"cell_type":"code","source":"target_sequences[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:27:29.420894Z","iopub.execute_input":"2024-05-19T09:27:29.421868Z","iopub.status.idle":"2024-05-19T09:27:29.428941Z","shell.execute_reply.started":"2024-05-19T09:27:29.421831Z","shell.execute_reply":"2024-05-19T09:27:29.427877Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"[[585, 4166],\n [586, 26, 4167, 7, 4168],\n [4169, 160, 30, 4170, 262, 4171],\n [4, 24, 381, 1166, 1, 4172],\n [587, 4173],\n [263, 32, 4174],\n [588, 551, 4175, 264],\n [589, 4176],\n [4177, 590, 2, 4178, 131, 18, 203],\n [164, 204, 2, 265, 165]]"},"metadata":{}}]},{"cell_type":"code","source":"# Define the model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_len))\nmodel.add(LSTM(rnn_units, return_sequences=True))\nmodel.add(Dense(vocab_size, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Prepare target data for training\npadded_target_sequences = np.expand_dims(padded_target_sequences, -1)\n\n# Train the model\nmodel.fit(padded_input_sequences, padded_target_sequences, epochs=50, batch_size=50)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:13:38.326546Z","iopub.execute_input":"2024-05-19T09:13:38.327084Z","iopub.status.idle":"2024-05-19T09:14:35.946004Z","shell.execute_reply.started":"2024-05-19T09:13:38.327050Z","shell.execute_reply":"2024-05-19T09:14:35.944995Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6166 - loss: 5.2630\nEpoch 2/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7100 - loss: 2.6536\nEpoch 3/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.7089 - loss: 2.5459\nEpoch 4/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.7055 - loss: 2.4733\nEpoch 5/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7180 - loss: 2.2910\nEpoch 6/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7084 - loss: 2.2849\nEpoch 7/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7151 - loss: 2.1496\nEpoch 8/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7204 - loss: 2.0189\nEpoch 9/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7166 - loss: 1.9433\nEpoch 10/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7232 - loss: 1.8184\nEpoch 11/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7303 - loss: 1.7006\nEpoch 12/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7384 - loss: 1.5754\nEpoch 13/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7329 - loss: 1.5282\nEpoch 14/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7504 - loss: 1.4008\nEpoch 15/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7663 - loss: 1.2846\nEpoch 16/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7861 - loss: 1.1731\nEpoch 17/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8044 - loss: 1.0850\nEpoch 18/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8306 - loss: 0.9635\nEpoch 19/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8482 - loss: 0.8670\nEpoch 20/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8774 - loss: 0.7321\nEpoch 21/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8920 - loss: 0.6512\nEpoch 22/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9127 - loss: 0.5628\nEpoch 23/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9229 - loss: 0.4852\nEpoch 24/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9386 - loss: 0.4087\nEpoch 25/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9531 - loss: 0.3388\nEpoch 26/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9619 - loss: 0.2885\nEpoch 27/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9738 - loss: 0.2349\nEpoch 28/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9794 - loss: 0.1976\nEpoch 29/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9840 - loss: 0.1613\nEpoch 30/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9875 - loss: 0.1363\nEpoch 31/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9895 - loss: 0.1184\nEpoch 32/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9922 - loss: 0.0947\nEpoch 33/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9945 - loss: 0.0778\nEpoch 34/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9955 - loss: 0.0625\nEpoch 35/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9963 - loss: 0.0551\nEpoch 36/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9969 - loss: 0.0481\nEpoch 37/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9970 - loss: 0.0390\nEpoch 38/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9977 - loss: 0.0354\nEpoch 39/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9981 - loss: 0.0287\nEpoch 40/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9972 - loss: 0.0279\nEpoch 41/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9977 - loss: 0.0258\nEpoch 42/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9980 - loss: 0.0231\nEpoch 43/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9977 - loss: 0.0219\nEpoch 44/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9982 - loss: 0.0199\nEpoch 45/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9983 - loss: 0.0168\nEpoch 46/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9979 - loss: 0.0170\nEpoch 47/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9985 - loss: 0.0145\nEpoch 48/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9977 - loss: 0.0156\nEpoch 49/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9982 - loss: 0.0140\nEpoch 50/50\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9988 - loss: 0.0125\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e24120bfcd0>"},"metadata":{}}]},{"cell_type":"code","source":"def predict(input_text):\n    # Preprocess the input text\n    input_seq = tokenizer.texts_to_sequences([input_text])\n    padded_input_seq = pad_sequences(input_seq, maxlen=max_seq_len, padding='post')\n\n    # Predict the output sequence\n    predictions = model.predict(padded_input_seq)\n    predicted_sequence = np.argmax(predictions, axis=-1)\n    \n    # Convert the predicted sequence back to text\n    decoded_sentence = ' '.join(tokenizer.index_word.get(index, '') for index in predicted_sequence[0])\n    \n    return decoded_sentence.strip()\n\n# Example prediction\ncorrected_text = predict('мотивация должна быыыть всегдаааа')\nprint('Corrected Text:', corrected_text)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:16:15.548880Z","iopub.execute_input":"2024-05-19T09:16:15.549550Z","iopub.status.idle":"2024-05-19T09:16:15.628516Z","shell.execute_reply.started":"2024-05-19T09:16:15.549515Z","shell.execute_reply":"2024-05-19T09:16:15.627494Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\nCorrected Text: \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FINE TUNING OF PRETRAINED CORRECTOR","metadata":{}},{"cell_type":"code","source":"!pip install datasets\n!apt install git-lfs\n!pip install transformers\n!pip install sentencepiece \n!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:34:18.277306Z","iopub.execute_input":"2024-05-19T16:34:18.278165Z","iopub.status.idle":"2024-05-19T16:35:17.054823Z","shell.execute_reply.started":"2024-05-19T16:34:18.278136Z","shell.execute_reply":"2024-05-19T16:35:17.053661Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (2.9.2-1).\n0 upgraded, 0 newly installed, 0 to remove and 65 not upgraded.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=0515edb524f206b0d599dc648676e7419cf901269d018c935b3b02118ca48c00\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Импорт библиотек\nimport numpy as np\nfrom datasets import Dataset\nimport tensorflow as tf\nimport nltk\nfrom transformers import T5TokenizerFast, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nimport torch\nfrom transformers.optimization import Adafactor, AdafactorSchedule\nfrom datasets import load_dataset, load_metric","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:34:12.083955Z","iopub.execute_input":"2024-05-19T16:34:12.084641Z","iopub.status.idle":"2024-05-19T16:34:12.090988Z","shell.execute_reply.started":"2024-05-19T16:34:12.084608Z","shell.execute_reply":"2024-05-19T16:34:12.089733Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[11], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    import tensorflow as\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2283669381.py, line 4)","output_type":"error"}]},{"cell_type":"code","source":"# загрузка параметров\nraw_datasets = load_dataset(\"xsum\")\nmetric = load_metric(\"rouge\")\nnltk.download('punkt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Определение параметров\n#REPO = \"t5-russian-spell\"  # Введите наазвание название репозитория\nMODEL_NAME = \"UrukHan/t5-russian-spell\" # Введите наазвание выбранной модели из хаба\nMAX_INPUT = 256  # Введите максимальную длинну входных данных  в токенах (длинна входных фраз в словах (можно считать полслова токен))\nMAX_OUTPUT  = 256 # Введите максимальную длинну прогнозов в токенах (можно уменьшить для задач суммризации или других задач где выход короче)\nBATCH_SIZE = 8 \ndf = pd.read_csv('/kaggle/input/dop-test-files/errors.csv').drop(columns=['Unnamed: 0'])\ndf.head(3) # Введите наазвание название датасета","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:29:40.770447Z","iopub.execute_input":"2024-05-19T16:29:40.770844Z","iopub.status.idle":"2024-05-19T16:29:40.801462Z","shell.execute_reply.started":"2024-05-19T16:29:40.770815Z","shell.execute_reply":"2024-05-19T16:29:40.800362Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                        label  \\\n0                           иностранный агент   \n1       свидетельствуют о проблемах с печенью   \n2  найдите способ быть полезными другим людям   \n\n                                        preds  \n0                            иностранный аген  \n1         свидетельствуют о праблемах спецнью  \n2  найдиче сьпособ быть полезном другим людем  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>иностранный агент</td>\n      <td>иностранный аген</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>свидетельствуют о проблемах с печенью</td>\n      <td>свидетельствуют о праблемах спецнью</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>найдите способ быть полезными другим людям</td>\n      <td>найдиче сьпособ быть полезном другим людем</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data = Dataset.from_pandas(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.max_length = MAX_OUTPUT","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = data['train']\ntest = data['test'].train_test_split(0.02)['test']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}