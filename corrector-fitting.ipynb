{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8456387,"sourceType":"datasetVersion","datasetId":3213578}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, GRU, Dense\nfrom transformers import AutoTokenizer\nfrom tensorflow.keras.models import Sequential\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:23:07.977124Z","iopub.execute_input":"2024-05-21T06:23:07.977457Z","iopub.status.idle":"2024-05-21T06:23:23.124609Z","shell.execute_reply.started":"2024-05-21T06:23:07.977428Z","shell.execute_reply":"2024-05-21T06:23:23.123846Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-21 06:23:09.598385: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-21 06:23:09.598470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-21 06:23:09.722791: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Пробую обучить нейросеть на Tensorflow","metadata":{}},{"cell_type":"code","source":"# Sample data preparation (assuming you have your data in a pandas DataFrame)\n# Replace this with your actual DataFrame\ndata = pd.DataFrame({\n    'input_text': ['i ike eting cakes', 'i go to shool everydy'],\n    'target_text': ['i like eating cakes', 'i go to school everyday']\n})\n\ndata = pd.read_csv('/kaggle/input/dop-test-files/errors.csv').drop(columns=['Unnamed: 0'])\ndata","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:07:19.103263Z","iopub.execute_input":"2024-05-19T09:07:19.104309Z","iopub.status.idle":"2024-05-19T09:07:19.130628Z","shell.execute_reply.started":"2024-05-19T09:07:19.104272Z","shell.execute_reply":"2024-05-19T09:07:19.129675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenization and Padding\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data['preds'].tolist() + data['label'].tolist())\n\n# Convert text to sequences\ninput_sequences = tokenizer.texts_to_sequences(data['preds'].tolist())\ntarget_sequences = tokenizer.texts_to_sequences(data['label'].tolist())\n\n# Padding sequences to the same length\nmax_seq_len = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in target_sequences))\npadded_input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='post')\npadded_target_sequences = pad_sequences(target_sequences, maxlen=max_seq_len, padding='post')\n\n# Define model parameters\nvocab_size = len(tokenizer.word_index) + 1\nembedding_dim = 1024\nrnn_units = 512","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:13:37.859781Z","iopub.execute_input":"2024-05-19T09:13:37.860609Z","iopub.status.idle":"2024-05-19T09:13:37.977520Z","shell.execute_reply.started":"2024-05-19T09:13:37.860575Z","shell.execute_reply":"2024-05-19T09:13:37.976634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_sequences[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:27:23.950120Z","iopub.execute_input":"2024-05-19T09:27:23.950976Z","iopub.status.idle":"2024-05-19T09:27:23.958082Z","shell.execute_reply.started":"2024-05-19T09:27:23.950941Z","shell.execute_reply":"2024-05-19T09:27:23.956956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_sequences[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:27:29.420894Z","iopub.execute_input":"2024-05-19T09:27:29.421868Z","iopub.status.idle":"2024-05-19T09:27:29.428941Z","shell.execute_reply.started":"2024-05-19T09:27:29.421831Z","shell.execute_reply":"2024-05-19T09:27:29.427877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_len))\nmodel.add(LSTM(rnn_units, return_sequences=True))\nmodel.add(Dense(vocab_size, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Prepare target data for training\npadded_target_sequences = np.expand_dims(padded_target_sequences, -1)\n\n# Train the model\nmodel.fit(padded_input_sequences, padded_target_sequences, epochs=50, batch_size=50)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:13:38.326546Z","iopub.execute_input":"2024-05-19T09:13:38.327084Z","iopub.status.idle":"2024-05-19T09:14:35.946004Z","shell.execute_reply.started":"2024-05-19T09:13:38.327050Z","shell.execute_reply":"2024-05-19T09:14:35.944995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(input_text):\n    # Preprocess the input text\n    input_seq = tokenizer.texts_to_sequences([input_text])\n    padded_input_seq = pad_sequences(input_seq, maxlen=max_seq_len, padding='post')\n\n    # Predict the output sequence\n    predictions = model.predict(padded_input_seq)\n    predicted_sequence = np.argmax(predictions, axis=-1)\n    \n    # Convert the predicted sequence back to text\n    decoded_sentence = ' '.join(tokenizer.index_word.get(index, '') for index in predicted_sequence[0])\n    \n    return decoded_sentence.strip()\n\n# Example prediction\ncorrected_text = predict('мотивация должна быыыть всегдаааа')\nprint('Corrected Text:', corrected_text)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:16:15.548880Z","iopub.execute_input":"2024-05-19T09:16:15.549550Z","iopub.status.idle":"2024-05-19T09:16:15.628516Z","shell.execute_reply.started":"2024-05-19T09:16:15.549515Z","shell.execute_reply":"2024-05-19T09:16:15.627494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Файн тюнинг предобученного корректора","metadata":{}},{"cell_type":"code","source":"!pip install datasets\n!apt install git-lfs\n!pip install transformers\n!pip install sentencepiece \n!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:23:23.126043Z","iopub.execute_input":"2024-05-21T06:23:23.126542Z","iopub.status.idle":"2024-05-21T06:24:17.611250Z","shell.execute_reply.started":"2024-05-21T06:23:23.126516Z","shell.execute_reply":"2024-05-21T06:24:17.610135Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (2.9.2-1).\n0 upgraded, 0 newly installed, 0 to remove and 65 not upgraded.\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=fa84351a195a47b8a6cd8916dd2b651da7c3677c7048ff5a5bbacc3ee4be1bf5\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Импорт библиотек\nimport numpy as np\nfrom datasets import Dataset\nimport tensorflow as tf\nimport nltk\nfrom transformers import T5TokenizerFast, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nimport torch\nfrom transformers.optimization import Adafactor, AdafactorSchedule\nfrom datasets import load_dataset, load_metric","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:38:29.691599Z","iopub.execute_input":"2024-05-21T06:38:29.691987Z","iopub.status.idle":"2024-05-21T06:38:29.697544Z","shell.execute_reply.started":"2024-05-21T06:38:29.691956Z","shell.execute_reply":"2024-05-21T06:38:29.696523Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# загрузка параметров\n#raw_datasets = load_dataset(\"xsum\")\nmetric = load_metric(\"rouge\")\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:38:32.726629Z","iopub.execute_input":"2024-05-21T06:38:32.727284Z","iopub.status.idle":"2024-05-21T06:38:33.393903Z","shell.execute_reply.started":"2024-05-21T06:38:32.727253Z","shell.execute_reply":"2024-05-21T06:38:33.392942Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2633626551.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric(\"rouge\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7eb5759157e4d1c98c6fc406debc6d1"}},"metadata":{}},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Определение параметров\nREPO = \"t5-russian-spell\"  # Введите наазвание название репозитория\nMODEL_NAME = \"UrukHan/t5-russian-spell\" # Введите наазвание выбранной модели из хаба\nMAX_INPUT = 256  # Введите максимальную длинну входных данных  в токенах (длинна входных фраз в словах (можно считать полслова токен))\nMAX_OUTPUT  = 256 # Введите максимальную длинну прогнозов в токенах (можно уменьшить для задач суммризации или других задач где выход короче)\nBATCH_SIZE = 20 \nDATASET = 'UrukHan/t5-russian-spell_I'   # Введите наазвание название датасета\n\n# Преобразование DataFrame в Dataset\ndata = Dataset.from_pandas(df)\n\n# Разделение DataFrame на обучающую и тестовую выборки\ntrain_df, test_df = train_test_split(df, test_size=0.2)\n\n# Преобразование обратно в Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:40:02.387440Z","iopub.execute_input":"2024-05-21T06:40:02.388066Z","iopub.status.idle":"2024-05-21T06:40:02.417958Z","shell.execute_reply.started":"2024-05-21T06:40:02.388033Z","shell.execute_reply":"2024-05-21T06:40:02.417074Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Загрузка модели и токенизатора\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\nmodel.config.max_length = MAX_OUTPUT  # по умолчанию 20, поэтому во всех моделях прогнозы обрезаются выходные последовательности\n# Закоментить после первого соъранения в репозиторий свой необъязательно\n\ntrain = train_dataset\ntest = test_dataset.train_test_split(0.02)  # Уменьшил так тестовыу. выборку чтоб не ждать долго расчет ошибок между эпохами\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model) #return_tensors=\"tf\"","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:40:04.035662Z","iopub.execute_input":"2024-05-21T06:40:04.036475Z","iopub.status.idle":"2024-05-21T06:40:04.986797Z","shell.execute_reply.started":"2024-05-21T06:40:04.036445Z","shell.execute_reply":"2024-05-21T06:40:04.986040Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Rouge expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    # Extract a few results\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:40:06.251150Z","iopub.execute_input":"2024-05-21T06:40:06.251776Z","iopub.status.idle":"2024-05-21T06:40:06.260094Z","shell.execute_reply.started":"2024-05-21T06:40:06.251743Z","shell.execute_reply":"2024-05-21T06:40:06.259109Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n  output_dir = '/kaggle/working/',\n  #overwrite_output_dir=True,\n  evaluation_strategy='steps',\n  #learning_rate=2e-5,\n  eval_steps=5000,\n  save_steps=5000,\n  num_train_epochs=10,\n  predict_with_generate=True,\n  per_device_train_batch_size=BATCH_SIZE,\n  per_device_eval_batch_size=BATCH_SIZE,\n  fp16=True,\n  save_total_limit=2,\n  #generation_max_length=256,\n  #generation_num_beams=4,\n  weight_decay=0.005,\n  #logging_dir='logs',\n  push_to_hub=False,\n  report_to=\"none\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:52:56.356197Z","iopub.execute_input":"2024-05-21T06:52:56.356870Z","iopub.status.idle":"2024-05-21T06:52:56.389047Z","shell.execute_reply.started":"2024-05-21T06:52:56.356839Z","shell.execute_reply":"2024-05-21T06:52:56.388311Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Выберем вручную оптимизатор. Т5 в оригинальной архитектуре использует Адафактор оптимизатор\noptimizer = Adafactor(\n    model.parameters(),\n    lr=1e-5,\n    eps=(1e-30, 1e-3),\n    clip_threshold=1.0,\n    decay_rate=-0.8,\n    beta1=None,\n    weight_decay=0.0,\n    relative_step=False,\n    scale_parameter=False,\n    warmup_init=False,\n)\nlr_scheduler = AdafactorSchedule(optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:52:57.071130Z","iopub.execute_input":"2024-05-21T06:52:57.071608Z","iopub.status.idle":"2024-05-21T06:52:57.086262Z","shell.execute_reply.started":"2024-05-21T06:52:57.071571Z","shell.execute_reply":"2024-05-21T06:52:57.085105Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"!pip install wandb\nimport os\n#os.environ['WANDB_DISABLED'] = 'True'\n#os.environ[\"WANDB_SILENT\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:49:57.143593Z","iopub.execute_input":"2024-05-21T06:49:57.144339Z","iopub.status.idle":"2024-05-21T06:50:09.226563Z","shell.execute_reply.started":"2024-05-21T06:49:57.144307Z","shell.execute_reply":"2024-05-21T06:50:09.225548Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n  model=model,\n  args=training_args,\n  train_dataset = train,\n  eval_dataset = test,\n  optimizers = (optimizer, lr_scheduler),\n  tokenizer = tokenizer,\n  compute_metrics=compute_metrics\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:53:00.751768Z","iopub.execute_input":"2024-05-21T06:53:00.752144Z","iopub.status.idle":"2024-05-21T06:53:02.008954Z","shell.execute_reply.started":"2024-05-21T06:53:00.752116Z","shell.execute_reply":"2024-05-21T06:53:02.007585Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m      2\u001b[0m   model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m   args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m   compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2085\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2086\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3288\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3286\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   3287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 3288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3291\u001b[0m     )\n\u001b[1;32m   3293\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   3295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n","\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']"],"ename":"ValueError","evalue":"You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Пробую обучить t5 трансформер на 60 миллионов параметров","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict, load_metric\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('/kaggle/input/dop-test-files/errors.csv').drop(columns=['Unnamed: 0'])\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:07:20.412723Z","iopub.execute_input":"2024-05-28T06:07:20.413597Z","iopub.status.idle":"2024-05-28T06:07:40.670665Z","shell.execute_reply.started":"2024-05-28T06:07:20.413555Z","shell.execute_reply":"2024-05-28T06:07:40.669592Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-28 06:07:31.752505: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-28 06:07:31.752641: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-28 06:07:31.903425: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                        label  \\\n0                           иностранный агент   \n1       свидетельствуют о проблемах с печенью   \n2  найдите способ быть полезными другим людям   \n3               я уже поставил белье в стирку   \n4                                 круглый мяч   \n\n                                        preds  \n0                            иностранный аген  \n1         свидетельствуют о праблемах спецнью  \n2  найдиче сьпособ быть полезном другим людем  \n3                 я уже поставил бельо встиру  \n4                               круглый мядчь  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>иностранный агент</td>\n      <td>иностранный аген</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>свидетельствуют о проблемах с печенью</td>\n      <td>свидетельствуют о праблемах спецнью</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>найдите способ быть полезными другим людям</td>\n      <td>найдиче сьпособ быть полезном другим людем</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>я уже поставил белье в стирку</td>\n      <td>я уже поставил бельо встиру</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>круглый мяч</td>\n      <td>круглый мядчь</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['preds'], df['label'], test_size=0.3, random_state=42)\n\ndf_train = pd.DataFrame({'preds': X_train, 'label': y_train})\ndf_val = pd.DataFrame({'preds': X_test, 'label': y_test})","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:07:40.672831Z","iopub.execute_input":"2024-05-28T06:07:40.673449Z","iopub.status.idle":"2024-05-28T06:07:40.685138Z","shell.execute_reply.started":"2024-05-28T06:07:40.673409Z","shell.execute_reply":"2024-05-28T06:07:40.684005Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Convert the DataFrame to a Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(df_train)\nval_dataset = Dataset.from_pandas(df_val)\n\n# Combine them into a DatasetDict\ndataset_dict = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset\n})\n\n# Initialize the tokenizer\ntokenizer = T5Tokenizer.from_pretrained('t5-small')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:07:40.686560Z","iopub.execute_input":"2024-05-28T06:07:40.687665Z","iopub.status.idle":"2024-05-28T06:07:41.626493Z","shell.execute_reply.started":"2024-05-28T06:07:40.687633Z","shell.execute_reply":"2024-05-28T06:07:41.625689Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a53359a8be854c8c928a57514272cdb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dfebc4ccaaa49c5ab6dfd04140a0ef7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90aed8fd5f904745afb4d9eca6bd6f56"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preprocessing function to tokenize the data\ndef preprocess_function(examples):\n    inputs = ['label: ' + inp for inp in examples['preds']]\n    targets = [tgt for tgt in examples['label']]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n    labels = tokenizer(targets, max_length=512, truncation=True, padding='max_length').input_ids\n    model_inputs['label'] = labels\n    return model_inputs\n\n# Apply the preprocessing function to the dataset\ntokenized_dataset = dataset_dict.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:07:41.628345Z","iopub.execute_input":"2024-05-28T06:07:41.628621Z","iopub.status.idle":"2024-05-28T06:07:42.676178Z","shell.execute_reply.started":"2024-05-28T06:07:41.628597Z","shell.execute_reply":"2024-05-28T06:07:42.675296Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1036 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7105dfe9467944f497e6393d83498c40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/444 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5bdae72cbe3430cbf7e4f9437878305"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install jiwer","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:07:42.677201Z","iopub.execute_input":"2024-05-28T06:07:42.677467Z","iopub.status.idle":"2024-05-28T06:07:58.694075Z","shell.execute_reply.started":"2024-05-28T06:07:42.677445Z","shell.execute_reply":"2024-05-28T06:07:58.692986Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.4 rapidfuzz-3.9.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load CER\ncer_metric = load_metric('cer')\n\n# Define compute_metrics function\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Replace label IDs (set to -100) with the padding token ID\n    decoded_labels = [[label if label != -100 else tokenizer.pad_token_id for label in labels] for labels in decoded_labels]\n\n    decoded_labels = tokenizer.batch_decode(decoded_labels, skip_special_tokens=True)\n\n    cer = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n\n    return {\"cer\": cer}","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:07:58.695435Z","iopub.execute_input":"2024-05-28T06:07:58.695746Z","iopub.status.idle":"2024-05-28T06:07:59.119463Z","shell.execute_reply.started":"2024-05-28T06:07:58.695714Z","shell.execute_reply":"2024-05-28T06:07:59.118595Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3828030161.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  cer_metric = load_metric('cer')\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for cer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/cer/cer.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c98e2492dfec482a95a2717f86d7ab1f"}},"metadata":{}}]},{"cell_type":"code","source":"# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    learning_rate=5e-5,\n    per_device_train_batch_size=10,\n    per_device_eval_batch_size=10,\n    num_train_epochs=30,\n    weight_decay=0.01\n)\n\n# Initialize the model\nmodel = T5ForConditionalGeneration.from_pretrained('t5-small')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:07:59.120584Z","iopub.execute_input":"2024-05-28T06:07:59.120863Z","iopub.status.idle":"2024-05-28T06:08:01.317991Z","shell.execute_reply.started":"2024-05-28T06:07:59.120839Z","shell.execute_reply":"2024-05-28T06:08:01.317087Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dce28c408d74c1a9a01201a46e93af8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58702633a8694ee18e652584a0db6497"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87c4be3973564e229499120b58b9448c"}},"metadata":{}}]},{"cell_type":"code","source":"import os\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"","metadata":{"execution":{"iopub.status.busy":"2024-05-28T05:57:16.131243Z","iopub.execute_input":"2024-05-28T05:57:16.131541Z","iopub.status.idle":"2024-05-28T05:57:16.135955Z","shell.execute_reply.started":"2024-05-28T05:57:16.131516Z","shell.execute_reply":"2024-05-28T05:57:16.135070Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['validation'],\n    tokenizer=tokenizer,\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:08:01.319396Z","iopub.execute_input":"2024-05-28T06:08:01.320089Z","iopub.status.idle":"2024-05-28T06:21:23.712304Z","shell.execute_reply.started":"2024-05-28T06:08:01.320054Z","shell.execute_reply":"2024-05-28T06:21:23.711450Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240528_061044-jz4mxk4t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/human_voice/huggingface/runs/jz4mxk4t' target=\"_blank\">serene-forest-6</a></strong> to <a href='https://wandb.ai/human_voice/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/human_voice/huggingface' target=\"_blank\">https://wandb.ai/human_voice/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/human_voice/huggingface/runs/jz4mxk4t' target=\"_blank\">https://wandb.ai/human_voice/huggingface/runs/jz4mxk4t</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='520' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [520/520 10:18, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.103593</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.081387</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.066618</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.055824</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.048332</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.045269</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.043742</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.042991</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.042584</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.382700</td>\n      <td>0.042496</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=520, training_loss=0.3698324891237112, metrics={'train_runtime': 800.7375, 'train_samples_per_second': 12.938, 'train_steps_per_second': 0.649, 'total_flos': 1402141063249920.0, 'train_loss': 0.3698324891237112, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Fine-tune the model\n#trainer.train()\n\n# Save the model and tokenizer\nmodel.save_pretrained('t5_small_fine_tuned_10_epochs_model')\ntokenizer.save_pretrained('t5_small_fine_tuned_10_epochs_tokenizer')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:23:12.789184Z","iopub.execute_input":"2024-05-28T06:23:12.789544Z","iopub.status.idle":"2024-05-28T06:23:13.252568Z","shell.execute_reply.started":"2024-05-28T06:23:12.789515Z","shell.execute_reply":"2024-05-28T06:23:13.251395Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('t5_small_fine_tuned_10_epochs_tokenizer/tokenizer_config.json',\n 't5_small_fine_tuned_10_epochs_tokenizer/special_tokens_map.json',\n 't5_small_fine_tuned_10_epochs_tokenizer/spiece.model',\n 't5_small_fine_tuned_10_epochs_tokenizer/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load the model and tokenizer\nmodel = T5ForConditionalGeneration.from_pretrained('./saved_model')\ntokenizer = T5Tokenizer.from_pretrained('./saved_model')\n\n# Function to correct mistakes\ndef correct_mistakes(text, model, tokenizer):\n    # Prepare the input text\n    input_text = \"correct: \" + text\n    inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n    \n    # Generate the corrected text\n    outputs = model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return corrected_text\n\n# Example usage\ntext = \"Привет, как дила?\"\ncorrected_text = correct_mistakes(text, model, tokenizer)\nprint(corrected_text)  # Output should be \"Привет, как дела?\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}