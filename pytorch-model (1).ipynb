{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np \nimport matplotlib","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:04:01.860672Z","iopub.execute_input":"2023-07-11T14:04:01.861881Z","iopub.status.idle":"2023-07-11T14:04:04.747138Z","shell.execute_reply.started":"2023-07-11T14:04:01.861834Z","shell.execute_reply":"2023-07-11T14:04:04.745726Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        int_sequence = []\n        for c in text:\n            ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MFCC(n_mfcc=20)\n)\n\n\nvalid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//3)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=34, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:04:04.749782Z","iopub.execute_input":"2023-07-11T14:04:04.750519Z","iopub.status.idle":"2023-07-11T14:04:04.914007Z","shell.execute_reply.started":"2023-07-11T14:04:04.750482Z","shell.execute_reply":"2023-07-11T14:04:04.911019Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"class BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:04:04.915772Z","iopub.execute_input":"2023-07-11T14:04:04.916490Z","iopub.status.idle":"2023-07-11T14:04:04.927458Z","shell.execute_reply.started":"2023-07-11T14:04:04.916440Z","shell.execute_reply":"2023-07-11T14:04:04.924329Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/rus-speech/Speeches.xlsx')\ny = [sentence for sentence in file['Русская речь']]\n\ndir_name = \"/kaggle/input/upd-speech/mono_voice/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in range(1, 2001):\n    file_name = f'{e}.wav'\n    sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    X.append(torch.Tensor(sampl))\n    if i % 100 == 0:\n        print(i)\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:04:04.931433Z","iopub.execute_input":"2023-07-11T14:04:04.931881Z","iopub.status.idle":"2023-07-11T14:04:31.895849Z","shell.execute_reply.started":"2023-07-11T14:04:04.931851Z","shell.execute_reply":"2023-07-11T14:04:31.894053Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n","output_type":"stream"}]},{"cell_type":"code","source":"X[0].shape","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:04:31.898665Z","iopub.execute_input":"2023-07-11T14:04:31.899791Z","iopub.status.idle":"2023-07-11T14:04:31.909209Z","shell.execute_reply.started":"2023-07-11T14:04:31.899741Z","shell.execute_reply":"2023-07-11T14:04:31.907799Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 79872])"},"metadata":{}}]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('4', 'четыре').replace('Р-220', 'р двести двадцать').replace('6', 'шесть').replace(\"-\", \" \")\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    sentence = \" \".join(sentence.split())\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:04:31.911172Z","iopub.execute_input":"2023-07-11T14:04:31.911610Z","iopub.status.idle":"2023-07-11T14:04:31.942897Z","shell.execute_reply.started":"2023-07-11T14:04:31.911568Z","shell.execute_reply":"2023-07-11T14:04:31.941771Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nX_train = X[:1800]\nX_test = X[1800:]\ny_train = y[:1800]\ny_test = y[1800:]","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:04:31.945374Z","iopub.execute_input":"2023-07-11T14:04:31.946145Z","iopub.status.idle":"2023-07-11T14:04:32.061682Z","shell.execute_reply.started":"2023-07-11T14:04:31.946103Z","shell.execute_reply":"2023-07-11T14:04:32.060504Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:04:32.063351Z","iopub.execute_input":"2023-07-11T14:04:32.063773Z","iopub.status.idle":"2023-07-11T14:04:32.071841Z","shell.execute_reply.started":"2023-07-11T14:04:32.063724Z","shell.execute_reply":"2023-07-11T14:04:32.070606Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 32, kernel_size=(4,4), stride=(3,3), padding=(2,2)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(64),\n            nn.GELU(),\n            nn.Conv2d(64, 32, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n        )\n        \n        self.fc_1 = nn.Sequential(\n            nn.Linear(224, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            nn.LayerNorm(270),\n            nn.GELU(),\n            nn.Linear(270, 270),\n            #nn.LayerNorm(256),\n            #nn.GELU(),\n        )\n        \n        self.BiGRU_1 = BidirectionalGRU(270, 270, 0, True)\n        self.BiGRU_2 = BidirectionalGRU(540, 270, 0, True)\n        self.BiGRU_3 = BidirectionalGRU(540, 270, 0, True)\n        #self.BiGRU_4 = BidirectionalGRU(800, 400, 0.5, True)\n        \n        self.fc_2 = nn.Sequential(\n            nn.Linear(540, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.fc_1(x)\n        x = self.BiGRU_1(x)\n        x = self.BiGRU_2(x)\n        x = self.BiGRU_3(x)\n        #x = self.BiGRU_4(x)\n        x = self.fc_2(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-11T15:47:38.471851Z","iopub.execute_input":"2023-07-11T15:47:38.472678Z","iopub.status.idle":"2023-07-11T15:47:38.491054Z","shell.execute_reply.started":"2023-07-11T15:47:38.472634Z","shell.execute_reply":"2023-07-11T15:47:38.489372Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class IterMeter(object):\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    train_loss = 0\n    train_cer, train_wer = [], []\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms) \n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        train_loss += loss.item() / len(train_loader)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 20 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n        #decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n        \"\"\"for j in range(len(decoded_preds)):\n            train_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n            train_wer.append(wer(decoded_targets[j], decoded_preds[j]))\"\"\"\n    \n    #avg_cer = sum(train_cer)/len(train_cer)\n    #avg_wer = sum(train_wer)/len(train_wer)\n    \n    \n            \n    #print('Train set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          #.format(train_loss, avg_cer, avg_wer, median_cer, median_wer))\n            \n    \n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n            output = model(spectrograms)\n            output = output.transpose(0, 1)\n\n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n\n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n\n\n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    median_cer = np.median(np.array(test_cer))\n    median_wer = np.median(np.array(test_wer))\n           \n    print('Test set:\\tAverage loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'\n          .format(test_loss, avg_cer, avg_wer, median_cer, median_wer))\n    \n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(35).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=34).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model.pt')","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:50:29.399929Z","iopub.execute_input":"2023-07-11T14:50:29.400375Z","iopub.status.idle":"2023-07-11T14:50:29.433687Z","shell.execute_reply.started":"2023-07-11T14:50:29.400305Z","shell.execute_reply":"2023-07-11T14:50:29.432209Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def predict(model, file_name, device):\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MFCC(n_mfcc=20)\n    \n    sampl = librosa.load(file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    print(spectrogram_tensor.size())\n\n    with torch.no_grad():\n        spectrogram_tensor.to(device)\n        output = model(spectrogram_tensor)\n        print(output.size())\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 34:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:50:31.031603Z","iopub.execute_input":"2023-07-11T14:50:31.032760Z","iopub.status.idle":"2023-07-11T14:50:31.049020Z","shell.execute_reply.started":"2023-07-11T14:50:31.032716Z","shell.execute_reply":"2023-07-11T14:50:31.047577Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"%%time \nlearning_rate = 0.0005\nbatch_size = 5\nepochs = 35\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-11T15:58:59.383812Z","iopub.execute_input":"2023-07-11T15:58:59.384929Z","iopub.status.idle":"2023-07-11T16:11:26.458531Z","shell.execute_reply.started":"2023-07-11T15:58:59.384865Z","shell.execute_reply":"2023-07-11T16:11:26.455940Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel1(\n  (conv): Sequential(\n    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Conv2d(1, 32, kernel_size=(4, 4), stride=(3, 3), padding=(2, 2))\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): GELU(approximate='none')\n    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): GELU(approximate='none')\n    (7): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): GELU(approximate='none')\n  )\n  (fc_1): Sequential(\n    (0): Linear(in_features=224, out_features=270, bias=True)\n    (1): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=270, out_features=270, bias=True)\n    (4): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (5): GELU(approximate='none')\n    (6): Linear(in_features=270, out_features=270, bias=True)\n  )\n  (BiGRU_1): BidirectionalGRU(\n    (BiGRU): GRU(270, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((270,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_2): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (BiGRU_3): BidirectionalGRU(\n    (BiGRU): GRU(540, 270, batch_first=True, bidirectional=True)\n    (layer_norm): LayerNorm((540,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (fc_2): Sequential(\n    (0): Linear(in_features=540, out_features=35, bias=True)\n  )\n  (softmax): LogSoftmax(dim=2)\n)\nNum Model Parameters 3776487\nTrain Epoch: 1 [0/1800 (0%)]\tLoss: 18.524796\nTrain Epoch: 1 [100/1800 (6%)]\tLoss: 5.950472\nTrain Epoch: 1 [200/1800 (11%)]\tLoss: 3.642438\nTrain Epoch: 1 [300/1800 (17%)]\tLoss: 3.699538\nTrain Epoch: 1 [400/1800 (22%)]\tLoss: 3.584062\nTrain Epoch: 1 [500/1800 (28%)]\tLoss: 3.415567\nTrain Epoch: 1 [600/1800 (33%)]\tLoss: 3.336859\nTrain Epoch: 1 [700/1800 (39%)]\tLoss: 3.391840\nTrain Epoch: 1 [800/1800 (44%)]\tLoss: 3.297283\nTrain Epoch: 1 [900/1800 (50%)]\tLoss: 3.324754\nTrain Epoch: 1 [1000/1800 (56%)]\tLoss: 3.291422\nTrain Epoch: 1 [1100/1800 (61%)]\tLoss: 3.374645\nTrain Epoch: 1 [1200/1800 (67%)]\tLoss: 3.257452\nTrain Epoch: 1 [1300/1800 (72%)]\tLoss: 3.391164\nTrain Epoch: 1 [1400/1800 (78%)]\tLoss: 3.385227\nTrain Epoch: 1 [1500/1800 (83%)]\tLoss: 3.249723\nTrain Epoch: 1 [1600/1800 (89%)]\tLoss: 3.275799\nTrain Epoch: 1 [1700/1800 (94%)]\tLoss: 3.270639\n\nevaluating...\nTest set:\tAverage loss: 3.1880, Average CER: 1.000000 Average WER: 1.0000\n\nTrain Epoch: 2 [0/1800 (0%)]\tLoss: 3.174751\nTrain Epoch: 2 [100/1800 (6%)]\tLoss: 3.123981\nTrain Epoch: 2 [200/1800 (11%)]\tLoss: 3.177379\nTrain Epoch: 2 [300/1800 (17%)]\tLoss: 3.106393\nTrain Epoch: 2 [400/1800 (22%)]\tLoss: 3.252828\nTrain Epoch: 2 [500/1800 (28%)]\tLoss: 3.153254\nTrain Epoch: 2 [600/1800 (33%)]\tLoss: 3.100945\nTrain Epoch: 2 [700/1800 (39%)]\tLoss: 3.013128\nTrain Epoch: 2 [800/1800 (44%)]\tLoss: 2.945084\nTrain Epoch: 2 [900/1800 (50%)]\tLoss: 2.954860\nTrain Epoch: 2 [1000/1800 (56%)]\tLoss: 3.008023\nTrain Epoch: 2 [1100/1800 (61%)]\tLoss: 2.905775\nTrain Epoch: 2 [1200/1800 (67%)]\tLoss: 2.872267\nTrain Epoch: 2 [1300/1800 (72%)]\tLoss: 2.758431\nTrain Epoch: 2 [1400/1800 (78%)]\tLoss: 2.648080\nTrain Epoch: 2 [1500/1800 (83%)]\tLoss: 2.691409\nTrain Epoch: 2 [1600/1800 (89%)]\tLoss: 2.546843\nTrain Epoch: 2 [1700/1800 (94%)]\tLoss: 2.357546\n\nevaluating...\nTest set:\tAverage loss: 2.3760, Average CER: 0.960377 Average WER: 0.9983\n\nTrain Epoch: 3 [0/1800 (0%)]\tLoss: 2.291608\nTrain Epoch: 3 [100/1800 (6%)]\tLoss: 2.280615\nTrain Epoch: 3 [200/1800 (11%)]\tLoss: 2.243214\nTrain Epoch: 3 [300/1800 (17%)]\tLoss: 2.100915\nTrain Epoch: 3 [400/1800 (22%)]\tLoss: 2.762552\nTrain Epoch: 3 [500/1800 (28%)]\tLoss: 1.944787\nTrain Epoch: 3 [600/1800 (33%)]\tLoss: 1.760543\nTrain Epoch: 3 [700/1800 (39%)]\tLoss: 1.865112\nTrain Epoch: 3 [800/1800 (44%)]\tLoss: 2.091958\nTrain Epoch: 3 [900/1800 (50%)]\tLoss: 1.538563\nTrain Epoch: 3 [1000/1800 (56%)]\tLoss: 1.348591\nTrain Epoch: 3 [1100/1800 (61%)]\tLoss: 1.245364\nTrain Epoch: 3 [1200/1800 (67%)]\tLoss: 1.660415\nTrain Epoch: 3 [1300/1800 (72%)]\tLoss: 1.247029\nTrain Epoch: 3 [1400/1800 (78%)]\tLoss: 1.373573\nTrain Epoch: 3 [1500/1800 (83%)]\tLoss: 1.323415\nTrain Epoch: 3 [1600/1800 (89%)]\tLoss: 1.310029\nTrain Epoch: 3 [1700/1800 (94%)]\tLoss: 1.715668\n\nevaluating...\nTest set:\tAverage loss: 1.3670, Average CER: 0.443113 Average WER: 0.9891\n\nTrain Epoch: 4 [0/1800 (0%)]\tLoss: 1.221851\nTrain Epoch: 4 [100/1800 (6%)]\tLoss: 1.352854\nTrain Epoch: 4 [200/1800 (11%)]\tLoss: 1.119349\nTrain Epoch: 4 [300/1800 (17%)]\tLoss: 1.485658\nTrain Epoch: 4 [400/1800 (22%)]\tLoss: 1.266224\nTrain Epoch: 4 [500/1800 (28%)]\tLoss: 0.947312\nTrain Epoch: 4 [600/1800 (33%)]\tLoss: 1.157955\nTrain Epoch: 4 [700/1800 (39%)]\tLoss: 0.868433\nTrain Epoch: 4 [800/1800 (44%)]\tLoss: 1.205497\nTrain Epoch: 4 [900/1800 (50%)]\tLoss: 1.160590\nTrain Epoch: 4 [1000/1800 (56%)]\tLoss: 1.039777\nTrain Epoch: 4 [1100/1800 (61%)]\tLoss: 1.155653\nTrain Epoch: 4 [1200/1800 (67%)]\tLoss: 0.999479\nTrain Epoch: 4 [1300/1800 (72%)]\tLoss: 0.847130\nTrain Epoch: 4 [1400/1800 (78%)]\tLoss: 1.096875\nTrain Epoch: 4 [1500/1800 (83%)]\tLoss: 0.986101\nTrain Epoch: 4 [1600/1800 (89%)]\tLoss: 1.296657\nTrain Epoch: 4 [1700/1800 (94%)]\tLoss: 0.772125\n\nevaluating...\nTest set:\tAverage loss: 1.0670, Average CER: 0.336902 Average WER: 0.9475\n\nTrain Epoch: 5 [0/1800 (0%)]\tLoss: 0.769220\nTrain Epoch: 5 [100/1800 (6%)]\tLoss: 0.807443\nTrain Epoch: 5 [200/1800 (11%)]\tLoss: 0.782429\nTrain Epoch: 5 [300/1800 (17%)]\tLoss: 0.886955\nTrain Epoch: 5 [400/1800 (22%)]\tLoss: 0.591430\nTrain Epoch: 5 [500/1800 (28%)]\tLoss: 1.083159\nTrain Epoch: 5 [600/1800 (33%)]\tLoss: 0.960413\nTrain Epoch: 5 [700/1800 (39%)]\tLoss: 0.583685\nTrain Epoch: 5 [800/1800 (44%)]\tLoss: 0.957215\nTrain Epoch: 5 [900/1800 (50%)]\tLoss: 0.665664\nTrain Epoch: 5 [1000/1800 (56%)]\tLoss: 1.942211\nTrain Epoch: 5 [1100/1800 (61%)]\tLoss: 0.785847\nTrain Epoch: 5 [1200/1800 (67%)]\tLoss: 0.700158\nTrain Epoch: 5 [1300/1800 (72%)]\tLoss: 0.703385\nTrain Epoch: 5 [1400/1800 (78%)]\tLoss: 0.754943\nTrain Epoch: 5 [1500/1800 (83%)]\tLoss: 0.729944\nTrain Epoch: 5 [1600/1800 (89%)]\tLoss: 0.692893\nTrain Epoch: 5 [1700/1800 (94%)]\tLoss: 0.442506\n\nevaluating...\nTest set:\tAverage loss: 0.9261, Average CER: 0.303235 Average WER: 0.9111\n\nTrain Epoch: 6 [0/1800 (0%)]\tLoss: 0.622426\nTrain Epoch: 6 [100/1800 (6%)]\tLoss: 0.534697\nTrain Epoch: 6 [200/1800 (11%)]\tLoss: 0.710639\nTrain Epoch: 6 [300/1800 (17%)]\tLoss: 0.783418\nTrain Epoch: 6 [400/1800 (22%)]\tLoss: 0.504277\nTrain Epoch: 6 [500/1800 (28%)]\tLoss: 0.666534\nTrain Epoch: 6 [600/1800 (33%)]\tLoss: 0.641167\nTrain Epoch: 6 [700/1800 (39%)]\tLoss: 0.547102\nTrain Epoch: 6 [800/1800 (44%)]\tLoss: 0.625668\nTrain Epoch: 6 [900/1800 (50%)]\tLoss: 0.356574\nTrain Epoch: 6 [1000/1800 (56%)]\tLoss: 0.420441\nTrain Epoch: 6 [1100/1800 (61%)]\tLoss: 0.643891\nTrain Epoch: 6 [1200/1800 (67%)]\tLoss: 0.525286\nTrain Epoch: 6 [1300/1800 (72%)]\tLoss: 1.686456\nTrain Epoch: 6 [1400/1800 (78%)]\tLoss: 0.798111\nTrain Epoch: 6 [1500/1800 (83%)]\tLoss: 0.728850\nTrain Epoch: 6 [1600/1800 (89%)]\tLoss: 0.619479\nTrain Epoch: 6 [1700/1800 (94%)]\tLoss: 0.481107\n\nevaluating...\nTest set:\tAverage loss: 0.8117, Average CER: 0.279324 Average WER: 0.9062\n\nTrain Epoch: 7 [0/1800 (0%)]\tLoss: 0.497050\nTrain Epoch: 7 [100/1800 (6%)]\tLoss: 0.328828\nTrain Epoch: 7 [200/1800 (11%)]\tLoss: 0.536348\nTrain Epoch: 7 [300/1800 (17%)]\tLoss: 0.489695\nTrain Epoch: 7 [400/1800 (22%)]\tLoss: 0.639165\nTrain Epoch: 7 [500/1800 (28%)]\tLoss: 0.499152\nTrain Epoch: 7 [600/1800 (33%)]\tLoss: 0.742298\nTrain Epoch: 7 [700/1800 (39%)]\tLoss: 0.510710\nTrain Epoch: 7 [800/1800 (44%)]\tLoss: 0.636072\nTrain Epoch: 7 [900/1800 (50%)]\tLoss: 0.532882\nTrain Epoch: 7 [1000/1800 (56%)]\tLoss: 0.626340\nTrain Epoch: 7 [1100/1800 (61%)]\tLoss: 0.476922\nTrain Epoch: 7 [1200/1800 (67%)]\tLoss: 0.653769\nTrain Epoch: 7 [1300/1800 (72%)]\tLoss: 0.521683\nTrain Epoch: 7 [1400/1800 (78%)]\tLoss: 0.622521\nTrain Epoch: 7 [1500/1800 (83%)]\tLoss: 0.772190\nTrain Epoch: 7 [1600/1800 (89%)]\tLoss: 0.526680\nTrain Epoch: 7 [1700/1800 (94%)]\tLoss: 0.631011\n\nevaluating...\nTest set:\tAverage loss: 0.8129, Average CER: 0.264991 Average WER: 0.8793\n\nTrain Epoch: 8 [0/1800 (0%)]\tLoss: 0.537874\nTrain Epoch: 8 [100/1800 (6%)]\tLoss: 0.541164\nTrain Epoch: 8 [200/1800 (11%)]\tLoss: 0.625504\nTrain Epoch: 8 [300/1800 (17%)]\tLoss: 0.605140\nTrain Epoch: 8 [400/1800 (22%)]\tLoss: 0.505965\nTrain Epoch: 8 [500/1800 (28%)]\tLoss: 0.438715\nTrain Epoch: 8 [600/1800 (33%)]\tLoss: 0.796630\nTrain Epoch: 8 [700/1800 (39%)]\tLoss: 0.489465\nTrain Epoch: 8 [800/1800 (44%)]\tLoss: 0.436485\nTrain Epoch: 8 [900/1800 (50%)]\tLoss: 0.517108\nTrain Epoch: 8 [1000/1800 (56%)]\tLoss: 0.335728\nTrain Epoch: 8 [1100/1800 (61%)]\tLoss: 0.585517\nTrain Epoch: 8 [1200/1800 (67%)]\tLoss: 0.394670\nTrain Epoch: 8 [1300/1800 (72%)]\tLoss: 0.659369\nTrain Epoch: 8 [1400/1800 (78%)]\tLoss: 0.524835\nTrain Epoch: 8 [1500/1800 (83%)]\tLoss: 0.644913\nTrain Epoch: 8 [1600/1800 (89%)]\tLoss: 0.418801\nTrain Epoch: 8 [1700/1800 (94%)]\tLoss: 0.624176\n\nevaluating...\nTest set:\tAverage loss: 0.8430, Average CER: 0.257226 Average WER: 0.8618\n\nTrain Epoch: 9 [0/1800 (0%)]\tLoss: 0.450598\nTrain Epoch: 9 [100/1800 (6%)]\tLoss: 0.530464\nTrain Epoch: 9 [200/1800 (11%)]\tLoss: 0.357835\nTrain Epoch: 9 [300/1800 (17%)]\tLoss: 0.339805\nTrain Epoch: 9 [400/1800 (22%)]\tLoss: 0.349599\nTrain Epoch: 9 [500/1800 (28%)]\tLoss: 0.392729\nTrain Epoch: 9 [600/1800 (33%)]\tLoss: 1.959620\nTrain Epoch: 9 [700/1800 (39%)]\tLoss: 0.467381\nTrain Epoch: 9 [800/1800 (44%)]\tLoss: 0.313689\nTrain Epoch: 9 [900/1800 (50%)]\tLoss: 0.597576\nTrain Epoch: 9 [1000/1800 (56%)]\tLoss: 0.472410\nTrain Epoch: 9 [1100/1800 (61%)]\tLoss: 0.692489\nTrain Epoch: 9 [1200/1800 (67%)]\tLoss: 0.507501\nTrain Epoch: 9 [1300/1800 (72%)]\tLoss: 0.449761\nTrain Epoch: 9 [1400/1800 (78%)]\tLoss: 0.301831\nTrain Epoch: 9 [1500/1800 (83%)]\tLoss: 0.363854\nTrain Epoch: 9 [1600/1800 (89%)]\tLoss: 0.663475\nTrain Epoch: 9 [1700/1800 (94%)]\tLoss: 0.777663\n\nevaluating...\nTest set:\tAverage loss: 0.7039, Average CER: 0.225309 Average WER: 0.8071\n\nTrain Epoch: 10 [0/1800 (0%)]\tLoss: 0.254223\nTrain Epoch: 10 [100/1800 (6%)]\tLoss: 0.451235\nTrain Epoch: 10 [200/1800 (11%)]\tLoss: 0.388333\nTrain Epoch: 10 [300/1800 (17%)]\tLoss: 0.375255\nTrain Epoch: 10 [400/1800 (22%)]\tLoss: 0.447916\nTrain Epoch: 10 [500/1800 (28%)]\tLoss: 0.461543\nTrain Epoch: 10 [600/1800 (33%)]\tLoss: 0.810328\nTrain Epoch: 10 [700/1800 (39%)]\tLoss: 0.306635\nTrain Epoch: 10 [800/1800 (44%)]\tLoss: 0.383721\nTrain Epoch: 10 [900/1800 (50%)]\tLoss: 0.240231\nTrain Epoch: 10 [1000/1800 (56%)]\tLoss: 0.235450\nTrain Epoch: 10 [1100/1800 (61%)]\tLoss: 0.763234\nTrain Epoch: 10 [1200/1800 (67%)]\tLoss: 0.458499\nTrain Epoch: 10 [1300/1800 (72%)]\tLoss: 0.560668\nTrain Epoch: 10 [1400/1800 (78%)]\tLoss: 0.407441\nTrain Epoch: 10 [1500/1800 (83%)]\tLoss: 0.301886\nTrain Epoch: 10 [1600/1800 (89%)]\tLoss: 0.414307\nTrain Epoch: 10 [1700/1800 (94%)]\tLoss: 0.213711\n\nevaluating...\nTest set:\tAverage loss: 0.7576, Average CER: 0.236693 Average WER: 0.8051\n\nTrain Epoch: 11 [0/1800 (0%)]\tLoss: 0.355739\nTrain Epoch: 11 [100/1800 (6%)]\tLoss: 0.225646\nTrain Epoch: 11 [200/1800 (11%)]\tLoss: 0.221835\nTrain Epoch: 11 [300/1800 (17%)]\tLoss: 0.429923\nTrain Epoch: 11 [400/1800 (22%)]\tLoss: 0.365797\nTrain Epoch: 11 [500/1800 (28%)]\tLoss: 0.282106\nTrain Epoch: 11 [600/1800 (33%)]\tLoss: 0.401405\nTrain Epoch: 11 [700/1800 (39%)]\tLoss: 0.455309\nTrain Epoch: 11 [800/1800 (44%)]\tLoss: 0.393216\nTrain Epoch: 11 [900/1800 (50%)]\tLoss: 0.833756\nTrain Epoch: 11 [1000/1800 (56%)]\tLoss: 0.259356\nTrain Epoch: 11 [1100/1800 (61%)]\tLoss: 0.334288\nTrain Epoch: 11 [1200/1800 (67%)]\tLoss: 0.291487\nTrain Epoch: 11 [1300/1800 (72%)]\tLoss: 0.364329\nTrain Epoch: 11 [1400/1800 (78%)]\tLoss: 0.361432\nTrain Epoch: 11 [1500/1800 (83%)]\tLoss: 0.485882\nTrain Epoch: 11 [1600/1800 (89%)]\tLoss: 0.460871\nTrain Epoch: 11 [1700/1800 (94%)]\tLoss: 0.554902\n\nevaluating...\nTest set:\tAverage loss: 0.8376, Average CER: 0.256294 Average WER: 0.8477\n\nTrain Epoch: 12 [0/1800 (0%)]\tLoss: 0.354871\nTrain Epoch: 12 [100/1800 (6%)]\tLoss: 0.468165\nTrain Epoch: 12 [200/1800 (11%)]\tLoss: 0.284837\nTrain Epoch: 12 [300/1800 (17%)]\tLoss: 0.261327\nTrain Epoch: 12 [400/1800 (22%)]\tLoss: 0.359381\nTrain Epoch: 12 [500/1800 (28%)]\tLoss: 1.398117\nTrain Epoch: 12 [600/1800 (33%)]\tLoss: 0.388823\nTrain Epoch: 12 [700/1800 (39%)]\tLoss: 0.197217\nTrain Epoch: 12 [800/1800 (44%)]\tLoss: 0.544405\nTrain Epoch: 12 [900/1800 (50%)]\tLoss: 0.410899\nTrain Epoch: 12 [1000/1800 (56%)]\tLoss: 0.254062\nTrain Epoch: 12 [1100/1800 (61%)]\tLoss: 0.175404\nTrain Epoch: 12 [1200/1800 (67%)]\tLoss: 0.212869\nTrain Epoch: 12 [1300/1800 (72%)]\tLoss: 0.252598\nTrain Epoch: 12 [1400/1800 (78%)]\tLoss: 0.225044\nTrain Epoch: 12 [1500/1800 (83%)]\tLoss: 0.208013\nTrain Epoch: 12 [1600/1800 (89%)]\tLoss: 0.322595\nTrain Epoch: 12 [1700/1800 (94%)]\tLoss: 0.159912\n\nevaluating...\nTest set:\tAverage loss: 0.6800, Average CER: 0.200428 Average WER: 0.7720\n\nTrain Epoch: 13 [0/1800 (0%)]\tLoss: 0.303560\nTrain Epoch: 13 [100/1800 (6%)]\tLoss: 0.265306\nTrain Epoch: 13 [200/1800 (11%)]\tLoss: 0.189325\nTrain Epoch: 13 [300/1800 (17%)]\tLoss: 0.143182\nTrain Epoch: 13 [400/1800 (22%)]\tLoss: 0.297743\nTrain Epoch: 13 [500/1800 (28%)]\tLoss: 0.314119\nTrain Epoch: 13 [600/1800 (33%)]\tLoss: 0.189327\nTrain Epoch: 13 [700/1800 (39%)]\tLoss: 0.084710\nTrain Epoch: 13 [800/1800 (44%)]\tLoss: 0.267463\nTrain Epoch: 13 [900/1800 (50%)]\tLoss: 0.310438\nTrain Epoch: 13 [1000/1800 (56%)]\tLoss: 0.239917\nTrain Epoch: 13 [1100/1800 (61%)]\tLoss: 0.143339\nTrain Epoch: 13 [1200/1800 (67%)]\tLoss: 0.159858\nTrain Epoch: 13 [1300/1800 (72%)]\tLoss: 0.227837\nTrain Epoch: 13 [1400/1800 (78%)]\tLoss: 0.245585\nTrain Epoch: 13 [1500/1800 (83%)]\tLoss: 0.313384\nTrain Epoch: 13 [1600/1800 (89%)]\tLoss: 0.233215\nTrain Epoch: 13 [1700/1800 (94%)]\tLoss: 0.305606\n\nevaluating...\nTest set:\tAverage loss: 0.7568, Average CER: 0.210213 Average WER: 0.7761\n\nTrain Epoch: 14 [0/1800 (0%)]\tLoss: 0.228538\nTrain Epoch: 14 [100/1800 (6%)]\tLoss: 0.095111\nTrain Epoch: 14 [200/1800 (11%)]\tLoss: 0.121183\nTrain Epoch: 14 [300/1800 (17%)]\tLoss: 0.147269\nTrain Epoch: 14 [400/1800 (22%)]\tLoss: 0.129825\nTrain Epoch: 14 [500/1800 (28%)]\tLoss: 0.179655\nTrain Epoch: 14 [600/1800 (33%)]\tLoss: 0.312055\nTrain Epoch: 14 [700/1800 (39%)]\tLoss: 0.105857\nTrain Epoch: 14 [800/1800 (44%)]\tLoss: 0.314863\nTrain Epoch: 14 [900/1800 (50%)]\tLoss: 0.142071\nTrain Epoch: 14 [1000/1800 (56%)]\tLoss: 1.450815\nTrain Epoch: 14 [1100/1800 (61%)]\tLoss: 0.105589\nTrain Epoch: 14 [1200/1800 (67%)]\tLoss: 0.249831\nTrain Epoch: 14 [1300/1800 (72%)]\tLoss: 0.134341\nTrain Epoch: 14 [1400/1800 (78%)]\tLoss: 0.212435\nTrain Epoch: 14 [1500/1800 (83%)]\tLoss: 0.032946\nTrain Epoch: 14 [1600/1800 (89%)]\tLoss: 0.235404\nTrain Epoch: 14 [1700/1800 (94%)]\tLoss: 0.256859\n\nevaluating...\nTest set:\tAverage loss: 0.6233, Average CER: 0.187469 Average WER: 0.7492\n\nTrain Epoch: 15 [0/1800 (0%)]\tLoss: 0.137536\nTrain Epoch: 15 [100/1800 (6%)]\tLoss: 0.122173\nTrain Epoch: 15 [200/1800 (11%)]\tLoss: 0.081146\nTrain Epoch: 15 [300/1800 (17%)]\tLoss: 0.095687\nTrain Epoch: 15 [400/1800 (22%)]\tLoss: 0.105613\nTrain Epoch: 15 [500/1800 (28%)]\tLoss: 0.991181\nTrain Epoch: 15 [600/1800 (33%)]\tLoss: 0.513690\nTrain Epoch: 15 [700/1800 (39%)]\tLoss: 0.190027\nTrain Epoch: 15 [800/1800 (44%)]\tLoss: 0.077731\nTrain Epoch: 15 [900/1800 (50%)]\tLoss: 0.157931\nTrain Epoch: 15 [1000/1800 (56%)]\tLoss: 0.111345\nTrain Epoch: 15 [1100/1800 (61%)]\tLoss: 0.128777\nTrain Epoch: 15 [1200/1800 (67%)]\tLoss: 0.115571\nTrain Epoch: 15 [1300/1800 (72%)]\tLoss: 0.116051\nTrain Epoch: 15 [1400/1800 (78%)]\tLoss: 0.038988\nTrain Epoch: 15 [1500/1800 (83%)]\tLoss: 0.131516\nTrain Epoch: 15 [1600/1800 (89%)]\tLoss: 0.160020\nTrain Epoch: 15 [1700/1800 (94%)]\tLoss: 0.107700\n\nevaluating...\nTest set:\tAverage loss: 0.6718, Average CER: 0.191239 Average WER: 0.7639\n\nTrain Epoch: 16 [0/1800 (0%)]\tLoss: 0.100841\nTrain Epoch: 16 [100/1800 (6%)]\tLoss: 0.168280\nTrain Epoch: 16 [200/1800 (11%)]\tLoss: 0.097115\nTrain Epoch: 16 [300/1800 (17%)]\tLoss: 0.036701\nTrain Epoch: 16 [400/1800 (22%)]\tLoss: 0.076233\nTrain Epoch: 16 [500/1800 (28%)]\tLoss: 0.116839\nTrain Epoch: 16 [600/1800 (33%)]\tLoss: 0.027528\nTrain Epoch: 16 [700/1800 (39%)]\tLoss: 0.051351\nTrain Epoch: 16 [800/1800 (44%)]\tLoss: 0.353506\nTrain Epoch: 16 [900/1800 (50%)]\tLoss: 0.030224\nTrain Epoch: 16 [1000/1800 (56%)]\tLoss: 0.109875\nTrain Epoch: 16 [1100/1800 (61%)]\tLoss: 0.165567\nTrain Epoch: 16 [1200/1800 (67%)]\tLoss: 0.028911\nTrain Epoch: 16 [1300/1800 (72%)]\tLoss: 0.122509\nTrain Epoch: 16 [1400/1800 (78%)]\tLoss: 0.107117\nTrain Epoch: 16 [1500/1800 (83%)]\tLoss: 0.146822\nTrain Epoch: 16 [1600/1800 (89%)]\tLoss: 0.303496\nTrain Epoch: 16 [1700/1800 (94%)]\tLoss: 0.095869\n\nevaluating...\nTest set:\tAverage loss: 0.7664, Average CER: 0.206120 Average WER: 0.7645\n\nTrain Epoch: 17 [0/1800 (0%)]\tLoss: 0.084664\nTrain Epoch: 17 [100/1800 (6%)]\tLoss: 0.036842\nTrain Epoch: 17 [200/1800 (11%)]\tLoss: 0.029922\nTrain Epoch: 17 [300/1800 (17%)]\tLoss: 0.954838\nTrain Epoch: 17 [400/1800 (22%)]\tLoss: 0.116874\nTrain Epoch: 17 [500/1800 (28%)]\tLoss: 0.075412\nTrain Epoch: 17 [600/1800 (33%)]\tLoss: 0.106174\nTrain Epoch: 17 [700/1800 (39%)]\tLoss: 0.053883\nTrain Epoch: 17 [800/1800 (44%)]\tLoss: 0.055650\nTrain Epoch: 17 [900/1800 (50%)]\tLoss: 0.050072\nTrain Epoch: 17 [1000/1800 (56%)]\tLoss: 0.222096\nTrain Epoch: 17 [1100/1800 (61%)]\tLoss: 0.044028\nTrain Epoch: 17 [1200/1800 (67%)]\tLoss: 0.060053\nTrain Epoch: 17 [1300/1800 (72%)]\tLoss: 0.118892\nTrain Epoch: 17 [1400/1800 (78%)]\tLoss: 0.051340\nTrain Epoch: 17 [1500/1800 (83%)]\tLoss: 0.185833\nTrain Epoch: 17 [1600/1800 (89%)]\tLoss: 0.162932\nTrain Epoch: 17 [1700/1800 (94%)]\tLoss: 0.220824\n\nevaluating...\nTest set:\tAverage loss: 0.7022, Average CER: 0.186891 Average WER: 0.7290\n\nTrain Epoch: 18 [0/1800 (0%)]\tLoss: 0.046612\nTrain Epoch: 18 [100/1800 (6%)]\tLoss: 0.077812\nTrain Epoch: 18 [200/1800 (11%)]\tLoss: 0.107489\nTrain Epoch: 18 [300/1800 (17%)]\tLoss: 0.040312\nTrain Epoch: 18 [400/1800 (22%)]\tLoss: 0.065319\nTrain Epoch: 18 [500/1800 (28%)]\tLoss: 0.053049\nTrain Epoch: 18 [600/1800 (33%)]\tLoss: 0.087846\nTrain Epoch: 18 [700/1800 (39%)]\tLoss: 0.101032\nTrain Epoch: 18 [800/1800 (44%)]\tLoss: 0.155791\nTrain Epoch: 18 [900/1800 (50%)]\tLoss: 0.129656\nTrain Epoch: 18 [1000/1800 (56%)]\tLoss: 0.095468\nTrain Epoch: 18 [1100/1800 (61%)]\tLoss: 0.037294\nTrain Epoch: 18 [1200/1800 (67%)]\tLoss: 0.322561\nTrain Epoch: 18 [1300/1800 (72%)]\tLoss: 0.075819\nTrain Epoch: 18 [1400/1800 (78%)]\tLoss: 0.129995\nTrain Epoch: 18 [1500/1800 (83%)]\tLoss: 0.052141\nTrain Epoch: 18 [1600/1800 (89%)]\tLoss: 0.039936\nTrain Epoch: 18 [1700/1800 (94%)]\tLoss: 0.142323\n\nevaluating...\nTest set:\tAverage loss: 0.7179, Average CER: 0.191364 Average WER: 0.7366\n\nTrain Epoch: 19 [0/1800 (0%)]\tLoss: 0.057129\nTrain Epoch: 19 [100/1800 (6%)]\tLoss: 0.024903\nTrain Epoch: 19 [200/1800 (11%)]\tLoss: 0.025904\nTrain Epoch: 19 [300/1800 (17%)]\tLoss: 0.035423\nTrain Epoch: 19 [400/1800 (22%)]\tLoss: 0.032363\nTrain Epoch: 19 [500/1800 (28%)]\tLoss: 0.036513\nTrain Epoch: 19 [600/1800 (33%)]\tLoss: 0.021821\nTrain Epoch: 19 [700/1800 (39%)]\tLoss: 0.016666\nTrain Epoch: 19 [800/1800 (44%)]\tLoss: 0.058507\nTrain Epoch: 19 [900/1800 (50%)]\tLoss: 0.035886\nTrain Epoch: 19 [1000/1800 (56%)]\tLoss: 0.063854\nTrain Epoch: 19 [1100/1800 (61%)]\tLoss: 0.039813\nTrain Epoch: 19 [1200/1800 (67%)]\tLoss: 0.106197\nTrain Epoch: 19 [1300/1800 (72%)]\tLoss: 0.038409\nTrain Epoch: 19 [1400/1800 (78%)]\tLoss: 0.518704\nTrain Epoch: 19 [1500/1800 (83%)]\tLoss: 0.115442\nTrain Epoch: 19 [1600/1800 (89%)]\tLoss: 0.111456\nTrain Epoch: 19 [1700/1800 (94%)]\tLoss: 0.052503\n\nevaluating...\nTest set:\tAverage loss: 0.7078, Average CER: 0.186990 Average WER: 0.7317\n\nTrain Epoch: 20 [0/1800 (0%)]\tLoss: 0.017283\nTrain Epoch: 20 [100/1800 (6%)]\tLoss: 0.031931\nTrain Epoch: 20 [200/1800 (11%)]\tLoss: 0.050322\nTrain Epoch: 20 [300/1800 (17%)]\tLoss: 0.043913\nTrain Epoch: 20 [400/1800 (22%)]\tLoss: 0.021153\nTrain Epoch: 20 [500/1800 (28%)]\tLoss: 0.063228\nTrain Epoch: 20 [600/1800 (33%)]\tLoss: 0.070339\nTrain Epoch: 20 [700/1800 (39%)]\tLoss: 0.071165\nTrain Epoch: 20 [800/1800 (44%)]\tLoss: 0.020934\nTrain Epoch: 20 [900/1800 (50%)]\tLoss: 0.911289\nTrain Epoch: 20 [1000/1800 (56%)]\tLoss: 0.038637\nTrain Epoch: 20 [1100/1800 (61%)]\tLoss: 0.031338\nTrain Epoch: 20 [1200/1800 (67%)]\tLoss: 0.032298\nTrain Epoch: 20 [1300/1800 (72%)]\tLoss: 0.022967\nTrain Epoch: 20 [1400/1800 (78%)]\tLoss: 0.011009\nTrain Epoch: 20 [1500/1800 (83%)]\tLoss: 0.550337\nTrain Epoch: 20 [1600/1800 (89%)]\tLoss: 0.601585\nTrain Epoch: 20 [1700/1800 (94%)]\tLoss: 0.064392\n\nevaluating...\nTest set:\tAverage loss: 0.7791, Average CER: 0.192202 Average WER: 0.7387\n\nTrain Epoch: 21 [0/1800 (0%)]\tLoss: 0.081893\nTrain Epoch: 21 [100/1800 (6%)]\tLoss: 0.030736\nTrain Epoch: 21 [200/1800 (11%)]\tLoss: 0.021566\nTrain Epoch: 21 [300/1800 (17%)]\tLoss: 0.062288\nTrain Epoch: 21 [400/1800 (22%)]\tLoss: 0.259333\nTrain Epoch: 21 [500/1800 (28%)]\tLoss: 0.072181\nTrain Epoch: 21 [600/1800 (33%)]\tLoss: 0.118231\nTrain Epoch: 21 [700/1800 (39%)]\tLoss: 0.230938\nTrain Epoch: 21 [800/1800 (44%)]\tLoss: 0.025861\nTrain Epoch: 21 [900/1800 (50%)]\tLoss: 0.019141\nTrain Epoch: 21 [1000/1800 (56%)]\tLoss: 0.034532\nTrain Epoch: 21 [1100/1800 (61%)]\tLoss: 0.114769\nTrain Epoch: 21 [1200/1800 (67%)]\tLoss: 0.036629\nTrain Epoch: 21 [1300/1800 (72%)]\tLoss: 0.014466\nTrain Epoch: 21 [1400/1800 (78%)]\tLoss: 0.032164\nTrain Epoch: 21 [1500/1800 (83%)]\tLoss: 0.056068\nTrain Epoch: 21 [1600/1800 (89%)]\tLoss: 0.031276\nTrain Epoch: 21 [1700/1800 (94%)]\tLoss: 0.030448\n\nevaluating...\nTest set:\tAverage loss: 0.7954, Average CER: 0.189027 Average WER: 0.7238\n\nTrain Epoch: 22 [0/1800 (0%)]\tLoss: 0.022079\nTrain Epoch: 22 [100/1800 (6%)]\tLoss: 0.038613\nTrain Epoch: 22 [200/1800 (11%)]\tLoss: 0.034613\nTrain Epoch: 22 [300/1800 (17%)]\tLoss: 0.056591\nTrain Epoch: 22 [400/1800 (22%)]\tLoss: 0.034383\nTrain Epoch: 22 [500/1800 (28%)]\tLoss: 0.012499\nTrain Epoch: 22 [600/1800 (33%)]\tLoss: 0.028970\nTrain Epoch: 22 [700/1800 (39%)]\tLoss: 0.029636\nTrain Epoch: 22 [800/1800 (44%)]\tLoss: 0.063338\nTrain Epoch: 22 [900/1800 (50%)]\tLoss: 0.014176\nTrain Epoch: 22 [1000/1800 (56%)]\tLoss: 0.026859\nTrain Epoch: 22 [1100/1800 (61%)]\tLoss: 0.014042\nTrain Epoch: 22 [1200/1800 (67%)]\tLoss: 0.011127\nTrain Epoch: 22 [1300/1800 (72%)]\tLoss: 0.017476\nTrain Epoch: 22 [1400/1800 (78%)]\tLoss: 0.026690\nTrain Epoch: 22 [1500/1800 (83%)]\tLoss: 0.013446\nTrain Epoch: 22 [1600/1800 (89%)]\tLoss: 0.046752\nTrain Epoch: 22 [1700/1800 (94%)]\tLoss: 0.022353\n\nevaluating...\nTest set:\tAverage loss: 0.8240, Average CER: 0.202633 Average WER: 0.7587\n\nTrain Epoch: 23 [0/1800 (0%)]\tLoss: 0.197588\nTrain Epoch: 23 [100/1800 (6%)]\tLoss: 0.095914\nTrain Epoch: 23 [200/1800 (11%)]\tLoss: 0.029550\nTrain Epoch: 23 [300/1800 (17%)]\tLoss: 0.113544\nTrain Epoch: 23 [400/1800 (22%)]\tLoss: 0.028385\nTrain Epoch: 23 [500/1800 (28%)]\tLoss: 0.010943\nTrain Epoch: 23 [600/1800 (33%)]\tLoss: 0.036267\nTrain Epoch: 23 [700/1800 (39%)]\tLoss: 0.011526\nTrain Epoch: 23 [800/1800 (44%)]\tLoss: 0.029876\nTrain Epoch: 23 [900/1800 (50%)]\tLoss: 0.059411\nTrain Epoch: 23 [1000/1800 (56%)]\tLoss: 0.035181\nTrain Epoch: 23 [1100/1800 (61%)]\tLoss: 0.074754\nTrain Epoch: 23 [1200/1800 (67%)]\tLoss: 0.024417\nTrain Epoch: 23 [1300/1800 (72%)]\tLoss: 0.051498\nTrain Epoch: 23 [1400/1800 (78%)]\tLoss: 0.024123\nTrain Epoch: 23 [1500/1800 (83%)]\tLoss: 0.053956\nTrain Epoch: 23 [1600/1800 (89%)]\tLoss: 0.085093\nTrain Epoch: 23 [1700/1800 (94%)]\tLoss: 0.033942\n\nevaluating...\nTest set:\tAverage loss: 0.8277, Average CER: 0.197146 Average WER: 0.7327\n\nTrain Epoch: 24 [0/1800 (0%)]\tLoss: 0.045617\nTrain Epoch: 24 [100/1800 (6%)]\tLoss: 0.022104\nTrain Epoch: 24 [200/1800 (11%)]\tLoss: 0.013861\nTrain Epoch: 24 [300/1800 (17%)]\tLoss: 0.027376\nTrain Epoch: 24 [400/1800 (22%)]\tLoss: 0.019289\nTrain Epoch: 24 [500/1800 (28%)]\tLoss: 0.167757\nTrain Epoch: 24 [600/1800 (33%)]\tLoss: 0.009169\nTrain Epoch: 24 [700/1800 (39%)]\tLoss: 0.008453\nTrain Epoch: 24 [800/1800 (44%)]\tLoss: 0.021984\nTrain Epoch: 24 [900/1800 (50%)]\tLoss: 0.009435\nTrain Epoch: 24 [1000/1800 (56%)]\tLoss: 0.065357\nTrain Epoch: 24 [1100/1800 (61%)]\tLoss: 0.339067\nTrain Epoch: 24 [1200/1800 (67%)]\tLoss: 0.011829\nTrain Epoch: 24 [1300/1800 (72%)]\tLoss: 0.038544\nTrain Epoch: 24 [1400/1800 (78%)]\tLoss: 0.084195\nTrain Epoch: 24 [1500/1800 (83%)]\tLoss: 0.025408\nTrain Epoch: 24 [1600/1800 (89%)]\tLoss: 0.023054\nTrain Epoch: 24 [1700/1800 (94%)]\tLoss: 0.012709\n\nevaluating...\nTest set:\tAverage loss: 0.7243, Average CER: 0.165008 Average WER: 0.6759\n\nTrain Epoch: 25 [0/1800 (0%)]\tLoss: 0.019350\nTrain Epoch: 25 [100/1800 (6%)]\tLoss: 0.003416\nTrain Epoch: 25 [200/1800 (11%)]\tLoss: 0.009658\nTrain Epoch: 25 [300/1800 (17%)]\tLoss: 0.001974\nTrain Epoch: 25 [400/1800 (22%)]\tLoss: 0.010298\nTrain Epoch: 25 [500/1800 (28%)]\tLoss: 0.164195\nTrain Epoch: 25 [600/1800 (33%)]\tLoss: 0.005845\nTrain Epoch: 25 [700/1800 (39%)]\tLoss: 0.009680\nTrain Epoch: 25 [800/1800 (44%)]\tLoss: 0.003316\nTrain Epoch: 25 [900/1800 (50%)]\tLoss: 0.010757\nTrain Epoch: 25 [1000/1800 (56%)]\tLoss: 0.038050\nTrain Epoch: 25 [1100/1800 (61%)]\tLoss: 0.027973\nTrain Epoch: 25 [1200/1800 (67%)]\tLoss: 0.004002\nTrain Epoch: 25 [1300/1800 (72%)]\tLoss: 1.131736\nTrain Epoch: 25 [1400/1800 (78%)]\tLoss: 0.023137\nTrain Epoch: 25 [1500/1800 (83%)]\tLoss: 0.152094\nTrain Epoch: 25 [1600/1800 (89%)]\tLoss: 0.004577\nTrain Epoch: 25 [1700/1800 (94%)]\tLoss: 0.012256\n\nevaluating...\nTest set:\tAverage loss: 0.7961, Average CER: 0.187552 Average WER: 0.7315\n\nTrain Epoch: 26 [0/1800 (0%)]\tLoss: 0.005220\nTrain Epoch: 26 [100/1800 (6%)]\tLoss: 0.004712\nTrain Epoch: 26 [200/1800 (11%)]\tLoss: 0.050585\nTrain Epoch: 26 [300/1800 (17%)]\tLoss: 0.054401\nTrain Epoch: 26 [400/1800 (22%)]\tLoss: 0.017568\nTrain Epoch: 26 [500/1800 (28%)]\tLoss: 0.041165\nTrain Epoch: 26 [600/1800 (33%)]\tLoss: 0.004999\nTrain Epoch: 26 [700/1800 (39%)]\tLoss: 0.161082\nTrain Epoch: 26 [800/1800 (44%)]\tLoss: 0.007764\nTrain Epoch: 26 [900/1800 (50%)]\tLoss: 0.010880\nTrain Epoch: 26 [1000/1800 (56%)]\tLoss: 0.004386\nTrain Epoch: 26 [1100/1800 (61%)]\tLoss: 0.005639\nTrain Epoch: 26 [1200/1800 (67%)]\tLoss: 0.008635\nTrain Epoch: 26 [1300/1800 (72%)]\tLoss: 0.096130\nTrain Epoch: 26 [1400/1800 (78%)]\tLoss: 0.003613\nTrain Epoch: 26 [1500/1800 (83%)]\tLoss: 0.018720\nTrain Epoch: 26 [1600/1800 (89%)]\tLoss: 0.008003\nTrain Epoch: 26 [1700/1800 (94%)]\tLoss: 0.003943\n\nevaluating...\nTest set:\tAverage loss: 0.7359, Average CER: 0.164650 Average WER: 0.6573\n\nTrain Epoch: 27 [0/1800 (0%)]\tLoss: 0.003645\nTrain Epoch: 27 [100/1800 (6%)]\tLoss: 0.004820\nTrain Epoch: 27 [200/1800 (11%)]\tLoss: 0.005171\nTrain Epoch: 27 [300/1800 (17%)]\tLoss: 0.042691\nTrain Epoch: 27 [400/1800 (22%)]\tLoss: 0.002038\nTrain Epoch: 27 [500/1800 (28%)]\tLoss: 0.002730\nTrain Epoch: 27 [600/1800 (33%)]\tLoss: 0.002842\nTrain Epoch: 27 [700/1800 (39%)]\tLoss: 0.010562\nTrain Epoch: 27 [800/1800 (44%)]\tLoss: 0.002556\nTrain Epoch: 27 [900/1800 (50%)]\tLoss: 0.009420\nTrain Epoch: 27 [1000/1800 (56%)]\tLoss: 0.002521\nTrain Epoch: 27 [1100/1800 (61%)]\tLoss: 0.001924\nTrain Epoch: 27 [1200/1800 (67%)]\tLoss: 0.030109\nTrain Epoch: 27 [1300/1800 (72%)]\tLoss: 0.006588\nTrain Epoch: 27 [1400/1800 (78%)]\tLoss: 0.013442\nTrain Epoch: 27 [1500/1800 (83%)]\tLoss: 0.017932\nTrain Epoch: 27 [1600/1800 (89%)]\tLoss: 0.024490\nTrain Epoch: 27 [1700/1800 (94%)]\tLoss: 0.004467\n\nevaluating...\nTest set:\tAverage loss: 0.7516, Average CER: 0.162408 Average WER: 0.6512\n\nTrain Epoch: 28 [0/1800 (0%)]\tLoss: 0.001177\nTrain Epoch: 28 [100/1800 (6%)]\tLoss: 0.001956\nTrain Epoch: 28 [200/1800 (11%)]\tLoss: 0.001686\nTrain Epoch: 28 [300/1800 (17%)]\tLoss: 0.002105\nTrain Epoch: 28 [400/1800 (22%)]\tLoss: 0.005448\nTrain Epoch: 28 [500/1800 (28%)]\tLoss: 0.014474\nTrain Epoch: 28 [600/1800 (33%)]\tLoss: 0.014144\nTrain Epoch: 28 [700/1800 (39%)]\tLoss: 0.002629\nTrain Epoch: 28 [800/1800 (44%)]\tLoss: 0.005313\nTrain Epoch: 28 [900/1800 (50%)]\tLoss: 0.031541\nTrain Epoch: 28 [1000/1800 (56%)]\tLoss: 0.000789\nTrain Epoch: 28 [1100/1800 (61%)]\tLoss: 0.001801\nTrain Epoch: 28 [1200/1800 (67%)]\tLoss: 0.004816\nTrain Epoch: 28 [1300/1800 (72%)]\tLoss: 0.002044\nTrain Epoch: 28 [1400/1800 (78%)]\tLoss: 0.007836\nTrain Epoch: 28 [1500/1800 (83%)]\tLoss: 0.004853\nTrain Epoch: 28 [1600/1800 (89%)]\tLoss: 0.002222\nTrain Epoch: 28 [1700/1800 (94%)]\tLoss: 0.002999\n\nevaluating...\nTest set:\tAverage loss: 0.7432, Average CER: 0.167787 Average WER: 0.6712\n\nTrain Epoch: 29 [0/1800 (0%)]\tLoss: 0.022744\nTrain Epoch: 29 [100/1800 (6%)]\tLoss: 0.002794\nTrain Epoch: 29 [200/1800 (11%)]\tLoss: 0.077679\nTrain Epoch: 29 [300/1800 (17%)]\tLoss: 0.003818\nTrain Epoch: 29 [400/1800 (22%)]\tLoss: 0.007762\nTrain Epoch: 29 [500/1800 (28%)]\tLoss: 0.003224\nTrain Epoch: 29 [600/1800 (33%)]\tLoss: 0.086092\nTrain Epoch: 29 [700/1800 (39%)]\tLoss: 0.001511\nTrain Epoch: 29 [800/1800 (44%)]\tLoss: 0.003472\nTrain Epoch: 29 [900/1800 (50%)]\tLoss: 0.001403\nTrain Epoch: 29 [1000/1800 (56%)]\tLoss: 0.002473\nTrain Epoch: 29 [1100/1800 (61%)]\tLoss: 0.002000\nTrain Epoch: 29 [1200/1800 (67%)]\tLoss: 0.001777\nTrain Epoch: 29 [1300/1800 (72%)]\tLoss: 0.001926\nTrain Epoch: 29 [1400/1800 (78%)]\tLoss: 0.001399\nTrain Epoch: 29 [1500/1800 (83%)]\tLoss: 0.001794\nTrain Epoch: 29 [1600/1800 (89%)]\tLoss: 0.006833\nTrain Epoch: 29 [1700/1800 (94%)]\tLoss: 0.000857\n\nevaluating...\nTest set:\tAverage loss: 0.7303, Average CER: 0.150773 Average WER: 0.6292\n\nTrain Epoch: 30 [0/1800 (0%)]\tLoss: 0.000780\nTrain Epoch: 30 [100/1800 (6%)]\tLoss: 0.001384\nTrain Epoch: 30 [200/1800 (11%)]\tLoss: 0.001701\nTrain Epoch: 30 [300/1800 (17%)]\tLoss: 0.001701\nTrain Epoch: 30 [400/1800 (22%)]\tLoss: 0.001051\nTrain Epoch: 30 [500/1800 (28%)]\tLoss: 0.001772\nTrain Epoch: 30 [600/1800 (33%)]\tLoss: 0.001217\nTrain Epoch: 30 [700/1800 (39%)]\tLoss: 0.000987\nTrain Epoch: 30 [800/1800 (44%)]\tLoss: 0.001239\nTrain Epoch: 30 [900/1800 (50%)]\tLoss: 0.001161\nTrain Epoch: 30 [1000/1800 (56%)]\tLoss: 0.001398\nTrain Epoch: 30 [1100/1800 (61%)]\tLoss: 0.001205\nTrain Epoch: 30 [1200/1800 (67%)]\tLoss: 0.001862\nTrain Epoch: 30 [1300/1800 (72%)]\tLoss: 0.001457\nTrain Epoch: 30 [1400/1800 (78%)]\tLoss: 0.002171\nTrain Epoch: 30 [1500/1800 (83%)]\tLoss: 0.002431\nTrain Epoch: 30 [1600/1800 (89%)]\tLoss: 0.001556\nTrain Epoch: 30 [1700/1800 (94%)]\tLoss: 0.002602\n\nevaluating...\nTest set:\tAverage loss: 0.7362, Average CER: 0.151399 Average WER: 0.6343\n\nTrain Epoch: 31 [0/1800 (0%)]\tLoss: 0.001101\nTrain Epoch: 31 [100/1800 (6%)]\tLoss: 0.003949\nTrain Epoch: 31 [200/1800 (11%)]\tLoss: 0.000804\nTrain Epoch: 31 [300/1800 (17%)]\tLoss: 0.000994\nTrain Epoch: 31 [400/1800 (22%)]\tLoss: 0.000771\nTrain Epoch: 31 [500/1800 (28%)]\tLoss: 0.000626\nTrain Epoch: 31 [600/1800 (33%)]\tLoss: 0.020241\nTrain Epoch: 31 [700/1800 (39%)]\tLoss: 0.000905\nTrain Epoch: 31 [800/1800 (44%)]\tLoss: 0.000870\nTrain Epoch: 31 [900/1800 (50%)]\tLoss: 0.001161\nTrain Epoch: 31 [1000/1800 (56%)]\tLoss: 0.001252\nTrain Epoch: 31 [1100/1800 (61%)]\tLoss: 0.001047\nTrain Epoch: 31 [1200/1800 (67%)]\tLoss: 0.000702\nTrain Epoch: 31 [1300/1800 (72%)]\tLoss: 0.000687\nTrain Epoch: 31 [1400/1800 (78%)]\tLoss: 0.001121\nTrain Epoch: 31 [1500/1800 (83%)]\tLoss: 0.000694\nTrain Epoch: 31 [1600/1800 (89%)]\tLoss: 0.001011\nTrain Epoch: 31 [1700/1800 (94%)]\tLoss: 0.000992\n\nevaluating...\nTest set:\tAverage loss: 0.7410, Average CER: 0.148830 Average WER: 0.6286\n\nTrain Epoch: 32 [0/1800 (0%)]\tLoss: 0.001045\nTrain Epoch: 32 [100/1800 (6%)]\tLoss: 0.000470\nTrain Epoch: 32 [200/1800 (11%)]\tLoss: 0.000834\nTrain Epoch: 32 [300/1800 (17%)]\tLoss: 0.000961\nTrain Epoch: 32 [400/1800 (22%)]\tLoss: 0.001115\nTrain Epoch: 32 [500/1800 (28%)]\tLoss: 0.001019\nTrain Epoch: 32 [600/1800 (33%)]\tLoss: 0.000824\nTrain Epoch: 32 [700/1800 (39%)]\tLoss: 0.000695\nTrain Epoch: 32 [800/1800 (44%)]\tLoss: 0.000882\nTrain Epoch: 32 [900/1800 (50%)]\tLoss: 0.000515\nTrain Epoch: 32 [1000/1800 (56%)]\tLoss: 0.003174\nTrain Epoch: 32 [1100/1800 (61%)]\tLoss: 0.000615\nTrain Epoch: 32 [1200/1800 (67%)]\tLoss: 0.001308\nTrain Epoch: 32 [1300/1800 (72%)]\tLoss: 0.001076\nTrain Epoch: 32 [1400/1800 (78%)]\tLoss: 0.057712\nTrain Epoch: 32 [1500/1800 (83%)]\tLoss: 0.003523\nTrain Epoch: 32 [1600/1800 (89%)]\tLoss: 0.001269\nTrain Epoch: 32 [1700/1800 (94%)]\tLoss: 0.000840\n\nevaluating...\nTest set:\tAverage loss: 0.7551, Average CER: 0.151513 Average WER: 0.6349\n\nTrain Epoch: 33 [0/1800 (0%)]\tLoss: 0.000413\nTrain Epoch: 33 [100/1800 (6%)]\tLoss: 0.000417\nTrain Epoch: 33 [200/1800 (11%)]\tLoss: 0.000783\nTrain Epoch: 33 [300/1800 (17%)]\tLoss: 0.000831\nTrain Epoch: 33 [400/1800 (22%)]\tLoss: 0.000903\nTrain Epoch: 33 [500/1800 (28%)]\tLoss: 0.000730\nTrain Epoch: 33 [600/1800 (33%)]\tLoss: 0.000702\nTrain Epoch: 33 [700/1800 (39%)]\tLoss: 0.000692\nTrain Epoch: 33 [800/1800 (44%)]\tLoss: 0.023220\nTrain Epoch: 33 [900/1800 (50%)]\tLoss: 0.001185\nTrain Epoch: 33 [1000/1800 (56%)]\tLoss: 0.000831\nTrain Epoch: 33 [1100/1800 (61%)]\tLoss: 0.002879\nTrain Epoch: 33 [1200/1800 (67%)]\tLoss: 0.000499\nTrain Epoch: 33 [1300/1800 (72%)]\tLoss: 0.000595\nTrain Epoch: 33 [1400/1800 (78%)]\tLoss: 0.000883\nTrain Epoch: 33 [1500/1800 (83%)]\tLoss: 0.000990\nTrain Epoch: 33 [1600/1800 (89%)]\tLoss: 0.000751\nTrain Epoch: 33 [1700/1800 (94%)]\tLoss: 0.000910\n\nevaluating...\nTest set:\tAverage loss: 0.7543, Average CER: 0.147510 Average WER: 0.6212\n\nTrain Epoch: 34 [0/1800 (0%)]\tLoss: 0.000430\nTrain Epoch: 34 [100/1800 (6%)]\tLoss: 0.000704\nTrain Epoch: 34 [200/1800 (11%)]\tLoss: 0.000401\nTrain Epoch: 34 [300/1800 (17%)]\tLoss: 0.000425\nTrain Epoch: 34 [400/1800 (22%)]\tLoss: 0.000695\nTrain Epoch: 34 [500/1800 (28%)]\tLoss: 0.000396\nTrain Epoch: 34 [600/1800 (33%)]\tLoss: 0.000886\nTrain Epoch: 34 [700/1800 (39%)]\tLoss: 0.000567\nTrain Epoch: 34 [800/1800 (44%)]\tLoss: 0.000431\nTrain Epoch: 34 [900/1800 (50%)]\tLoss: 0.000842\nTrain Epoch: 34 [1000/1800 (56%)]\tLoss: 0.000939\nTrain Epoch: 34 [1100/1800 (61%)]\tLoss: 0.000519\nTrain Epoch: 34 [1200/1800 (67%)]\tLoss: 0.001141\nTrain Epoch: 34 [1300/1800 (72%)]\tLoss: 0.000870\nTrain Epoch: 34 [1400/1800 (78%)]\tLoss: 0.002358\nTrain Epoch: 34 [1500/1800 (83%)]\tLoss: 0.001200\nTrain Epoch: 34 [1600/1800 (89%)]\tLoss: 0.001022\nTrain Epoch: 34 [1700/1800 (94%)]\tLoss: 0.000789\n\nevaluating...\nTest set:\tAverage loss: 0.7532, Average CER: 0.148118 Average WER: 0.6173\n\nTrain Epoch: 35 [0/1800 (0%)]\tLoss: 0.000354\nTrain Epoch: 35 [100/1800 (6%)]\tLoss: 0.001253\nTrain Epoch: 35 [200/1800 (11%)]\tLoss: 0.000797\nTrain Epoch: 35 [300/1800 (17%)]\tLoss: 0.000547\nTrain Epoch: 35 [400/1800 (22%)]\tLoss: 0.000919\nTrain Epoch: 35 [500/1800 (28%)]\tLoss: 0.002143\nTrain Epoch: 35 [600/1800 (33%)]\tLoss: 0.000826\nTrain Epoch: 35 [700/1800 (39%)]\tLoss: 0.000691\nTrain Epoch: 35 [800/1800 (44%)]\tLoss: 0.000870\nTrain Epoch: 35 [900/1800 (50%)]\tLoss: 0.000474\nTrain Epoch: 35 [1000/1800 (56%)]\tLoss: 0.000225\nTrain Epoch: 35 [1100/1800 (61%)]\tLoss: 0.001042\nTrain Epoch: 35 [1200/1800 (67%)]\tLoss: 0.000522\nTrain Epoch: 35 [1300/1800 (72%)]\tLoss: 0.000524\nTrain Epoch: 35 [1400/1800 (78%)]\tLoss: 0.000600\nTrain Epoch: 35 [1500/1800 (83%)]\tLoss: 0.000533\nTrain Epoch: 35 [1600/1800 (89%)]\tLoss: 0.000503\nTrain Epoch: 35 [1700/1800 (94%)]\tLoss: 0.001104\n\nevaluating...\nTest set:\tAverage loss: 0.7542, Average CER: 0.147584 Average WER: 0.6210\n\nCPU times: user 12min 12s, sys: 15 s, total: 12min 27s\nWall time: 12min 27s\n","output_type":"stream"}]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cpu\")\n\nmodel = torch.load('/kaggle/working/model.pt')\n\n#1543 1882 1372\n\nmodel.to(device)\npredict(model, '/kaggle/input/upd-speech/mono_voice/1964.wav', device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:26:24.339488Z","iopub.execute_input":"2023-05-24T10:26:24.340405Z","iopub.status.idle":"2023-05-24T10:26:24.350954Z","shell.execute_reply.started":"2023-05-24T10:26:24.340364Z","shell.execute_reply":"2023-05-24T10:26:24.349831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:16:11.401175Z","iopub.execute_input":"2023-05-13T16:16:11.401879Z","iopub.status.idle":"2023-05-13T16:16:11.430020Z","shell.execute_reply.started":"2023-05-13T16:16:11.401838Z","shell.execute_reply":"2023-05-13T16:16:11.428960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(directory):\n    total_duration = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            filepath = os.path.join(directory, filename)\n            with wave.open(filepath, 'r') as wav_file:\n                frames = wav_file.getnframes()\n                rate = wav_file.getframerate()\n                duration = frames / float(rate)\n                total_duration += duration\n    return total_duration\n\ndirectory = '/kaggle/input/upd-speech/mono_voice'\ntotal_duration = get_wav_duration(directory)\nprint('Total duration of WAV files:', total_duration, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:15.415086Z","iopub.execute_input":"2023-07-05T10:09:15.415876Z","iopub.status.idle":"2023-07-05T10:09:18.755936Z","shell.execute_reply.started":"2023-07-05T10:09:15.415836Z","shell.execute_reply":"2023-07-05T10:09:18.754693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return '{:02d}:{:02d}:{:02d}'.format(int(hours), int(minutes), int(seconds))\nseconds = 3661\nformatted_time = format_time(total_duration)\nprint(formatted_time)  # Output: '01:01:01'","metadata":{"execution":{"iopub.status.busy":"2023-07-05T10:09:23.353548Z","iopub.execute_input":"2023-07-05T10:09:23.354296Z","iopub.status.idle":"2023-07-05T10:09:23.361628Z","shell.execute_reply.started":"2023-07-05T10:09:23.354254Z","shell.execute_reply":"2023-07-05T10:09:23.360431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}